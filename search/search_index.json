{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#baktfold","title":"baktfold","text":"<p>Rapid &amp; standardized annotation of bacterial genomes, MAGs &amp; plasmids using protein structural information</p> <p><code>baktfold</code> is a sensitive annotation tool for bacterial genomes, MAGs &amp; plasmids genomes using protein structural homology. </p> <p><code>baktfold</code> is very similar to phold but goes beyond phages to bacterial annotation. <code>baktfold</code> takes all \"hypothetical proteins\" from Bakta's output and uses the ProstT5 protein language model to rapidly translate protein amino acid sequences to the 3Di token alphabet used by Foldseek. Foldseek is then used to search these against a series of databases (SwissProt, AlphaFold Database non-singleton clusters, PDB and CATH).</p> <p>Additionally, instead of using ProstT5, you can specify protein structures that you have pre-computed for your hypothetical proteins.</p> <p>You can also specify custom databases to search against using <code>--custom-db</code></p> <p>Baktfold is currently under active development. We would welcome any and all feedback (especially bugs) via Issues</p>"},{"location":"#google-colab-notebook","title":"Google Colab Notebook","text":"<p>If you don't want to install <code>baktfold</code> locally, you can run it without any code using the Google Colab notebook</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>baktfold</li> <li>Google Colab Notebook</li> <li>Table of Contents</li> <li>Install<ul> <li>Conda (recommended)</li> <li>Pip</li> <li>Source</li> <li>Database Installation</li> </ul> </li> <li>Example</li> <li>Usage</li> <li>Output<ul> <li>Conceptual terms</li> </ul> </li> <li>Citations</li> </ul>"},{"location":"#install","title":"Install","text":""},{"location":"#conda-recommended","title":"Conda (recommended)","text":"<ul> <li>The best way to install <code>baktfold</code> is using conda, as this will install Foldseek (the only non-Python dependency) along with the Python dependencies</li> <li>We would highly recommend installing conda via miniforge.</li> <li>To install baktfold:</li> </ul> <pre><code>conda create -n baktfoldENV -c conda-forge -c bioconda baktfold \n</code></pre> <ul> <li>To utilise phold with GPU, a GPU compatible version of pytorch must be installed. By default conda will usually install a CPU-only version.</li> <li>If you have an NVIDIA GPU, please try:</li> </ul> <pre><code>conda create -n baktfoldENV -c conda-forge -c bioconda baktfold pytorch=*=cuda*\n</code></pre> <ul> <li>If you have a Mac with M-series Apple Silicon, you may need to install a particular version of Pytorch to utilise GPU-acceleration</li> <li>The same is true if you use other non-NVIDIA e.g. AMD GPUs</li> <li>See this link for some more detail and further links</li> </ul>"},{"location":"#pip","title":"Pip","text":"<ul> <li>You can also install baktfold using pip.</li> </ul> <pre><code>pip install baktfold\n</code></pre> <ul> <li>You will need to have Foldseek (ideally v10.941cd33) installed and available in the $PATH.</li> </ul>"},{"location":"#source","title":"Source","text":"<ul> <li>You can install the latest version of baktfold with potentially untested and unreleased changes into a conda environment using conda as follows:</li> </ul> <pre><code>conda create -n baktfoldENV foldseek\nconda activate baktfoldENV\ngit clone https://github.com/gbouras13/baktfold.git\ncd baktfold\npip install .\nbaktfold --help\n</code></pre>"},{"location":"#database-installation","title":"Database Installation","text":"<ul> <li>To download and install baktfold's databases (use as many threads with <code>-t</code> as you can to speed up downloading)</li> </ul> <pre><code>baktfold install -d baktfold_db -t 8\n</code></pre> <ul> <li>If you have an NVIDIA GPU, you will need to format the database to allow it to use Foldseek-GPU with <code>--foldseek-gpu</code><ul> <li>Note: you can do this after downloading the database with the above command (it won't redownload the database, only do the relevant Foldseek database padding)</li> </ul> </li> </ul> <pre><code>baktfold install -d baktfold_db --foldseek-gpu\n</code></pre>"},{"location":"#example","title":"Example","text":"<ul> <li>You will first need to run bakta and use the resulting <code>.json</code> file as input for <code>baktfold</code><ul> <li>We will add other input formats eventually, put we would always recommend running Bakta first, as it is awesome and comprehensive</li> </ul> </li> <li>To use <code>baktfold run</code> using a dummy test example</li> </ul> <pre><code># with nvidia gpu \nbaktfold run -i tests/test_data/assembly_bakta_output/assembly.json  -o baktfold_output -f -t 8 -d baktfold_db   --foldseek-gpu\n# without nvidia gpu available\nbaktfold run -i tests/test_data/assembly_bakta_output/assembly.json  -o baktfold_output -f -t 8 -d baktfold_db   \n</code></pre> <ul> <li>To use <code>baktfold proteins</code> using a dummy test example protein <code>.faa</code> file</li> <li>Note that this can be any <code>.faa</code> (It does not have to be the output of Bakta)</li> </ul> <pre><code># with nvidia gpu \nbaktfold proteins -i tests/test_data/assembly.hypotheticals.faa  -o baktfold_proteins_output -f -t 8 -d baktfold_db   --foldseek-gpu\n# without nvidia gpu available\nbaktfold proteins -i tests/test_data/assembly.hypotheticals.faa  -o baktfold_proteins_output -f -t 8 -d baktfold_db   \n</code></pre>"},{"location":"#usage","title":"Usage","text":"<ul> <li>The two most useful commands are <code>baktfold run</code> and <code>baktfold proteins</code></li> <li><code>baktfold run</code> accepts a Bakta json file as its input, and by default, it will annotate all hypothetical CDS and return a variety of Bakta-like compliant output formats. All other annotations will be inherited from the Bakta output</li> <li><code>baktfold proteins</code> accepts a protein FASTA <code>.faa</code> format file as input. It will annotate all protein sequences and return a variety of <code>bakta_proteins</code>-like output formats</li> <li> <p><code>baktfold predict</code> and <code>baktfold compare</code> split <code>baktfold run</code> into the ProstT5 and Foldseek modules, while <code>baktfold proteins-predict</code> and <code>baktfold proteins-compare</code> do the same for <code>baktfold proteins</code> (useful if you have non-NVIDIA GPUs)</p> </li> <li> <p>It is recommend you run baktfold with a GPU if you can.</p> </li> <li>If you do not have a GPU, baktfold will still run, but the ProstT5 step will be fairly slow.</li> <li>If you have a NVIDIA GPU, you can also use the <code>--foldseek-gpu</code> parameter to accelerate Foldseek further</li> </ul> <pre><code>Usage: baktfold [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  -h, --help     Show this message and exit.\n  -V, --version  Show the version and exit.\n\nCommands:\n  citation          Print the citation(s) for this tool\n  compare           Runs Foldseek vs baktfold db\n  install           Installs ProstT5 model and baktfold database\n  predict           Uses ProstT5 to predict 3Di tokens - GPU recommended\n  proteins          baktfold protein-predict then comapare all in one - GPU...\n  proteins-compare  Runs Foldseek vs baktfold db on proteins input\n  proteins-predict  Runs ProstT5 on a multiFASTA input - GPU recommended\n  run               baktfold predict then comapare all in one - GPU...\n</code></pre> <pre><code>Usage: baktfold run [OPTIONS]\n\n  baktfold predict then comapare all in one - GPU recommended\n\nOptions:\n  -h, --help                     Show this message and exit.\n  -V, --version                  Show the version and exit.\n  -i, --input PATH               Path to input file in Bakta Genbank format or\n                                 Bakta JSON format  [required]\n  -o, --output PATH              Output directory   [default: output_baktfold]\n  -t, --threads INTEGER          Number of threads  [default: 1]\n  -p, --prefix TEXT              Prefix for output files  [default: baktfold]\n  -d, --database TEXT            Specific path to installed baktfold database\n  -f, --force                    Force overwrites the output directory\n  --batch-size INTEGER           batch size for ProstT5. 1 is usually fastest.\n                                 [default: 1]\n  --cpu                          Use cpus only.\n  --omit-probs                   Do not output per residue 3Di probabilities\n                                 from ProstT5. Mean per protein 3Di\n                                 probabilities will always be output.\n  --save-per-residue-embeddings  Save the ProstT5 embeddings per resuide in a\n                                 h5 file\n  --save-per-protein-embeddings  Save the ProstT5 embeddings as means per\n                                 protein in a h5 file\n  --mask-threshold FLOAT         Masks 3Di residues below this value of\n                                 ProstT5 confidence for Foldseek searches\n                                 [default: 25]\n  -e, --evalue FLOAT             Evalue threshold for Foldseek  [default:\n                                 1e-3]\n  -s, --sensitivity FLOAT        Sensitivity parameter for foldseek  [default:\n                                 9.5]\n  --keep-tmp-files               Keep temporary intermediate files,\n                                 particularly the large foldseek_results.tsv\n                                 of all Foldseek hits\n  --max-seqs INTEGER             Maximum results per query sequence allowed to\n                                 pass the prefilter. You may want to reduce\n                                 this to save disk space for enormous datasets\n                                 [default: 1000]\n  --ultra-sensitive              Runs baktfold with maximum sensitivity by\n                                 skipping Foldseek prefilter. Not recommended\n                                 for large datasets.\n  --extra-foldseek-params TEXT   Extra foldseek search params\n  --custom-db TEXT               Path to custom database\n  --foldseek-gpu                 Use this to enable compatibility with\n                                 Foldseek-GPU search acceleration\n  --custom-annotations PATH      Custom Foldseek DB annotations, 2 column tsv.\n                                 Column 1 matches the Foldseek headers, column\n                                 2 is the description.\n  -a, --all-proteins             annotate all proteins (not just\n                                 hypotheticals)\n</code></pre>"},{"location":"#output","title":"Output","text":"<ul> <li>The majority of outputs match bakta.</li> <li>Specifically, all the format compliant outputs match bakta's.</li> <li>The differences are:<ul> <li><code>&lt;prefix&gt;.inference.tsv</code> is changed compared to bakta. In <code>baktfold</code>, this file gives a quick overview of the different <code>baktfold</code> databases the query protein has hit (if any)</li> <li>For example:</li> </ul> </li> </ul> <pre><code>ID  Length  Product Swissprot   AFDBClusters    PDB CATH\nMEGJMNBEGN_27   162 HTH-type quorum-sensing regulator RhlR  swissprot_P54292    afdbclusters_A0A9E1VSB0 pdb_5l09    cath_3sztB01\nMEGJMNBEGN_30   68  hypothetical protein                \nMEGJMNBEGN_70   94  hypothetical protein        afdbclusters_A0A1I3V7E0     \n</code></pre> <pre><code>* `&lt;prefix&gt;_&lt;database&gt;_tophit.tsv` files give the detailed Foldseek alignment information for each tophit found for each database.\n* For example:\n</code></pre> <pre><code>query   target  bitscore    fident  evalue  qStart  qEnd    qLen    qCov    tStart  tEnd    tLen    tCov\nMEGJMN_070  AF-A0A1I3V7E0-F1-model_v6   292 0.41    2.619e-06   1   91  93  0.97    1   95  99  0.95\n</code></pre> <pre><code>* The full Foldseek search outputs are not kept by default (only tophits) - you can keep the full Foldseek search tsvs using `--keep-tmp-files`. They will be called `foldseek_results_&lt;database&gt;.tsv`\n* `baktfold_3di.fasta` which gives the 3Di tokens for each input CDS\n* `baktfold_prostT5_3di_mean_probabilities.csv` and `baktfold_prostT5_3di_all_probabilities.json`, which give some score of the confidence ProstT5 has in its predictions. You can disable this output with `--omit-probs`\n* Baktfold does not have plotting functionality like Bakta (yet)\n</code></pre>"},{"location":"#conceptual-terms","title":"Conceptual terms","text":"<ul> <li>As Baktfold inherits annotations from Bakta, please see the explanation in bakta for all other concepts here</li> <li>Baktfold adds one conceptual term in addition to Bakta's:<ul> <li>PSTC: protein structure clusters. These comprise of structure-based annotations to any of Baktfold's databases</li> </ul> </li> </ul>"},{"location":"#citations","title":"Citations","text":"<ul> <li> <p>A manuscript describing <code>baktfold</code> is in preparation.</p> </li> <li> <p>Please be sure to cite the following core dependencies - citing all bioinformatics tools that you use helps us, so helps you get better bioinformatics tools:</p> </li> <li> <p>Foldseek - (https://github.com/steineggerlab/foldseek) van Kempen M, Kim S, Tumescheit C, Mirdita M, Lee J, Gilchrist C, S\u00f6ding J, and Steinegger M. Fast and accurate protein structure search with Foldseek. Nature Biotechnology (2023), doi:10.1038/s41587-023-01773-0 </p> </li> <li> <p>ProstT5 - (https://github.com/mheinzinger/ProstT5) Michael Heinzinger, Konstantin Weissenow, Joaquin Gomez Sanchez, Adrian Henkel, Martin Steinegger, Burkhard Rost. ProstT5: Bilingual language model for protein sequence and structure. NAR Genomics and Bioinformatics (2024) doi:10.1101/2023.07.23.550085 </p> </li> <li> <p>Please also consider citing these databases where relevant:</p> </li> <li> <p>AFDB/SwissProt - Mihaly Varadi, Damian Bertoni, Paulyna Magana, Urmila Paramval, Ivanna Pidruchna, Malarvizhi Radhakrishnan, Maxim Tsenkov, Sreenath Nair, Milot Mirdita, Jingi Yeo, Oleg Kovalevskiy, Kathryn Tunyasuvunakool, Agata Laydon, Augustin \u017d\u00eddek, Hamish Tomlinson, Dhavanthi Hariharan, Josh Abrahamson, Tim Green, John Jumper, Ewan Birney, Martin Steinegger, Demis Hassabis, Sameer Velankar, AlphaFold Protein Structure Database in 2024: providing structure coverage for over 214 million protein sequences, Nucleic Acids Research, Volume 52, Issue D1, 5 January 2024, Pages D368\u2013D375, https://doi.org/10.1093/nar/gkad1011</p> </li> <li>CATH - Orengo CA, Michie AD, Jones S, Jones DT, Swindells MB, Thornton JM. CATH--a hierarchic classification of protein domain structures. Structure. 1997 Aug 15;5(8):1093-108. doi: 10.1016/s0969-2126(97)00260-8. PMID: 9309224.</li> <li>PDB - H.M. Berman, J. Westbrook, Z. Feng, G. Gilliland, T.N. Bhat, H. Weissig, I.N. Shindyalov, P.E. Bourne, The Protein Data Bank (2000) Nucleic Acids Research 28: 235-242 https://doi.org/10.1093/nar/28.1.235</li> </ul>"},{"location":"reference/","title":"Index","text":"<p>Module for manipulating genbank files some taken from phynteny https://github.com/susiegriggo/Phynteny</p> <p>Some code adapted from @mheinzinger </p> <p>https://github.com/mheinzinger/ProstT5/blob/main/scripts/generate_foldseek_db.py</p> <p>Code adapted from @mheinzinger </p> <p>https://github.com/mheinzinger/ProstT5/blob/main/scripts/predict_3Di_encoderOnly.py</p> <p>Originally taken from Michael Hall's tbpore https://github.com/mbhall88/tbpore/blob/main/tbpore/external_tools.py</p> <p>Also used by a variety of other tools (Dnaapler, Plassembler, Pharokka)</p>"},{"location":"reference/#src.baktfold.databases.db.calc_md5_sum","title":"<code>calc_md5_sum(tarball_path, buffer_size=1024 * 1024)</code>","text":"<p>Calculate the MD5 checksum of the given file.</p> <p>Parameters:</p> Name Type Description Default <code>tarball_path</code> <code>Path</code> <p>The path to the file for which the MD5 checksum needs to be calculated.</p> required <code>buffer_size</code> <code>int</code> <p>The buffer size for reading the file.</p> <code>1024 * 1024</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The MD5 checksum of the file.</p> Source code in <code>src/baktfold/databases/db.py</code> <pre><code>def calc_md5_sum(tarball_path: Path, buffer_size: int = 1024 * 1024) -&gt; str:\n    \"\"\"\n    Calculate the MD5 checksum of the given file.\n\n    Args:\n        tarball_path (Path): The path to the file for which the MD5 checksum needs to be calculated.\n        buffer_size (int): The buffer size for reading the file.\n\n    Returns:\n        str: The MD5 checksum of the file.\n    \"\"\"\n\n    md5 = hashlib.md5()\n    with tarball_path.open(\"rb\") as fh:\n        data = fh.read(buffer_size)\n        while data:\n            md5.update(data)\n            data = fh.read(buffer_size)\n    return md5.hexdigest()\n</code></pre>"},{"location":"reference/#src.baktfold.databases.db.check_db_installation","title":"<code>check_db_installation(db_dir, foldseek_gpu)</code>","text":"<p>Check if the baktfold database is installed.</p> <p>Parameters:</p> Name Type Description Default <code>db_dir</code> <code>Path</code> <p>The directory where the database is installed.</p> required <code>foldseek_gpu</code> <code>bool</code> <p>Whether to install foldseek-gpu compatible baktfold db</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if all required files are present, False otherwise.</p> Source code in <code>src/baktfold/databases/db.py</code> <pre><code>def check_db_installation(db_dir: Path, foldseek_gpu: bool) -&gt; bool:\n    \"\"\"\n    Check if the baktfold database is installed.\n\n    Args:\n        db_dir Path: The directory where the database is installed.\n        foldseek_gpu bool: Whether to install foldseek-gpu compatible baktfold db\n\n    Returns:\n        bool: True if all required files are present, False otherwise.\n    \"\"\"\n    downloaded_flag = True\n    for file_name in BAKTFOLD_DB_NAMES:\n        path = Path(db_dir) / file_name\n        if not path.is_file():\n            logger.warning(f\"baktfold Database file {path} is missing\")\n            downloaded_flag = False\n            break\n\n    gpu_flag = True\n    if foldseek_gpu:\n        for file_name in baktfold_DB_FOLDSEEK_GPU_NAMES:\n            path = Path(db_dir) / file_name\n            if not path.is_file():\n                logger.warning(f\"baktfold Foldseek-GPU Database file {path} is missing\")\n                gpu_flag = False\n                break \n\n    return downloaded_flag, gpu_flag\n</code></pre>"},{"location":"reference/#src.baktfold.databases.db.check_prostT5_download","title":"<code>check_prostT5_download(model_dir, model_name)</code>","text":"<p>Args:     model_dir (Path): Directory where the model and tokenizer is be stored.     model_name (str): Name of the pre-trained T5 model.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>bool to tell baktfold whether to download ProstT5</p> Source code in <code>src/baktfold/databases/db.py</code> <pre><code>def check_prostT5_download(model_dir: Path, model_name: str) -&gt; bool:\n    \"\"\"\n     Args:\n        model_dir (Path): Directory where the model and tokenizer is be stored.\n        model_name (str): Name of the pre-trained T5 model.\n    Returns:\n        bool: bool to tell baktfold whether to download ProstT5\n    \"\"\"\n\n    # assumes already has been downloaded\n    download = False\n\n    if model_name == \"Rostlab/ProstT5_fp16\":\n\n        model_sub_dir = \"models--Rostlab--ProstT5_fp16\"\n        DICT = PROSTT5_MD5_DICTIONARY\n\n\n    for key in DICT:\n        for nested_key in DICT[key]:\n            file_path = Path(\n                f\"{model_dir}/{model_sub_dir}/{key}/{nested_key}\"\n            )\n\n            # check file exists\n            if file_path.exists():\n                md5_sum = calc_md5_sum(file_path)\n                if md5_sum != DICT[key][nested_key]:\n                    logger.warning(\n                        f\"Corrupt model file {file_path}! MD5 should be '{DICT[key][nested_key]}' but is '{md5_sum}'\"\n                    )\n                    download = True\n            else:\n                logger.warning(f\"Model file {file_path} does not exist.\")\n                download = True\n\n    return download\n</code></pre>"},{"location":"reference/#src.baktfold.databases.db.download","title":"<code>download(tarball_path, cache_dir)</code>","text":"<p>Download the database from the given URL using HF.</p> <p>Parameters:</p> Name Type Description Default <code>tarball_path</code> <code>Path</code> <p>The path where the downloaded tarball should be saved.</p> required Source code in <code>src/baktfold/databases/db.py</code> <pre><code>def download(tarball_path: Path, cache_dir: Path) -&gt; None:\n    \"\"\"\n    Download the database from the given URL using HF.\n\n    Args:\n        tarball_path (Path): The path where the downloaded tarball should be saved.\n    \"\"\"\n\n    hf_tarball_path = hf_hub_download(\n        repo_id=\"gbouras13/baktfold-db\",\n        repo_type=\"dataset\",\n        filename=\"baktfold_db.tar.gz\"  ,\n        cache_dir=f\"{cache_dir}\"\n    )\n    # move from cache_dir to the base\n    # need to get the actual path not symlink\n\n    real_tarball = Path(hf_tarball_path).resolve()\n    tarball_path.parent.mkdir(parents=True, exist_ok=True)\n\n    shutil.move(real_tarball, tarball_path)\n\n    logger.info(f\"Tarball saved to {tarball_path}\")\n</code></pre>"},{"location":"reference/#src.baktfold.databases.db.download_requests","title":"<code>download_requests(db_url, tarball_path)</code>","text":"<p>Downloads a file from a given URL using the requests library.</p> <p>Parameters:</p> Name Type Description Default <code>db_url</code> <code>str</code> <p>The URL of the file to download.</p> required <code>tarball_path</code> <code>Path</code> <p>The path to save the downloaded file.</p> required <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; download_requests(\"https://zenodo.org/records/17347516/files/baktfold_db.tar.gz\", Path(\"baktfold_db.tar.gz\"))\n</code></pre> Source code in <code>src/baktfold/databases/db.py</code> <pre><code>def download_requests(db_url: str, tarball_path: Path):\n    \"\"\"\n    Downloads a file from a given URL using the requests library.\n\n    Args:\n      db_url (str): The URL of the file to download.\n      tarball_path (Path): The path to save the downloaded file.\n\n    Returns:\n      None\n\n    Examples:\n      &gt;&gt;&gt; download_requests(\"https://zenodo.org/records/17347516/files/baktfold_db.tar.gz\", Path(\"baktfold_db.tar.gz\"))\n    \"\"\"\n\n    headers = {\n        \"User-Agent\": f\"baktfold/{CURRENT_DB_VERSION} (contact: george.bouras@adelaide.edu.au)\"\n    }\n\n    try:\n        with tarball_path.open(\"wb\") as fh_out, requests.get(\n            db_url, stream=True, headers=headers\n        ) as resp:\n            total_length = resp.headers.get(\"content-length\")\n            if total_length is not None:  # content length header is set\n                total_length = int(total_length)\n            with alive_bar(total=total_length, scale=\"SI\") as bar:\n                for data in resp.iter_content(chunk_size=1024 * 1024):\n                    fh_out.write(data)\n                    bar(count=len(data))\n    except:\n        logger.error(\n            f\"ERROR: Could not download file from Zenodo! url={db_url}, path={tarball_path}\"\n        )\n</code></pre>"},{"location":"reference/#src.baktfold.databases.db.download_zenodo_prostT5","title":"<code>download_zenodo_prostT5(model_dir, logdir, threads)</code>","text":"<p>Download the ProstT5 model from Zenodo</p> <p>Parameters:</p> Name Type Description Default <code>db_url</code> <code>str</code> <p>The URL of the database.</p> required <code>tarball_path</code> <code>Path</code> <p>The path where the downloaded tarball should be saved.</p> required Source code in <code>src/baktfold/databases/db.py</code> <pre><code>def download_zenodo_prostT5(model_dir, logdir, threads):\n    \"\"\"\n    Download the ProstT5 model from Zenodo\n\n    Args:\n        db_url (str): The URL of the database.\n        tarball_path (Path): The path where the downloaded tarball should be saved.\n    \"\"\"\n\n    db_url = VERSION_DICTIONARY[CURRENT_DB_VERSION][\"prostt5_backup_url\"]\n    requiredmd5 = VERSION_DICTIONARY[CURRENT_DB_VERSION][\"prostt5_backup_md5\"]\n\n    logger.info(f\"Downloading ProstT5 model backup from {db_url}\")\n\n    tarball = VERSION_DICTIONARY[CURRENT_DB_VERSION][\"prostt5_backup_tarball\"]\n    tarball_path = Path(f\"{model_dir}/{tarball}\")\n    download_requests(db_url, tarball_path)\n\n    md5_sum = calc_md5_sum(tarball_path)\n\n    if md5_sum == requiredmd5:\n        logger.info(f\"ProstT5 model backup file download OK: {md5_sum}\")\n    else:\n        logger.error(\n            f\"Error: corrupt file! MD5 should be '{requiredmd5}' but is '{md5_sum}'\"\n        )\n\n    logger.info(\n        f\"Extracting ProstT5 model backup tarball: file={tarball_path}, output={model_dir}\"\n    )\n\n    try:\n        with tarball_path.open(\"rb\") as fh_in, tarfile.open(\n            fileobj=fh_in, mode=\"r:gz\"\n        ) as tar_file:\n            tar_file.extractall(path=str(model_dir))\n\n    except OSError:\n        logger.warning(\"Encountered OSError: {}\".format(OSError))\n        logger.error(f\"Could not extract {tarball_path} to {model_dir}\")\n\n    tarball_path.unlink()\n</code></pre>"},{"location":"reference/#src.baktfold.databases.db.foldseek_makepaddedseqdb","title":"<code>foldseek_makepaddedseqdb(db_dir)</code>","text":"<p>Runs the Foldseek makepaddedseqdb command on a given database directory.</p> <p>Parameters:</p> Name Type Description Default <code>db_dir</code> <code>Path</code> <p>The path to the database directory.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; foldseek_makepaddedseqdb(Path(\"baktfold_db\"))\n</code></pre> Source code in <code>src/baktfold/databases/db.py</code> <pre><code>def foldseek_makepaddedseqdb(db_dir: Path) -&gt; None:\n    \"\"\"\n    Runs the Foldseek makepaddedseqdb command on a given database directory.\n\n    Args:\n      db_dir (Path): The path to the database directory.\n\n    Returns:\n      None\n\n    Examples:\n      &gt;&gt;&gt; foldseek_makepaddedseqdb(Path(\"baktfold_db\"))\n    \"\"\"\n\n    dbs = [\"AFDBClusters\", \"pdb\", \"cath\", \"swissprot\"]\n    logdir = Path(db_dir) / \"logdir\"\n\n    for db_name in dbs:\n        db_path = Path(db_dir) / db_name\n        db_path_gpu = Path(db_dir) / f\"{db_name}_gpu\"\n\n        foldseek_makepaddedseqdb = ExternalTool(\n            tool=\"foldseek\",\n            input=\"\",\n            output=\"\",\n            params=f\"makepaddedseqdb {db_path} {db_path_gpu}\",\n            logdir=logdir,\n        )\n\n        ExternalTool.run_tool(foldseek_makepaddedseqdb)\n</code></pre>"},{"location":"reference/#src.baktfold.databases.db.install_database","title":"<code>install_database(db_dir, foldseek_gpu, threads)</code>","text":"<p>Install the baktfold database.</p> <p>Parameters:</p> Name Type Description Default <code>db_dir</code> <code>Path</code> <p>The directory where the database should be installed.</p> required <code>foldseek_gpu</code> <code>bool</code> <p>Whether to install foldseek-gpu compatible baktfold db</p> required <code>threads</code> <code>int</code> <p>Number of threads available (makes downloading faster)</p> required Source code in <code>src/baktfold/databases/db.py</code> <pre><code>def install_database(db_dir: Path, foldseek_gpu: bool, threads: int) -&gt; None:\n    \"\"\"\n    Install the baktfold database.\n\n    Args:\n        db_dir Path: The directory where the database should be installed.\n        foldseek_gpu bool: Whether to install foldseek-gpu compatible baktfold db\n        threads int: Number of threads available (makes downloading faster)\n    \"\"\"\n\n    # check the database is installed\n    logger.info(f\"Checking baktfold database installation in {db_dir}.\")\n    downloaded_flag, gpu_flag = check_db_installation(db_dir, foldseek_gpu)\n    if downloaded_flag:\n        logger.info(\"All baktfold databases files are present\")\n    else:\n        logger.info(\"Some baktfold databases files are missing\")\n\n        DICT = VERSION_DICTIONARY\n        db_url = DICT[CURRENT_DB_VERSION][\"db_url\"]\n        logger.info(f\"Downloading baktfold DB from {db_url}\")\n\n        requiredmd5s = DICT[CURRENT_DB_VERSION][\"md5\"]\n        tarball = DICT[CURRENT_DB_VERSION][\"tarball\"]\n\n        tarball_path = Path(f\"{db_dir}/{tarball}\")\n        logdir = Path(db_dir) / \"logdir\"\n\n        try: \n            logger.info(f\"Downloading from HuggingFace\")\n            download(tarball_path, db_dir)\n        except:\n            logger.warning(\n                f\"Could not download file from HuggingFace: path={tarball_path}\"\n            )\n            logger.warning(f\"Trying now with requests\")\n            download_requests(db_url, tarball_path)\n\n\n        md5_sum = calc_md5_sum(tarball_path)\n\n\n        if md5_sum in requiredmd5s:\n            logger.info(f\"baktfold database file download OK: {md5_sum}\")\n        else:\n            logger.error(\n                f\"Error: corrupt database file! MD5 should be '{requiredmd5s}' but is '{md5_sum}'\"\n            )\n\n        logger.info(\n            f\"Extracting baktfold database tarball: file={tarball_path}, output={db_dir}\"\n        )\n        untar(tarball_path, db_dir, DICT)\n        tarball_path.unlink()\n\n    if foldseek_gpu:\n        if gpu_flag:\n            logger.info(\"All baktfold database files compatible with Foldseek-GPU are present\")\n        else:\n            logger.info(\"Some baktfold database files compatible with Foldseek-GPU are missing\")\n            logger.info(\"Creating them\")\n            foldseek_makepaddedseqdb(db_dir)\n\n    logger.info(\"Database download and processing complete\")\n</code></pre>"},{"location":"reference/#src.baktfold.databases.db.untar","title":"<code>untar(tarball_path, output_path, DICT)</code>","text":"<p>Extract the tarball to the output path.</p> <p>Parameters:</p> Name Type Description Default <code>tarball_path</code> <code>Path</code> <p>The path to the tarball file.</p> required <code>output_path</code> <code>Path</code> <p>The path where the contents of the tarball should be extracted.</p> required <code>DICT</code> <code>dict</code> <p>version dictionary</p> required Source code in <code>src/baktfold/databases/db.py</code> <pre><code>def untar(tarball_path: Path, output_path: Path, DICT: dict) -&gt; None:\n    \"\"\"\n    Extract the tarball to the output path.\n\n    Args:\n        tarball_path (Path): The path to the tarball file.\n        output_path (Path): The path where the contents of the tarball should be extracted.\n        DICT (dict): version dictionary\n    \"\"\"\n    try:\n        with tarball_path.open(\"rb\") as fh_in, tarfile.open(\n            fileobj=fh_in, mode=\"r:gz\"\n        ) as tar_file:\n            tar_file.extractall(path=str(output_path))\n\n        tarpath = Path(output_path) / DICT[CURRENT_DB_VERSION][\"dir_name\"]\n\n        # Get a list of all files in the directory\n        files_to_move = [f for f in tarpath.iterdir() if f.is_file()]\n\n        # Move each file to the destination directory\n        for file_name in files_to_move:\n            destination_path = output_path / file_name.name\n            shutil.move(file_name, destination_path)\n        # remove the directory\n        remove_directory(tarpath)\n\n    except OSError:\n        logger.warning(\"Encountered OSError: {}\".format(OSError))\n        logger.error(f\"Could not extract {tarball_path} to {output_path}\")\n</code></pre>"},{"location":"reference/#src.baktfold.databases.db.validate_db","title":"<code>validate_db(database, default_dir, foldseek_gpu)</code>","text":"<p>Validates the baktfold database is installed.</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>str</code> <p>The directory where the database is installed.</p> required <code>default_dir</code> <code>str</code> <p>Default DB location</p> required <code>foldseek_gpu</code> <code>bool</code> <p>Whether to install foldseek-gpu compatible baktfold db</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>Path</code> <p>True if all required files are present, False otherwise.</p> Source code in <code>src/baktfold/databases/db.py</code> <pre><code>def validate_db(database: str, default_dir: str, foldseek_gpu: bool) -&gt; Path:\n    \"\"\"\n    Validates the baktfold database is installed.\n\n    Args:\n        database str: The directory where the database is installed.\n        default_dir str: Default DB location\n        foldseek_gpu bool: Whether to install foldseek-gpu compatible baktfold db\n\n    Returns:\n        bool: True if all required files are present, False otherwise.\n    \"\"\"\n    # set default DB if not specified\n    if database is not None:\n        database: Path = Path(database)\n    else:\n        database = Path(default_dir)\n\n    # check the database is installed\n    logger.info(f\"Checking baktfold database installation in {database}\")\n    downloaded_flag, gpu_flag = check_db_installation(database, foldseek_gpu)\n    if downloaded_flag == True:\n        logger.info(\"All baktfold databases files are present\")\n    else:\n        if database == Path(default_dir):  # default\n            logger.error(\n                f\"baktfold database not found. Please run baktfold install to download and install the baktfold database\"\n            )\n        else:  # specific\n            logger.error(\n                f\"baktfold database not found. Please run baktfold install -d {database} to download and install the baktfold database\"\n            )\n    if foldseek_gpu:\n        if gpu_flag:\n            logger.info(\"All baktfold database files compatible with Foldseek-GPU are present\")\n        else:\n            logger.error(\n                f\"baktfold database files compatible with Foldseek-GPU not found. Please run baktfold install -d {database} --foldseek_gpu\"\n            )\n\n\n    return database\n</code></pre>"},{"location":"reference/#src.baktfold.subcommands.predict.mask_low_confidence_aa","title":"<code>mask_low_confidence_aa(sequence, scores, threshold=0.5)</code>","text":"<p>Masks all low confidence AA to X if their corresponding ProstT5 confidence score is below the given threshold.</p> <p>sequence (str): The amino acid sequence. scores (List[float]): A list of confidence scores for each amino acid. threshold (float, optional): The confidence threshold below which amino acids are converted to lowercase. Default is 0.5.</p> <p>str: The modified amino acid sequence with low-confidence residues in lowercase.</p> Source code in <code>src/baktfold/subcommands/predict.py</code> <pre><code>def mask_low_confidence_aa(sequence, scores, threshold=0.5):\n    \"\"\"\n    Masks all low confidence AA to X if their corresponding ProstT5 confidence score is below the given threshold.\n\n    Parameters:\n    sequence (str): The amino acid sequence.\n    scores (List[float]): A list of confidence scores for each amino acid.\n    threshold (float, optional): The confidence threshold below which amino acids are converted to lowercase. Default is 0.5.\n\n    Returns:\n    str: The modified amino acid sequence with low-confidence residues in lowercase.\n    \"\"\"\n    return \"\".join('X' if float(score) &lt; threshold else aa \n                   for aa, score in zip(sequence, *scores))\n</code></pre>"},{"location":"reference/#src.baktfold.subcommands.predict.subcommand_predict","title":"<code>subcommand_predict(hypotheticals, cds_dict, output, prefix, cpu, omit_probs, model_dir, model_name, checkpoint_path, batch_size, save_per_residue_embeddings, save_per_protein_embeddings, threads, mask_threshold, has_duplicate_locus)</code>","text":"<p>Wrapper command for baktfold predict. Predicts embeddings using ProstT5 encoder + CNN prediction head.</p> <p>Parameters:</p> Name Type Description Default <code>hypotheticals</code> <code>Dict[str, any]</code> <p>feature dict for all Bakta hypothetical proteins</p> required <code>cds_dict</code> <code>Dict[str, any]</code> <p>id:aa dictionary</p> required <code>output</code> <code>str</code> <p>Output directory path.</p> required <code>prefix</code> <code>str</code> <p>Prefix for output file names.</p> required <code>cpu</code> <code>bool</code> <p>Flag indicating whether to use CPU for prediction.</p> required <code>omit_probs</code> <code>bool</code> <p>Flag indicating whether to omit prediction probabilities from ProstT5.</p> required <code>model_dir</code> <code>str</code> <p>Directory containing the ProstT5 model.</p> required <code>model_name</code> <code>str</code> <p>Name of the ProstT5 model.</p> required <code>checkpoint_path</code> <code>Path</code> <p>Path to ProstT5 CNN checkpoint.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for prediction.</p> required <code>proteins_flag</code> <code>bool</code> <p>True if baktfold proteins-predict, false otherwise</p> required <code>save_per_residue_embeddings</code> <code>bool</code> <p>Whether to save per residue embeddings to h5 file. Defaults to False.</p> required <code>save_per_protein_embeddings</code> <code>bool</code> <p>Whether to save mean per protein embeddings to h5 file. Defaults to False.</p> required <p>Returns:</p> Name Type Description <code>hypotheticals</code> <code>Dict[str, any]</code> <p>feature dict for all Bakta hypothetical proteins. Updated with ProstT5 3Di strings (unmasked)</p> Source code in <code>src/baktfold/subcommands/predict.py</code> <pre><code>def subcommand_predict(\n    hypotheticals: dict,\n    cds_dict: dict,\n    output: Path,\n    prefix: str,\n    cpu: bool,\n    omit_probs: bool,\n    model_dir: Path,\n    model_name: str,\n    checkpoint_path: Path,\n    batch_size: int,\n    save_per_residue_embeddings: bool,\n    save_per_protein_embeddings: bool,\n    threads: int,\n    mask_threshold: float,\n    has_duplicate_locus: bool\n) -&gt; bool:\n    \"\"\"\n    Wrapper command for baktfold predict. Predicts embeddings using ProstT5 encoder + CNN prediction head.\n\n    Args:\n        hypotheticals (Dict[str, any]): feature dict for all Bakta hypothetical proteins\n        cds_dict (Dict[str, any]): id:aa dictionary\n        output (str): Output directory path.\n        prefix (str): Prefix for output file names.\n        cpu (bool): Flag indicating whether to use CPU for prediction.\n        omit_probs (bool): Flag indicating whether to omit prediction probabilities from ProstT5.\n        model_dir (str): Directory containing the ProstT5 model.\n        model_name (str): Name of the ProstT5 model.\n        checkpoint_path (Path): Path to ProstT5 CNN checkpoint.\n        batch_size (int): Batch size for prediction.\n        proteins_flag (bool): True if baktfold proteins-predict, false otherwise\n        save_per_residue_embeddings (bool, optional): Whether to save per residue embeddings to h5 file. Defaults to False.\n        save_per_protein_embeddings (bool, optional): Whether to save mean per protein embeddings to h5 file. Defaults to False.\n\n    Returns:\n        hypotheticals (Dict[str, any]): feature dict for all Bakta hypothetical proteins. Updated with ProstT5 3Di strings (unmasked)\n    \"\"\"\n\n    logger.info('Predicting 3Di sequences using ProstT5')\n\n    fasta_aa: Path = Path(output) / f\"{prefix}_aa.fasta\"\n\n    ############\n    # prostt5\n    ############\n\n    fasta_3di: Path = Path(output) / f\"{prefix}_3di.fasta\"\n    # embeddings h5 - will only be generated if flag is true\n    output_h5_per_residue: Path = Path(output) / f\"{prefix}_embeddings_per_residue.h5\"\n    output_h5_per_protein: Path = Path(output) / f\"{prefix}_embeddings_per_protein.h5\"\n\n    if cpu is True:\n        half_precision = False\n    else:\n        half_precision = True\n\n    if omit_probs:\n        output_probs = False\n    else:\n        output_probs = True\n\n    prediction_dict = get_embeddings(\n        hypotheticals,\n        cds_dict,\n        output,\n        prefix,\n        model_dir,\n        model_name,\n        checkpoint_path,\n        fasta_3di,\n        output_h5_per_residue,\n        output_h5_per_protein,\n        half_precision=half_precision,\n        max_residues=5000,\n        max_seq_len=1000,\n        max_batch=batch_size,\n        cpu=cpu,\n        output_probs=output_probs,\n        save_per_residue_embeddings=save_per_residue_embeddings,\n        save_per_protein_embeddings=save_per_protein_embeddings,\n        threads=threads,\n        mask_threshold=mask_threshold,\n        has_duplicate_locus=has_duplicate_locus\n    )\n\n    mask_prop_threshold = mask_threshold/100\n\n    #######\n    # update the feature dict with 3Di \n    # easiest just \n    #######\n\n\n\n    # for feat in hypotheticals:\n    #     pred = prediction_dict.get(feat['locus']) # None if it doesn't exist\n    #     feat['3di'] = pred[2].tolist() if pred is not None else None\n\n    ########\n    ## write the AA CDS to file\n    ######\n\n\n    # check all the lengths of the predictions are &gt;0 in case of OOMs and filter out those that arent\n    prediction_dict = {\n                k: v for k, v in prediction_dict.items() if len(v[0]) &gt; 0\n            }\n\n\n    with open(fasta_aa, \"w+\") as out_f:\n        for cds_id, prot_seq in cds_dict.items():\n\n            out_f.write(f\"&gt;{cds_id}\\n\")\n\n                # prediction_contig_dict[seq_id][2] these are teh ProstT5 confidence scores from 0-1 - need to convert to list\n\n            try:\n                # this will fail if ProstT5 OOM fails (or fails for some other reason)\n                prot_seq = mask_low_confidence_aa(prot_seq, prediction_dict[cds_id][2].tolist(), threshold=mask_prop_threshold)\n            except (KeyError, IndexError):\n                # in that case, just return 'X' aka masked proteins\n                prot_seq = \"X\" * len(prot_seq)\n\n            out_f.write(f\"{prot_seq}\\n\")\n\n\n    return hypotheticals\n</code></pre>"},{"location":"reference/#src.baktfold.subcommands.compare.subcommand_compare","title":"<code>subcommand_compare(hypotheticals, output, threads, evalue, sensitivity, database, prefix, predictions_dir, structures, structure_dir, logdir, proteins_flag, max_seqs, ultra_sensitive, extra_foldseek_params, custom_db, foldseek_gpu, custom_annotations, has_duplicate_locus, fast)</code>","text":"<p>Compare 3Di or PDB structures to the baktfold DB</p> <p>Parameters:</p> Name Type Description Default <code>hypotheticals</code> <code>Dict</code> <p>hypothetical features dictionary</p> required <code>output</code> <code>Path</code> <p>Path to the output directory.</p> required <code>threads</code> <code>int</code> <p>Number of threads to use.</p> required <code>evalue</code> <code>float</code> <p>E-value threshold.</p> required <code>card_vfdb_evalue</code> <code>float</code> <p>E-value threshold for CARD and VFDB databases.</p> required <code>sensitivity</code> <code>float</code> <p>Sensitivity threshold.</p> required <code>database</code> <code>Path</code> <p>Path to the reference database.</p> required <code>prefix</code> <code>str</code> <p>Prefix for output files.</p> required <code>predictions_dir</code> <code>Optional[Path]</code> <p>Path to the directory containing predictions.</p> required <code>structures</code> <code>bool</code> <p>Flag indicating whether structures files are used.</p> required <code>structure_dir</code> <code>Optional[Path]</code> <p>Path to the directory containing structures (.pdb or .cif) files.</p> required <code>logdir</code> <code>Path</code> <p>Path to the directory for log files.</p> required <code>proteins_flag</code> <code>bool</code> <p>Flag indicating whether proteins are used.</p> required <code>max_seqs</code> <code>int</code> <p>Maximum results per query sequence allowed to pass the prefilter for foldseek.</p> required <code>ultra_sensitive</code> <code>bool</code> <p>Whether to skip foldseek prefilter for maximum sensitivity</p> required <code>extra_foldseek_params</code> <code>str</code> <p>Extra foldseek search parameters</p> required <code>custom_db</code> <code>str</code> <p>Custom foldseek database</p> required <code>foldseek_gpu</code> <code>bool</code> <p>Use Foldseek-GPU acceleration and ungappedprefilter</p> required <code>custom_annotations</code> <code>Optional[Path]</code> <p>Path to the tsv containing the custom_db annotations, 2 columns </p> required <code>has_duplicate_locus</code> <code>bool</code> <p>If same locus tag has multiple annots (can happen in some euks)</p> required <code>fast</code> <code>bool</code> <p>If true, skips AFDB search</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if sub-databases are created successfully, False otherwise.</p> Source code in <code>src/baktfold/subcommands/compare.py</code> <pre><code>def subcommand_compare(\n    hypotheticals: Dict,\n    output: Path,\n    threads: int,\n    evalue: float,\n    sensitivity: float,\n    database: Path,\n    prefix: str,\n    predictions_dir: Optional[Path],\n    structures: bool,\n    structure_dir: Optional[Path],\n    logdir: Path,\n    proteins_flag: bool,\n    max_seqs: int,\n    ultra_sensitive: bool,\n    extra_foldseek_params: str,\n    custom_db: str,\n    foldseek_gpu: bool,\n    custom_annotations: Optional[Path],\n    has_duplicate_locus: bool, \n    fast: bool\n) -&gt; bool:\n    \"\"\"\n    Compare 3Di or PDB structures to the baktfold DB\n\n    Parameters:\n        hypotheticals (Dict):  hypothetical features dictionary\n        output (Path): Path to the output directory.\n        threads (int): Number of threads to use.\n        evalue (float): E-value threshold.\n        card_vfdb_evalue (float): E-value threshold for CARD and VFDB databases.\n        sensitivity (float): Sensitivity threshold.\n        database (Path): Path to the reference database.\n        prefix (str): Prefix for output files.\n        predictions_dir (Optional[Path]): Path to the directory containing predictions.\n        structures (bool): Flag indicating whether structures files are used.\n        structure_dir (Optional[Path]): Path to the directory containing structures (.pdb or .cif) files.\n        logdir (Path): Path to the directory for log files.\n        proteins_flag (bool): Flag indicating whether proteins are used.\n        max_seqs (int): Maximum results per query sequence allowed to pass the prefilter for foldseek.\n        ultra_sensitive (bool): Whether to skip foldseek prefilter for maximum sensitivity\n        extra_foldseek_params (str): Extra foldseek search parameters\n        custom_db (str): Custom foldseek database\n        foldseek_gpu (bool): Use Foldseek-GPU acceleration and ungappedprefilter\n        custom_annotations (Optional[Path]): Path to the tsv containing the custom_db annotations, 2 columns \n        has_duplicate_locus (bool): If same locus tag has multiple annots (can happen in some euks)\n        fast (bool): If true, skips AFDB search\n    Returns:\n        bool: True if sub-databases are created successfully, False otherwise.\n    \"\"\"\n\n\n    # input predictions or structures\n    if structures is False:\n        # prostT5\n        fasta_aa_input: Path = Path(predictions_dir) / f\"{prefix}_aa.fasta\"\n        fasta_3di_input: Path = Path(predictions_dir) / f\"{prefix}_3di.fasta\"\n\n    fasta_aa: Path = Path(output) / f\"{prefix}_aa.fasta\"\n    fasta_3di: Path = Path(output) / f\"{prefix}_3di.fasta\"\n\n    ## copy the AA and 3Di from predictions directory \n    # if structures is false and baktfold compare is the command\n    # Otherwise it will just copy itself\n\n    if structures is False:\n        if fasta_3di_input.exists():\n            logger.info(\n                f\"Checked that the 3Di CDS file {fasta_3di_input} exists from baktfold predict\"\n            )\n            if fasta_3di.exists() is False:\n                shutil.copyfile(fasta_3di_input, fasta_3di)\n        else:\n            logger.error(\n                f\"The 3Di CDS file {fasta_3di_input} does not exist. Please run baktfold predict and/or check the prediction directory {predictions_dir}\"\n            )\n        # copy the aa to file\n        if fasta_aa_input.exists():\n            logger.info(\n                f\"Checked that the AA CDS file {fasta_aa_input} exists from baktfold predict.\"\n            )\n            if fasta_aa.exists() is False:\n                shutil.copyfile(fasta_aa_input, fasta_aa)\n        else:\n            logger.error(\n                f\"The AA CDS file {fasta_aa_input} does not exist. Please run baktfold predict and/or check the prediction directory {predictions_dir}\"\n                )\n\n    ## write the AAs to file if structures is true because can't just copy from prediction_dir\n    else:\n        ## write the CDS to file\n        logger.info(f\"Writing the AAs to file {fasta_aa}.\")\n\n        with open(fasta_aa, \"w+\") as out_f:\n            for entry in hypotheticals:\n                if has_duplicate_locus:\n                    header = f\"&gt;{entry['id']}\\n\"\n                else:\n                    header = f\"&gt;{entry['locus']}\\n\"\n                seq = f\"{entry['aa']}\\n\"\n                out_f.write(header)\n                out_f.write(seq)\n\n\n    ############\n    # create foldseek db\n    ############\n\n    foldseek_query_db_path: Path = Path(output) / \"foldseek_db\"\n    foldseek_query_db_path.mkdir(parents=True, exist_ok=True)\n\n    if structures is True:\n        logger.info(\"Creating a foldseek query database from structures.\")\n\n        generate_foldseek_db_from_structures(\n            fasta_aa,\n            foldseek_query_db_path,\n            structure_dir,\n            logdir,\n            prefix,\n            proteins_flag,\n        )\n    else:\n        generate_foldseek_db_from_aa_3di(\n            fasta_aa, fasta_3di, foldseek_query_db_path, logdir, prefix\n        )\n\n    short_db_name = prefix\n\n    # db search \n\n    database_name = \"swissprot\"\n\n    if short_db_name == database_name:\n        logger.error(\n            f\"Please choose a different -p {prefix} as this conflicts with the {database_name}\"\n        )\n\n    #####\n    # foldseek search\n    #####\n\n    query_db: Path = Path(foldseek_query_db_path) / short_db_name\n    target_db: Path = Path(database) / database_name\n\n    # make result and temp dirs\n    result_db_base: Path = Path(output) / \"result_db\"\n    result_db_base.mkdir(parents=True, exist_ok=True)\n    result_db: Path = Path(result_db_base) / \"result_db\"\n\n    temp_db: Path = Path(output) / \"temp_db\"\n    temp_db.mkdir(parents=True, exist_ok=True)\n\n    # make result tsv\n    result_tsv: Path = Path(output) / \"foldseek_results_swissprot.tsv\"\n\n    # run foldseek search\n    run_foldseek_search(\n        query_db,\n        target_db,\n        result_db,\n        temp_db,\n        threads,\n        logdir,\n        evalue,\n        sensitivity,\n        max_seqs,\n        ultra_sensitive,\n        extra_foldseek_params,\n        foldseek_gpu,\n        structures\n    )\n\n\n    create_result_tsv(query_db, target_db, result_db, result_tsv, logdir, foldseek_gpu, structures, threads)\n\n    swissprot_df = get_tophit(result_tsv, structures, cath=False)\n\n\n\n\n    #####\n    # foldseek search AFDB Clusters\n    # by default yes, but not if no fast\n    #####\n\n    if not fast:\n\n        database_name = \"AFDBClusters\"\n\n        if short_db_name == database_name:\n            logger.error(\n                f\"Please choose a different -p {prefix} as this conflicts with the {database_name}\"\n            )\n\n        query_db: Path = Path(foldseek_query_db_path) / short_db_name\n        target_db: Path = Path(database) / database_name\n\n        # make result and temp dirs\n        result_db_base: Path = Path(output) / \"result_db\"\n        result_db_base.mkdir(parents=True, exist_ok=True)\n        result_db: Path = Path(result_db_base) / \"result_afdb_db\"\n\n        temp_db: Path = Path(output) / \"temp_db\"\n        temp_db.mkdir(parents=True, exist_ok=True)\n\n        # make result tsv\n        result_tsv: Path = Path(output) / \"foldseek_results_afdb_clusters.tsv\"\n\n        # run foldseek search\n        run_foldseek_search(\n            query_db,\n            target_db,\n            result_db,\n            temp_db,\n            threads,\n            logdir,\n            evalue,\n            sensitivity,\n            max_seqs,\n            ultra_sensitive,\n            extra_foldseek_params,\n            foldseek_gpu,\n            structures\n        )\n\n\n        create_result_tsv(query_db, target_db, result_db, result_tsv, logdir, foldseek_gpu, structures, threads)\n\n        afdbclusters_df = get_tophit(result_tsv,structures, cath=False)\n\n    else:\n        logger.info(\"Skipping AFDB Clusters search as --fast specified.\")\n\n    #####\n    # foldseek search pdb\n    #####\n\n\n    database_name = \"pdb\"\n\n    if short_db_name == database_name:\n        logger.error(\n            f\"Please choose a different -p {prefix} as this conflicts with the {database_name}\"\n        )\n\n    query_db: Path = Path(foldseek_query_db_path) / short_db_name\n    target_db: Path = Path(database) / database_name\n\n    # make result and temp dirs\n    result_db_base: Path = Path(output) / \"result_db\"\n    result_db_base.mkdir(parents=True, exist_ok=True)\n    result_db: Path = Path(result_db_base) / \"result_pdb_db\"\n\n    temp_db: Path = Path(output) / \"temp_db\"\n    temp_db.mkdir(parents=True, exist_ok=True)\n\n    # make result tsv\n    result_tsv: Path = Path(output) / \"foldseek_results_pdb.tsv\"\n\n    # run foldseek search\n    run_foldseek_search(\n        query_db,\n        target_db,\n        result_db,\n        temp_db,\n        threads,\n        logdir,\n        evalue,\n        sensitivity,\n        max_seqs,\n        ultra_sensitive,\n        extra_foldseek_params,\n        foldseek_gpu,\n        structures\n    )\n\n\n    create_result_tsv(query_db, target_db, result_db, result_tsv, logdir, foldseek_gpu, structures, threads)\n\n    pdb_df = get_tophit(result_tsv,structures, cath=False)\n\n\n    #####\n    # foldseek search cath\n    #####\n\n\n    database_name = \"cath\"\n\n    if short_db_name == database_name:\n        logger.error(\n            f\"Please choose a different -p {prefix} as this conflicts with the {database_name}\"\n        )\n\n    query_db: Path = Path(foldseek_query_db_path) / short_db_name\n    target_db: Path = Path(database) / database_name\n\n    # make result and temp dirs\n    result_db_base: Path = Path(output) / \"result_db\"\n    result_db_base.mkdir(parents=True, exist_ok=True)\n    result_db: Path = Path(result_db_base) / \"result_cath_db\"\n    result_db_greedy_best_hits: Path = Path(result_db_base) / \"result_cath_db_greedy_best_hits\"\n\n    temp_db: Path = Path(output) / \"temp_db\"\n    temp_db.mkdir(parents=True, exist_ok=True)\n\n    # make result tsv\n    result_tsv: Path = Path(output) / \"foldseek_results_cath.tsv\"\n    result_greedy_tsv: Path = Path(output) /  \"foldseek_results_cath_greedy_tophit\"\n\n    # run foldseek search\n    run_foldseek_search(\n        query_db,\n        target_db,\n        result_db,\n        temp_db,\n        threads,\n        logdir,\n        evalue,\n        sensitivity,\n        max_seqs,\n        ultra_sensitive,\n        extra_foldseek_params,\n        foldseek_gpu,\n        structures\n    )\n\n    # this keeps the greedy best hits for cath\n    # we actually don't keep the single tophit - multidomain/fold proteins should have multiple non-overlapping CATH hits\n    # this is equivalent to using --greedy-best-hits with foldseek easy-search\n    summarise_hits(result_db, result_db_greedy_best_hits, logdir, threads)\n\n    # saves all CATH hits first\n    create_result_tsv(query_db, target_db, result_db, result_tsv, logdir, foldseek_gpu, structures, threads)\n    # save greedy CATH tophits\n    create_result_tsv(query_db, target_db, result_db_greedy_best_hits, result_greedy_tsv, logdir, foldseek_gpu, structures, threads)\n\n    # this just reads it in with appropriate headers\n    cath_df = get_tophit(result_greedy_tsv, structures, cath=True)\n\n    # write tophits\n    swissprot_tophit_path: Path = Path(output) / \"baktfold_swissprot_tophit.tsv\"\n    io.write_foldseek_tophit(swissprot_df, swissprot_tophit_path)\n\n    if not fast:\n        afdb_tophit_path: Path = Path(output) / \"baktfold_afdbclusters_tophit.tsv\"\n        io.write_foldseek_tophit(afdbclusters_df, afdb_tophit_path)\n\n    pdb_tophit_path: Path = Path(output) / \"baktfold_pdb_tophit.tsv\"\n    io.write_foldseek_tophit(pdb_df, pdb_tophit_path)\n\n    cath_tophit_path: Path = Path(output) / \"baktfold_cath_tophit.tsv\"\n    io.write_foldseek_tophit(cath_df, cath_tophit_path)\n    # remove result_greedy_tsv (identical to tophit, will make it confusing)\n    remove_file(result_greedy_tsv) \n\n    # custom db output \n\n    #####\n    # custom db\n    #####\n\n\n    if custom_db:\n\n        try:\n\n            logger.info(f\"Foldseek will also be run against your custom database {custom_db}\")\n            # make result and temp dirs\n            result_db_custom: Path = Path(result_db_base) / \"result_db_custom\"\n            result_tsv_custom: Path = Path(output) / \"foldseek_results_custom.tsv\"\n\n            run_foldseek_search(\n            query_db,\n            Path(custom_db),\n            result_db_custom,\n            temp_db,\n            threads,\n            logdir,\n            evalue,\n            sensitivity,\n            max_seqs,\n            ultra_sensitive,\n            extra_foldseek_params,\n            foldseek_gpu,\n            structures\n        )\n\n            create_result_tsv(query_db, Path(custom_db),\n                result_db_custom,\n                result_tsv_custom, logdir, foldseek_gpu, structures, threads)\n\n            custom_df = get_tophit(result_tsv_custom,structures, cath=False)\n\n            custom_db_tophit_path: Path = Path(output) / \"baktfold_custom_db_tophit.tsv\"\n            io.write_foldseek_tophit(custom_df, custom_db_tophit_path)\n\n        except:\n            logger.error(f\"Foldseek failed to run against your custom database {custom_db}. Please check that it is formatted correctly as a Foldseek database\")\n\n\n    ####\n    # lookup\n    ####\n\n    if proteins_flag: # baktfold proteins \n\n        # note aas passed as hypotheticals to the overall function - so in and out as aas\n\n        aas = pstc.parse(hypotheticals, swissprot_df, 'swissprot', has_duplicate_locus=False)\n        if not fast:\n            aas = pstc.parse(aas, afdbclusters_df, 'afdb', has_duplicate_locus=False)\n        aas = pstc.parse(aas, pdb_df, 'pdb', has_duplicate_locus=False)\n        aas = pstc.parse(aas, cath_df, 'cath', has_duplicate_locus=False)\n        if custom_db:\n            aas = pstc.parse(aas, custom_df, 'custom_db', has_duplicate_locus=False)\n\n        # get the lookup descriptions for each of them\n        # this requires the DB\n\n        #aas = pstc.lookup(aas, Path(database), custom_annotations)\n        aas = pstc.lookup_sql(aas, Path(database), threads)\n        # add the custom annotations if it is provided\n        if custom_annotations:\n            aas = pstc.lookup_custom(aas, Path(database), custom_annotations)\n\n        return aas\n\n    else: # baktfold run\n\n        # add the Swissprot and AFDB and PDB tophits to the json\n        hypotheticals = pstc.parse(hypotheticals, swissprot_df, 'swissprot', has_duplicate_locus)\n        if not fast:\n            hypotheticals = pstc.parse(hypotheticals, afdbclusters_df, 'afdb', has_duplicate_locus)\n        hypotheticals = pstc.parse(hypotheticals, pdb_df, 'pdb', has_duplicate_locus)\n        hypotheticals = pstc.parse(hypotheticals, cath_df, 'cath', has_duplicate_locus)\n        if custom_db:\n            hypotheticals = pstc.parse(hypotheticals, custom_df, 'custom_db', has_duplicate_locus)\n\n        # get the lookup descriptions for each of them\n        # hypotheticals = pstc.lookup(hypotheticals, Path(database), custom_annotations)\n        hypotheticals = pstc.lookup_sql(hypotheticals, Path(database), threads)\n        if custom_annotations:\n            hypotheticals = pstc.lookup_custom(hypotheticals, Path(database), custom_annotations)\n\n        return hypotheticals\n</code></pre>"},{"location":"reference/#src.baktfold.io.insdc.move_product_to_note_if_exists","title":"<code>move_product_to_note_if_exists(qualifiers)</code>","text":"<p>If a 'product' qualifier exists, append it to 'note' and remove 'product'.</p> <p>Designed for the eukaryotic entries</p>"},{"location":"reference/#src.baktfold.io.insdc.move_product_to_note_if_exists--parameters","title":"Parameters","text":"dict <p>Feature qualifiers dictionary (values are usually lists).</p>"},{"location":"reference/#src.baktfold.io.insdc.move_product_to_note_if_exists--returns","title":"Returns","text":"<p>None     Modifies qualifiers in place.</p> Source code in <code>src/baktfold/io/insdc.py</code> <pre><code>def move_product_to_note_if_exists(qualifiers):\n    \"\"\"\n    If a 'product' qualifier exists, append it to 'note' and remove 'product'.\n\n    Designed for the eukaryotic entries\n\n    Parameters\n    ----------\n    qualifiers : dict\n        Feature qualifiers dictionary (values are usually lists).\n\n    Returns\n    -------\n    None\n        Modifies qualifiers in place.\n    \"\"\"\n    product = qualifiers.get(\"product\")\n    if not product:\n        return\n\n    # Ensure note exists and is a list\n    if \"note\" not in qualifiers:\n        qualifiers[\"note\"] = []\n\n    if isinstance(product, list):\n        qualifiers[\"note\"].extend(product)\n    else:\n        qualifiers[\"note\"].append(product)\n\n    qualifiers.pop(\"product\", None)\n</code></pre>"},{"location":"reference/#src.baktfold.io.insdc.revise_dbxref_insdc","title":"<code>revise_dbxref_insdc(dbxrefs)</code>","text":"<p>Remove INSDC non-compliant DbXrefs.</p> Source code in <code>src/baktfold/io/insdc.py</code> <pre><code>def revise_dbxref_insdc(dbxrefs: Sequence[str]) -&gt; Tuple[Sequence[str], Sequence[str]]:\n    \"\"\"Remove INSDC non-compliant DbXrefs.\"\"\"\n    insdc_valid_dbxrefs = [bc.DB_XREF_UNIPROTKB, bc.DB_XREF_GO, bc.DB_XREF_PFAM, bc.DB_XREF_RFAM]\n    valid_dbxrefs = []\n    invalid_dbxrefs = []\n    for dbxref in dbxrefs:\n        if(dbxref.split(':')[0] in insdc_valid_dbxrefs):\n            valid_dbxrefs.append(dbxref)\n        else:\n            invalid_dbxrefs.append(dbxref)\n    return valid_dbxrefs, invalid_dbxrefs\n</code></pre>"},{"location":"reference/#src.baktfold.io.insdc.revise_product_insdc","title":"<code>revise_product_insdc(product)</code>","text":"<p>Revise product name for INSDC compliant submissions</p> Source code in <code>src/baktfold/io/insdc.py</code> <pre><code>def revise_product_insdc(product: str):\n    \"\"\"Revise product name for INSDC compliant submissions\"\"\"\n\n    old_product = product\n    if(re.search(r'(uncharacteri[sz]ed)', product, flags=re.IGNORECASE)):  # replace putative synonyms)\n        product = re.sub(r'(uncharacteri[sz]ed)', 'putative', product, flags=re.IGNORECASE)\n        logger.info('fix product: replace putative synonyms. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    if(product.count('(') != product.count(')')):  # remove unbalanced parentheses\n        product = product.replace('(', '').replace(')', '')  # ToDo: find and replace only legend parentheses\n        logger.info('fix product: remove unbalanced parantheses. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    if(product.count('[') != product.count(']')):  # remove unbalanced brackets\n        product = product.replace('[', '').replace(']', '')  # ToDo: find and replace only legend bracket\n        logger.info('fix product: remove unbalanced brackets. new=%s, old=%s', product, old_product)\n\n    return product\n</code></pre>"},{"location":"reference/#src.baktfold.io.json_in.parse_json_input","title":"<code>parse_json_input(input_path, faa_path, all_proteins)</code>","text":"<p>Parses genome annotations from input JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to input JSON file.</p> required <code>faa_path</code> <code>str</code> <p>Path to output file for hypothetical proteins.</p> required <code>all_proteins</code> <code>bool</code> <p>Whether to keep all proteins or only hypothetical ones.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the data, features, and whether there are duplicate locus tags.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; parse_json_input('input.json', 'hypotheticals.faa', False)\n(data, features, False)\n</code></pre> Source code in <code>src/baktfold/io/json_in.py</code> <pre><code>def parse_json_input(input_path, faa_path, all_proteins):\n    \"\"\"\n    Parses genome annotations from input JSON file.\n\n    Args:\n      input_path (str): Path to input JSON file.\n      faa_path (str): Path to output file for hypothetical proteins.\n      all_proteins (bool): Whether to keep all proteins or only hypothetical ones.\n\n    Returns:\n      tuple: A tuple containing the data, features, and whether there are duplicate locus tags.\n\n    Examples:\n      &gt;&gt;&gt; parse_json_input('input.json', 'hypotheticals.faa', False)\n      (data, features, False)\n    \"\"\"\n\n    ############################################################################\n    # Checks and configurations\n    # - check parameters and setup global configuration\n    # - test database\n    # - test binary dependencies\n    ############################################################################\n\n    try:\n        if input_path == '':\n            raise ValueError('File path argument must be non-empty')\n        annotation_path = Path(input_path).resolve()\n        cfg.check_readability('annotation', annotation_path)\n        cfg.check_content_size('annotation', annotation_path)\n    except:\n        logger.error(f'ERROR: annotation file {annotation_path} not valid!')\n\n    #print(f'baktfold v{cfg.version}')\n\n    logger.info(f'Parsing genome annotations from input: {annotation_path}')\n    with xopen(str(annotation_path), threads=0) as fh:\n        data = json.load(fh)\n    features = data['features']\n    features_by_sequence = {seq['id']: [] for seq in data['sequences']}\n    for feature in data['features']:\n        seq_id = feature['sequence'] if 'sequence' in feature else feature['contig']  # &lt;1.10.0 compatibility\n        sequence_features = features_by_sequence.get(seq_id)\n        sequence_features.append(feature)\n\n    # keep all proteins\n    if all_proteins:\n        hypotheticals = [feat for feat in features if feat['type'] == bc.FEATURE_CDS ]\n    else:\n\n        hypotheticals = [feat for feat in features if feat['type'] == bc.FEATURE_CDS and 'hypothetical' in feat]\n\n\n    # check if dupe locus tags (euks can have multiple CDS same locus tag e.g. Cladocopium goreaui CAMXCT020000001.1)\n    seen_loci = set()\n    has_duplicate_locus = False\n\n    for feat in hypotheticals:\n        locus = feat['locus']\n        if locus in seen_loci:\n            has_duplicate_locus = True\n            logger.warning(\"Multiple CDS per locus tag were detected in your input JSON.\")\n            logger.warning(\"CDS id (which is unique) rather than locus tag will be used for ProstT5+Foldseek searches.\")\n            break\n        seen_loci.add(locus)\n\n    if has_duplicate_locus:\n        # write hypothetical proteins to file with id (not locus) as guaranteed exists and unique\n        with faa_path.open('wt') as fh:\n            for feat in hypotheticals:\n                fh.write(f\"&gt;{feat['id']}\\n{feat['aa']}\\n\")\n\n    else:\n        # write hypothetical proteins to file - almost always\n        with faa_path.open('wt') as fh:\n            for feat in hypotheticals:\n                fh.write(f\"&gt;{feat['locus']}\\n{feat['aa']}\\n\")\n\n    try:\n        genome_block = data.get(\"genome\")\n\n        if genome_block is None:\n            logger.error(\"No 'genome' block found in input JSON. Please check.\")\n            translation_table = None\n        else:\n            if \"translation_table\" not in genome_block:\n                logger.error(\"No translation table found in input JSON. Please check your input.\")\n            else:\n                raw_value = genome_block[\"translation_table\"]\n\n                try:\n                    translation_table = int(raw_value)\n                    logger.info(\n                        f\"Translation table {translation_table} detected from input JSON\"\n                    )\n\n                except (ValueError, TypeError):\n                    translation_table = str(raw_value)\n                    logger.warning(\n                        f\"Translation table '{raw_value}' is not an integer. \"\n                        f\"Parsing it as a string.\"\n                    )\n\n    except Exception as e:\n        logger.exception(\n            f\"Unexpected error while parsing translation table: {e}\"\n        )\n        translation_table = None\n\n    logger.info('Parsing complete')\n\n    return data, features, has_duplicate_locus, translation_table\n</code></pre>"},{"location":"reference/#src.baktfold.io.handle_genbank.get_fasta_run_pyrodigal_gv","title":"<code>get_fasta_run_pyrodigal_gv(input, threads)</code>","text":"<p>Check if a file is in the nucleotide FASTA format. If so, run pyrodigal-gv and convert the CDS to a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Path</code> <p>Path to the FASTA file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the CDS in the FASTA file.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided file is not a FASTA file.</p> Source code in <code>src/baktfold/io/handle_genbank.py</code> <pre><code>def get_fasta_run_pyrodigal_gv(input: Path, threads: int) -&gt; dict:\n    \"\"\"\n\n    Check if a file is in the nucleotide FASTA format. If so, run pyrodigal-gv and convert the CDS to a dictionary.\n\n    Args:\n        input (Path): Path to the FASTA file.\n\n    Returns:\n        dict: A dictionary representation of the CDS in the FASTA file.\n\n    Raises:\n        ValueError: If the provided file is not a FASTA file.\n    \"\"\"\n\n    if is_gzip_file(input.strip()):\n        try:\n            with gzip.open(input.strip(), \"rt\") as handle:\n                # gb_dict = SeqIO.to_dict(SeqIO.parse(handle, \"gb\"))\n                fasta_dict = SeqIO.to_dict(SeqIO.parse(handle, \"fasta\"))\n        except ValueError:\n            logger.warning(f\"{input} is not a FASTA file\")\n            logger.error(\n                f\"Your input {input} is neither Genbank nor FASTA format. Please check your input\"\n            )\n            raise\n    else:\n        try:\n            with open(input.strip(), \"rt\") as handle:\n                fasta_dict = SeqIO.to_dict(SeqIO.parse(handle, \"fasta\"))\n        except ValueError:\n            logger.warning(f\"{input} is not a FASTA file\")\n            logger.error(\n                f\"Your input {input} is neither Genbank nor FASTA format. Please check your input\"\n            )\n            raise\n\n    # then run pyrodigal\n\n    gb_dict = {}\n\n    orf_finder = pyrodigal_gv.ViralGeneFinder(meta=True)\n\n    def _find_genes(record):\n        \"\"\"\n    Finds all genes in a given SeqRecord.\n\n    Args:\n      record (SeqRecord): The SeqRecord to search for genes in.\n\n    Returns:\n      list: A list of SeqFeatures representing the genes found in the SeqRecord.\n\n    Examples:\n      &gt;&gt;&gt; _find_genes(SeqRecord(seq=Seq('ATGC'), id='example', name='example', description='example', dbxrefs=[]))\n      [SeqFeature(FeatureLocation(ExactPosition(0), ExactPosition(3), strand=1), type='gene', qualifiers={'locus_tag': ['example_1']}), ...]\n    \"\"\"\n        genes = orf_finder.find_genes(str(record.seq))\n        return (record.id, record.seq, genes)\n\n    def run_pool(pool, records):\n        \"\"\"\n    Runs a multiprocessing pool to process a list of SeqRecords.\n\n    Args:\n      records (list): A list of SeqRecords to process.\n      func (function): The function to apply to each SeqRecord.\n      num_processes (int): The number of processes to use in the pool. Defaults to the number of CPUs on the system.\n\n    Returns:\n      list: A list of results from applying the function to each SeqRecord.\n\n    Examples:\n      &gt;&gt;&gt; run_pool([SeqRecord(seq=Seq('ATGC'), id='example', name='example', description='example', dbxrefs=[]), ...], _find_genes, 4)\n      [[SeqFeature(FeatureLocation(ExactPosition(0), ExactPosition(3), strand=1), type='gene', qualifiers={'locus_tag': ['example_1']}), ...], ...]\n    \"\"\"\n        for record_id, record_seq, genes in pool.imap(_find_genes, records):\n            i = 0\n            all_features = []\n            for gene in genes:\n                i += 1\n                location = FeatureLocation(\n                    start=gene.begin, end=gene.end, strand=gene.strand\n                )\n                feature = SeqFeature(location, type=\"CDS\")\n                counter = \"{:04d}\".format(i)\n                cds_id = f\"{record_id}_CDS_\" + counter\n                feature.qualifiers[\"ID\"] = cds_id\n                feature.qualifiers[\"function\"] = \"unknown function\"\n                feature.qualifiers[\"product\"] = \"hypothetical protein\"\n                feature.qualifiers[\"phrog\"] = \"No_PHROG\"\n                feature.qualifiers[\"source\"] = (\n                    f\"Pyrodigal-gv_{pyrodigal_gv.__version__}\"\n                )\n                feature.qualifiers[\"transl_table\"] = gene.translation_table\n                # from the API\n                # translation_table (int, optional) \u2013 An alternative translation table to use to translate the gene.\n                # Use None (the default) to translate using the translation table this gene was found with.\n                feature.qualifiers[\"translation\"] = gene.translate(\n                    include_stop=False\n                ).upper()\n                all_features.append(feature)\n\n            seq_record = SeqIO.SeqRecord(\n                seq=Seq(record_seq), id=record_id, description=\"\", features=all_features\n            )\n            gb_dict[record_id] = seq_record\n\n        return gb_dict\n\n    with multiprocessing.pool.ThreadPool(threads) as pool:\n        if is_gzip_file(input.strip()):\n            with gzip.open(input.strip(), \"rt\") as handle:\n                records = SeqIO.parse(handle, \"fasta\")\n                gb_dict = run_pool(pool, records)\n        else:\n            with open(input.strip(), \"rt\") as handle:\n                records = SeqIO.parse(handle, \"fasta\")\n                gb_dict = run_pool(pool, records)\n\n    return gb_dict\n</code></pre>"},{"location":"reference/#src.baktfold.io.handle_genbank.get_genbank","title":"<code>get_genbank(genbank)</code>","text":"<p>Convert a GenBank file to a dictionary.</p> <p>This function reads a GenBank file and converts it into a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>genbank</code> <code>Path</code> <p>Path to the GenBank file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the GenBank file.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided file is not a GenBank file.</p> Source code in <code>src/baktfold/io/handle_genbank.py</code> <pre><code>def get_genbank(genbank: Path) -&gt; dict:\n    \"\"\"\n    Convert a GenBank file to a dictionary.\n\n    This function reads a GenBank file and converts it into a dictionary.\n\n    Args:\n        genbank (Path): Path to the GenBank file.\n\n    Returns:\n        dict: A dictionary representation of the GenBank file.\n\n    Raises:\n        ValueError: If the provided file is not a GenBank file.\n    \"\"\"\n\n    logger.info(f\"Checking if input {genbank} is a Genbank format file\")\n    logger.info(f\"If so, also detecting the likely input style out of Pharokka, Bakta and NCBI Refseq style.\")\n    def parse_records(handle):\n        \"\"\"\n    Parses a genbank file and returns a list of SeqRecords.\n\n    Args:\n      file_path (str): The path to the genbank file to parse.\n      file_format (str): The format of the genbank file. Defaults to 'genbank'.\n\n    Returns:\n      list: A list of SeqRecords parsed from the genbank file.\n\n    Examples:\n      &gt;&gt;&gt; parse_records('example.gb')\n      [SeqRecord(seq=Seq('ATGC'), id='example', name='example', description='example', dbxrefs=[]), ...]\n    \"\"\"\n        try:\n            records = list(SeqIO.parse(handle, \"gb\"))\n            if not records:\n                return {}, None\n            gb_dict = {record.id: record for record in records}\n            record = records[0]\n\n            comment = record.annotations.get(\"comment\", \"\")\n            cds_feature = next((f for f in record.features if f.type == \"CDS\"), None)\n\n            if cds_feature is None:\n                logger.error(f\"{genbank} appears to be a Genbank formatted file but no CDS was found. Please check your input.\")\n                return gb_dict, None\n\n            # Check if 'Bakta' appears in the Comment - will appear there\n            if \"Bakta\" in comment and \"locus_tag\" in cds_feature.qualifiers:\n                logger.info(f\"Detected Bakta style input Genbank. Using locus_tag qualifier from Bakta as the CDS IDs for Phold.\")\n                method = \"Bakta\"\n            else:\n                if \"phrog\" not in cds_feature.qualifiers and \"protein_id\" in cds_feature.qualifiers:\n                    logger.info(f\"Detected NCBI Refseq style input Genbank. Using protein_id qualifier as the CDS IDs for Phold.\")\n                    method = \"NCBI\"\n                elif \"phrog\" in cds_feature.qualifiers and \"ID\" in cds_feature.qualifiers:\n                    logger.info(f\"Detected Pharokka style input Genbank. Using ID qualifier from Pharokka as the CDS IDs for Phold.\")\n                    method = \"Pharokka\"\n                else:\n                    logger.error(\n                                f\"Feature {cds_feature} could not be parsed. Therefore, the input style format for {genbank} could not be detected. Please check your input.\"\n                            )\n            return identify_long_ids(gb_dict), method\n        except Exception as e:\n            logger.warning(f\"{genbank} is not a genbank file\")\n            return {}, None\n\n    try:\n        if is_gzip_file(genbank.strip()):\n            with gzip.open(genbank.strip(), \"rt\") as handle:\n                return parse_records(handle)\n        else:\n            with open(genbank.strip(), \"rt\") as handle:\n                return parse_records(handle)\n    except Exception as e:\n        logger.warning(f\"{genbank} is not a genbank file\")\n        return {}, None\n</code></pre>"},{"location":"reference/#src.baktfold.io.handle_genbank.get_proteins","title":"<code>get_proteins(fasta)</code>","text":"<p>Convert an Amino Acid FASTA file to a dictionary.</p> <p>This function reads a AA FASTA file and converts it into a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fasta</code> <code>Path</code> <p>Path to the FASTA file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the FASTA file.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided file is not a FASTA file.</p> Source code in <code>src/baktfold/io/handle_genbank.py</code> <pre><code>def get_proteins(fasta: Path) -&gt; dict:\n    \"\"\"\n    Convert an Amino Acid FASTA file to a dictionary.\n\n    This function reads a AA FASTA file and converts it into a dictionary.\n\n    Args:\n        fasta (Path): Path to the FASTA file.\n\n    Returns:\n        dict: A dictionary representation of the FASTA file.\n\n    Raises:\n        ValueError: If the provided file is not a FASTA file.\n    \"\"\"\n\n    if is_gzip_file(fasta.strip()):\n        try:\n            fasta_dict = {}\n            with gzip.open(fasta.strip(), \"rt\") as handle:\n                sequence_id = \"\"\n                sequence = \"\"\n                for line in handle:\n                    line = line.strip()\n                    if line.startswith(\"&gt;\"):\n                        if sequence_id:\n                            fasta_dict[sequence_id] = sequence\n                        sequence_id = line[1:]\n                        sequence = \"\"\n                    else:\n                        sequence += line\n                if sequence_id:\n                    fasta_dict[sequence_id] = sequence\n            handle.close()\n        except ValueError:\n            logger.error(f\"{fasta.strip()} is not a FASTA file!\")\n            raise\n\n    else:\n        try:\n            fasta_dict = {}\n            with open(fasta.strip(), \"rt\", errors=\"ignore\") as handle:\n                sequence_id = \"\"\n                sequence = \"\"\n                for line in handle:\n                    line = line.strip()\n                    if line.startswith(\"&gt;\"):\n                        if sequence_id:\n                            fasta_dict[sequence_id] = sequence\n                        sequence_id = line[1:]\n                        sequence = \"\"\n                    else:\n                        sequence += line\n                if sequence_id:\n                    fasta_dict[sequence_id] = sequence\n            handle.close()\n        except ValueError:\n            logger.error(f\"{fasta.strip()} is not a FASTA file!\")\n            raise\n\n    return fasta_dict\n</code></pre>"},{"location":"reference/#src.baktfold.io.handle_genbank.identify_long_ids","title":"<code>identify_long_ids(gb_dict)</code>","text":"<p>Checks all feature IDs in gb_dict. If longer than 54 chars (line break from Pharokka/biopython reading GBK files), removes the space</p> <p>Parameters:</p> Name Type Description Default <code>dict</code> <p>A dictionary representation of the GenBank file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the GenBank file.</p> Source code in <code>src/baktfold/io/handle_genbank.py</code> <pre><code>def identify_long_ids(gb_dict: dict) -&gt; dict:\n    \"\"\"\n\n    Checks all feature IDs in gb_dict. If longer than 54 chars (line break from Pharokka/biopython reading GBK files), removes the space\n\n    Args:\n        dict: A dictionary representation of the GenBank file.\n\n    Returns:\n        dict: A dictionary representation of the GenBank file.\n    \"\"\"\n\n    # remove spaces in ID/locus tag\n    for record_id, record in gb_dict.items():\n        for cds_feature in record.features:\n            try:\n                # if pharokka &gt; 54 char IDs/locus tage, phold/biopython will parse with a space\n                # no spaces in\n                # for really long CDS IDs (over 54 chars), a space will be introduced\n                # this is because the ID will go over a second line\n                # weird bug noticed it on the Mgnify contigs annotated with Pharokka\n                cds_id = cds_feature.qualifiers[\"ID\"][0]\n                if len(cds_id) &gt;= 54:\n                    logger.warning(\n                        f\"The CDS ID is {cds_id} is longer than 54 characters. It is recommended that you use short contig headers (which will therefore lead to shorter CDS ids).\"\n                    )\n                    cds_feature.qualifiers[\"ID\"][0] = cds_feature.qualifiers[\"ID\"][\n                        0\n                    ].replace(\" \", \"\")\n            except:\n                # will be GenBank/NCBI formatted\n                # ID isn't a field and should be properly formatted - famous last words probably\n                continue\n\n    return gb_dict\n</code></pre>"},{"location":"reference/#src.baktfold.io.handle_genbank.is_gzip_file","title":"<code>is_gzip_file(f)</code>","text":"<p>Method copied from Phispy see https://github.com/linsalrob/PhiSpy/blob/master/PhiSpyModules/helper_functions.py</p> <p>This is an elegant solution to test whether a file is gzipped by reading the first two characters. I also use a version of this in fastq_pair if you want a C version :) See https://stackoverflow.com/questions/3703276/how-to-tell-if-a-file-is-gzip-compressed for inspiration</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Path</code> <p>The file to test.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the file is gzip compressed, otherwise False.</p> Source code in <code>src/baktfold/io/handle_genbank.py</code> <pre><code>def is_gzip_file(f: Path) -&gt; bool:\n    \"\"\"\n    Method copied from Phispy see https://github.com/linsalrob/PhiSpy/blob/master/PhiSpyModules/helper_functions.py\n\n    This is an elegant solution to test whether a file is gzipped by reading the first two characters.\n    I also use a version of this in fastq_pair if you want a C version :)\n    See https://stackoverflow.com/questions/3703276/how-to-tell-if-a-file-is-gzip-compressed for inspiration\n    Args:\n        f (Path): The file to test.\n\n    Returns:\n        bool: True if the file is gzip compressed, otherwise False.\n    \"\"\"\n    with open(f, \"rb\") as i:\n        return binascii.hexlify(i.read(2)) == b\"1f8b\"\n</code></pre>"},{"location":"reference/#src.baktfold.io.handle_genbank.open_protein_fasta_file","title":"<code>open_protein_fasta_file(input_file)</code>","text":"<p>Open a fasta file, whether it is gzipped or plain text.</p> <p>input_file (str): The path to the fasta file, either gzipped or plain.</p> <p>Union[IO[str], gzip.GzipFile]: A file handle to the opened fasta file.</p> Source code in <code>src/baktfold/io/handle_genbank.py</code> <pre><code>def open_protein_fasta_file(input_file: str) -&gt; Union[IO[str], gzip.GzipFile]:\n    \"\"\"\n    Open a fasta file, whether it is gzipped or plain text.\n\n    Parameters:\n    input_file (str): The path to the fasta file, either gzipped or plain.\n\n    Returns:\n    Union[IO[str], gzip.GzipFile]: A file handle to the opened fasta file.\n    \"\"\"\n    input_file = Path(input_file)\n\n    if input_file.suffix == \".gz\":\n        return gzip.open(input_file, \"rt\")\n    else:\n        return open(input_file, \"r\")\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.add_optional_qualifiers","title":"<code>add_optional_qualifiers(entry, qualifiers, single_valued=None, multi_valued=None)</code>","text":"<p>Add optional INSDC qualifiers to a feature entry dict in Bakta style.</p>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.add_optional_qualifiers--parameters","title":"Parameters","text":"dict <p>The feature dictionary being built.</p> dict <p>The qualifiers dictionary from Bio.SeqFeature.</p> set or list <p>Qualifiers expected to be single-valued (take the first if multiple).</p> set or list <p>Qualifiers that can have multiple values (keep as list if &gt;1, else single value).</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def add_optional_qualifiers(entry, qualifiers, single_valued=None, multi_valued=None):\n    \"\"\"\n    Add optional INSDC qualifiers to a feature entry dict in Bakta style.\n\n    Parameters\n    ----------\n    entry : dict\n        The feature dictionary being built.\n    qualifiers : dict\n        The qualifiers dictionary from Bio.SeqFeature.\n    single_valued : set or list\n        Qualifiers expected to be single-valued (take the first if multiple).\n    multi_valued : set or list\n        Qualifiers that can have multiple values (keep as list if &gt;1, else single value).\n    \"\"\"\n\n    single_valued = single_valued or set()\n    multi_valued = multi_valued or set()\n\n    # Multi-valued qualifiers\n    for key in multi_valued:\n        vals = qualifiers.get(key)\n        if vals:\n            entry[key] = vals if len(vals) &gt; 1 else vals[0]\n\n    # Single-valued qualifiers\n    for key in single_valued:\n        vals = qualifiers.get(key)\n        if vals:\n            if key == \"locus_tag\":\n                entry[\"locus\"] = vals[0] # this is what bakta needs\n            else:\n                entry[key] = vals[0]\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.build_bakta_sequence_entry","title":"<code>build_bakta_sequence_entry(rec)</code>","text":"<p>Convert a  SeqRecord into a Bakta-style sequence entry. Missing fields are filled with None.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def build_bakta_sequence_entry(rec):\n    \"\"\"\n    Convert a  SeqRecord into a Bakta-style sequence entry.\n    Missing fields are filled with None.\n    \"\"\"\n\n    seq = str(rec.seq)\n\n    # -----------------------------------------\n    # Extract source feature qualifiers - genbank always has source field\n    # -----------------------------------------\n    source_feat = next((f for f in rec.features if f.type == \"source\"), None)\n\n    source_qualifiers = {}\n\n    # Defaults (None) for all fields\n    mol_type = None\n    organism = None\n    strain = None\n    db_xref = None\n    note = None\n\n    plasmid = None\n    chromosome = None\n    completeness_hint = None\n\n    if source_feat:\n        q = source_feat.qualifiers\n\n        mol_type = q.get(\"mol_type\", [None])[0]\n        organism = q.get(\"organism\", [None])[0]\n        strain = q.get(\"strain\", [None])[0]\n        note = q.get(\"note\", [None])[0]\n\n        if \"db_xref\" in q:\n            val = q[\"db_xref\"]\n            db_xref = val[0] if len(val) == 1 else val\n\n        plasmid = q.get(\"plasmid\", [None])[0]\n        chromosome = q.get(\"chromosome\", [None])[0]\n        completeness_hint = q.get(\"completeness\", [None])[0]\n\n    # -----------------------------------------\n    # Infer topology\n    # -----------------------------------------\n    topology = rec.annotations.get(\"topology\")\n    if topology not in {\"linear\", \"circular\"}:\n        topology = \"linear\"\n\n    # -----------------------------------------\n    # Infer type\n    # -----------------------------------------\n    if plasmid is not None or \"plasmid\" in rec.annotations:\n        seq_type = \"plasmid\"\n    elif chromosome is not None or \"chromosome\" in rec.annotations:\n        seq_type = \"chromosome\"\n    else:\n        seq_type = \"contig\"\n\n    # -----------------------------------------\n    # Infer completeness (conservative)\n    # -----------------------------------------\n    complete = False\n\n    if topology == \"circular\":\n        complete = True\n    elif completeness_hint is not None and completeness_hint.lower() == \"complete\":\n        complete = True\n    elif note and \"complete genome\" in note.lower():\n        complete = True\n\n    # -----------------------------------------\n    # Infer genetic codefor description\n    # -----------------------------------------\n    gcode = None\n\n    if \"genetic_code\" in rec.annotations:\n        gcode = rec.annotations[\"genetic_code\"]\n    elif \"gcode\" in rec.annotations:\n        gcode = rec.annotations[\"gcode\"]\n    elif source_feat and \"transl_table\" in source_feat.qualifiers:\n        gcode = source_feat.qualifiers[\"transl_table\"][0]\n\n    # Conservative fallback to 1 for euks\n    if gcode is None:\n        gcode = 1 \n\n    description_parts = [\n        f\"[gcode={gcode}]\",\n        f\"[topology={topology}]\",\n    ]\n\n    description = \" \".join(description_parts)\n\n    # -----------------------------------------\n    # Build entry\n    # -----------------------------------------\n    entry = {\n        \"id\": rec.id,\n        \"description\": description,\n        \"nt\": seq,\n        \"length\": len(seq),\n        \"complete\": complete,\n        \"type\": seq_type,\n        \"topology\": topology,\n        \"simple_id\": rec.id,\n        \"orig_id\": rec.id,\n        \"orig_description\": None,\n    }\n\n    # -----------------------------------------\n    # Add source qualifiers if present\n    # -----------------------------------------\n    if organism is not None:\n        entry[\"organism\"] = organism\n    if mol_type is not None:\n        entry[\"mol_type\"] = mol_type\n    if strain is not None:\n        entry[\"strain\"] = strain\n    if db_xref is not None:\n        entry[\"db_xref\"] = db_xref\n    if note is not None:\n        entry[\"note\"] = note\n\n\n    # this is from bakta\n    # \"id\": \"contig_1\",\n    # \"description\": \"[gcode=11] [topology=linear]\",\n    # \"nt\": \"AT\"\n    # \"length\": 5165988,\n    # \"complete\": false,\n    # \"type\": \"contig\",\n    # \"topology\": \"linear\",\n    # \"simple_id\": \"contig_1\",\n    # \"orig_id\": \"GCF_002368115_000000000001\",\n    # \"orig_description\": \"\"\n\n    # Add source qualifiers only if they exist\n    if organism is not None:\n        entry[\"organism\"] = organism\n\n    if mol_type is not None:\n        entry[\"mol_type\"] = mol_type\n\n    if strain is not None:\n        entry[\"strain\"] = strain\n\n    if db_xref is not None:\n        entry[\"db_xref\"] = db_xref\n\n    if note is not None:\n        entry[\"note\"] = note\n\n    return entry\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.calc_genome_stats","title":"<code>calc_genome_stats(records)</code>","text":"<p>Compute correct genome stats (size, GC, N-ratio, N50, N90) for records from a multi-contig  GenBank file.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def calc_genome_stats(records):\n    \"\"\"\n    Compute correct genome stats (size, GC, N-ratio, N50, N90) for records from a multi-contig\n     GenBank file.\n    \"\"\"\n\n    if not records:\n        raise ValueError(\"No GenBank records found.\")\n\n    # lengths of all contigs\n    contig_lengths = [len(r.seq) for r in records]\n    total_length = sum(contig_lengths)\n\n    # concatenate sequences for global GC + N calculation\n    full_seq = \"\".join(str(r.seq) for r in records)\n\n    # GC as fraction (Bakta wants 0\u20131)\n    gc_perc = gc_fraction(full_seq)\n\n    # N-ratio\n    n_ratio = full_seq.count(\"N\") / total_length\n\n    # ---------- N50 / N90 ----------\n    sorted_lengths = sorted(contig_lengths, reverse=True)\n\n    def nx_metric(sorted_lens, total, threshold):\n        \"\"\"\n        Generic N{threshold} function.\n        threshold: 0.5 for N50, 0.9 for N90\n        \"\"\"\n        cutoff = total * threshold\n        running = 0\n        for l in sorted_lens:\n            running += l\n            if running &gt;= cutoff:\n                return l\n        return sorted_lens[-1]  # fallback (should not happen)\n\n    n50 = nx_metric(sorted_lengths, total_length, 0.5)\n    n90 = nx_metric(sorted_lengths, total_length, 0.9)\n\n    return {\n        \"size\": total_length,\n        \"gc\": gc_perc,\n        \"n_ratio\": n_ratio,\n        \"n50\": n50,\n        \"n90\": n90,\n        \"coding_ratio\": None  \n    }\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.convert_assembly_gap_feature","title":"<code>convert_assembly_gap_feature(feature, rec, id)</code>","text":"<p>Convert a GenBank assembly_gap feature to a simplified Bakta-style 'gap' feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The assembly_gap feature from the GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The full GenBank record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Simplified Bakta-style gap feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_assembly_gap_feature(feature, rec, id):\n    \"\"\"\n    Convert a GenBank assembly_gap feature to a simplified Bakta-style 'gap' feature.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The assembly_gap feature from the GBK.\n        rec: Bio.SeqRecord\n            The full GenBank record containing the sequence.\n\n    Returns:\n        dict: Simplified Bakta-style gap feature.\n    \"\"\"\n\n    # Coordinates (1-based)\n    strand = \".\" # bakta uses \".\" for strand on gaps\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    qualifiers = feature.qualifiers\n\n    #  may provide estimated_length but coordinates already give an exact span\n    est_len = qualifiers.get(\"estimated_length\", [None])[0]\n    if est_len is not None:\n        length = int(est_len)\n    else:\n        length = stop - start + 1  # fallback from coordinates\n\n\n    gap_entry = {\n        \"type\": \"gap\",\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"length\": length,\n        \"id\": id,\n    }\n\n    # no need to add estimated length separately - it is covered by length in the json \n\n    # if est_len:\n    #     gap_entry[\"estimated_length\"] = est_len\n\n    return gap_entry\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.convert_cds_feature","title":"<code>convert_cds_feature(feature, seq_record, translation_table, id)</code>","text":"<p>Convert a Prokka CDS Biopython SeqFeature to a Bakta CDS JSON entry.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_cds_feature(feature, seq_record, translation_table, id):\n    \"\"\"\n    Convert a Prokka CDS Biopython SeqFeature to a Bakta CDS JSON entry.\n    \"\"\"\n\n    # ----------- Location info -----------\n\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    # Handle CompoundLocation (join)\n    starts = None\n    stops = None\n\n    if feature.location.__class__.__name__ == \"CompoundLocation\":\n        starts = []\n        stops = []\n        for part in feature.location.parts:\n            starts.append(int(part.start) + 1)\n            stops.append(int(part.end))\n\n\n    # frame: Bakta uses 1/2/3; Prokka codon_start is [\"1\",\"2\",\"3\"]\n    codon_start = int(feature.qualifiers.get(\"codon_start\", [\"1\"])[0])\n    frame = codon_start\n\n    qualifiers = feature.qualifiers\n\n    # ----------- Basic qualifiers -----------\n    gene = qualifiers.get(\"gene\", [None])[0]\n    product = qualifiers.get(\"product\", [None])[0]\n\n\n    # fall back to start_stop_strand if there is no locus tag\n    if 'locus_tag' in qualifiers and qualifiers['locus_tag']:\n        locus_tag = qualifiers['locus_tag'][0]\n    else:\n        logger.warning(f\"No locus_tag found for feature {id}\")\n        locus_tag = f\"{GENOME_RANDOM_BACKUP_LOCUSTAG_STR}_{start}_{stop}\"\n        logger.warning(f\"Generating a locus_tag: {locus_tag}\")\n\n    note = qualifiers.get(\"note\", [None])[0]\n    locus = locus_tag\n\n    # pseudo\n\n    protein_id = qualifiers.get(\"protein_id\", [None])[0]\n\n    # ----------- Extract nucleotides -----------\n    nt_seq = feature.extract(seq_record.seq)\n    nt = str(nt_seq)\n\n    # ----------- Extract amino acids -----------\n    aa = feature.qualifiers.get(\"translation\", [\"\"])[0]\n\n    # Compute translation if Prokka didn't provide it\n    if not aa:\n        try:\n            aa = str(nt_seq.translate(table=translation_table, cds=True))\n        except Exception:\n            aa = \"\"\n\n    # ----------- aa MD5 hexdigest -----------\n    aa_hexdigest = hashlib.md5(aa.encode()).hexdigest()\n\n    # ----------- Hypothetical? -----------\n    hypothetical = product is None or \"hypothetical protein\" in product.lower()\n\n    # ----------- Compute protein stats -----------\n    seq_stats = None\n    if aa:\n        try:\n            analysed = ProteinAnalysis(aa)\n            seq_stats = {\n                \"molecular_weight\": analysed.molecular_weight(),\n                \"isoelectric_point\": analysed.isoelectric_point()\n            }\n        except Exception:\n            seq_stats = None\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xref = qualifiers.get(\"db_xref\", [so.SO_CDS.id])\n\n    # Append so.SO_CDS.id only if it\u2019s not already present\n    if so.SO_CDS.id not in db_xref:\n        db_xref.append(so.SO_CDS.id)\n\n    # ----------- Make Bakta-format dict -----------\n    bakta_cds = {\n        \"type\": \"cds\",\n        \"sequence\": seq_record.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"starts\": starts,\n        \"stops\": stops,\n        \"strand\": strand,\n        \"frame\": frame,\n        \"gene\": gene,\n        \"product\": product,\n        \"db_xrefs\": db_xref,  \n        \"nt\": nt,\n        \"aa\": aa,\n        \"aa_hexdigest\": aa_hexdigest,\n        \"start_type\": None,\n        \"rbs_motif\": None,\n        \"genes\": [],\n        \"note\": note,\n        \"seq_stats\": seq_stats,\n        \"id\": id,\n        \"locus\": locus,\n        \"protein_id\": protein_id\n    }\n\n# Feature Key           CDS\n\n# Definition            coding sequence; sequence of nucleotides that\n#                       corresponds with the sequence of amino acids in a\n#                       protein (location includes stop codon); \n#                       feature includes amino acid conceptual translation.\n\n# Optional qualifiers   /allele=\"text\"\n#                       /artificial_location=\"[artificial_location_value]\"\n#                       /circular_RNA\n#                       /codon_start=&lt;1 or 2 or 3&gt;\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /EC_number=\"text\"\n#                       /exception=\"[exception_value]\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /number=unquoted text (single token)\n#                       /old_locus_tag=\"text\" (single token)\n#                       /operon=\"text\"\n#                       /product=\"text\"\n#                       /protein_id=\"&lt;identifier&gt;\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /ribosomal_slippage\n#                       /standard_name=\"text\"\n#                       /translation=\"text\"\n#                       /transl_except=(pos:&lt;location&gt;,aa:&lt;amino_acid&gt;)\n#                       /transl_table =&lt;integer&gt;\n#                       /trans_splicing\n\n    multi_valued = {\"EC_number\", \"exception\", \"experiment\", \"function\",  \"gene_synonym\",  \"inference\", }\n    single_valued = {\"allele\", \"artificial_location\",  \"map\", \"number\",  \"old_locus_tag\", \"operon\", \"phenotype\", \"pseudogene\", \"standard_name\", \"transl_except\", \"transl_table\"}\n\n    add_optional_qualifiers(bakta_cds, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"circular_RNA\", \"pseudo\", \"ribosomal_slippage\", \"trans_splicing\"]:\n        if flag in qualifiers:\n            bakta_cds[flag] = flag in qualifiers\n\n    if hypothetical:\n        bakta_cds[\"hypothetical\"] = True\n\n    return bakta_cds\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.convert_exon_feature","title":"<code>convert_exon_feature(feature, rec, id)</code>","text":"<p>Convert a GenBank exon feature to a simplified Bakta-style 'exon' feature.</p>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.convert_exon_feature--parameters","title":"Parameters","text":"Bio.SeqFeature <p>The exon feature from the GenBank record.</p> Bio.SeqRecord <p>The full GenBank record.</p> str <p>Unique feature ID.</p>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.convert_exon_feature--returns","title":"Returns","text":"<p>dict     Bakta-style exon feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_exon_feature(feature, rec, id):\n    \"\"\"\n    Convert a GenBank exon feature to a simplified Bakta-style 'exon' feature.\n\n    Parameters\n    ----------\n    feature : Bio.SeqFeature\n        The exon feature from the GenBank record.\n    rec : Bio.SeqRecord\n        The full GenBank record.\n    id : str\n        Unique feature ID.\n\n    Returns\n    -------\n    dict\n        Bakta-style exon feature.\n    \"\"\"\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    qualifiers = feature.qualifiers\n\n    db_xrefs = qualifiers.get(\"db_xref\", [])\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /EC_number=\"text\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /number=unquoted text (single token)\n#                       /old_locus_tag=\"text\" (single token)\n#                       /product=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /standard_name=\"text\"\n#                       /trans_splicing\n\n\n    # Extract commonly used INSDC qualifiers\n    exon_entry = {\n            \"type\": \"exon\",\n            \"sequence\": rec.id,\n            \"start\": start,\n            \"stop\": stop,\n            \"strand\": strand,\n            \"id\": id,\n            \"db_xrefs\": db_xrefs\n        }\n\n    multi_valued = {\"EC_number\",\"experiment\",\"function\",  \"gene_synonym\",  \"inference\",\"note\" }\n    single_valued = {\"allele\", \"gene\", \"locus_tag\", \"map\", \"number\",   \"old_locus_tag\", \"operon\", \"pseudogene\", \"standard_name\"   }\n\n    add_optional_qualifiers(exon_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"pseudo\", \"trans_splicing\"]:\n        if flag in qualifiers:\n            exon_entry[flag] = True\n\n    return exon_entry\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.convert_gene_feature","title":"<code>convert_gene_feature(feature, rec, id)</code>","text":"<p>Convert a Funannotate GenBank gene feature to Bakta-style JSON.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The rRNA feature from the GBK.</p> required <code>rec</code> <p>str The record from the GBK.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Bakta-style rRNA feature</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_gene_feature(feature, rec, id):\n    \"\"\"\n    Convert a Funannotate GenBank gene feature to Bakta-style JSON.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The rRNA feature from the GBK.\n        rec: str\n            The record from the GBK.\n    Returns:\n        dict: Bakta-style rRNA feature\n    \"\"\"\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n\n    qualifiers = feature.qualifiers\n\n    # fall back to start_stop_strand if there is no locus tag\n    if 'locus_tag' in qualifiers and qualifiers['locus_tag']:\n        locus_tag = qualifiers['locus_tag'][0]\n    else:\n        logger.warning(f\"No locus_tag found for feature {id}\")\n        locus_tag = f\"{GENOME_RANDOM_BACKUP_LOCUSTAG_STR}_{start}_{stop}\"\n        logger.warning(f\"Generating a locus_tag: {locus_tag}\")\n\n\n\n    gene_entry = {\n        \"type\": \"gene\",\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"db_xrefs\": [so.SO_GENE.id], \n        \"id\": id,\n        \"locus\": locus_tag\n    }\n\n\n# Feature Key           gene \n\n\n# Definition            region of biological interest identified as a gene \n#                       and for which a name has been assigned;\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /operon=\"text\"\n#                       /product=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /phenotype=\"text\"\n#                       /standard_name=\"text\"\n#                       /trans_splicing\n\n\n# Comment               the gene feature describes the interval of DNA that \n#                       corresponds to a genetic trait or phenotype; the feature is,\n#                       by definition, not strictly bound to it's positions at the \n#                       ends;  it is meant to represent a region where the gene is \n#                       located.\n\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\" }\n    single_valued = {\"allele\",  \"map\",  \"old_locus_tag\", \"operon\", \"phenotype\", \"standard_name\"}\n\n    qualifiers = feature.qualifiers\n\n    add_optional_qualifiers(gene_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"pseudo\", \"trans_splicing\"]:\n        if flag in qualifiers:\n            gene_entry[flag] = flag in qualifiers\n\n    return gene_entry\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.convert_mat_peptide_feature","title":"<code>convert_mat_peptide_feature(feature, rec, id)</code>","text":"<p>Convert a mat_peptide feature to a Bakta-style feature.</p> <p>mus musculus chrom 1 NC_000067</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The mRNA feature from the GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Bakta-style misc_RNA feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_mat_peptide_feature(feature, rec, id):\n    \"\"\"\n    Convert a mat_peptide feature to a Bakta-style feature.\n\n    mus musculus chrom 1 NC_000067\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The mRNA feature from the GBK.\n        rec: Bio.SeqRecord\n            The record containing the sequence.\n\n    Returns:\n        dict: Bakta-style misc_RNA feature.\n    \"\"\"\n\n    seq = str(rec.seq)\n\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    # Handle CompoundLocation (join)\n    starts = None\n    stops = None\n\n    if feature.location.__class__.__name__ == \"CompoundLocation\":\n        starts = []\n        stops = []\n        for part in feature.location.parts:\n            starts.append(int(part.start) + 1)\n            stops.append(int(part.end))\n\n    so_code =  so.SO_MAT_PEPTIDE.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n    qualifiers = feature.qualifiers\n\n\n    # Extract commonly used INSDC qualifiers\n    mat_peptide_entry = {\n            \"type\": \"mat_peptide\",\n            \"sequence\": rec.id,\n            \"start\": start,\n            \"stop\": stop,\n            # Join support\n            \"starts\": starts,\n            \"stops\": stops,\n            \"strand\": strand,\n            \"id\": id,\n            \"db_xrefs\": db_xrefs\n        }\n\n\n# Feature Key           mat_peptide\n\n\n# Definition            mature peptide or protein coding sequence; coding\n#                       sequence for the mature or final peptide or protein\n#                       product following post-translational modification; the\n#                       location does not include the stop codon (unlike the\n#                       corresponding CDS);\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /EC_number=\"text\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /product=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /standard_name=\"text\"\n\n    multi_valued = {\"EC_number\",\"experiment\", \"function\",  \"gene_synonym\",  \"inference\",\"note\" }\n    single_valued = {\"allele\", \"gene\", \"locus_tag\", \"map\", \"number\",   \"old_locus_tag\", \"operon\", \"pseudogene\", \"standard_name\"}\n\n    add_optional_qualifiers(mat_peptide_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers) - no flags\n    # for flag in [\"pseudo\"]:\n    #     if flag in qualifiers:\n    #         mat_peptide_entry[flag] = True\n\n\n    #  mat_peptide     complement(join(194724303..194724321,194744661..194744721,\n    #                  194746996..194747031,194750435..194750476,\n    #                  194757818..194757865,194759962..194760144,\n    #                  194764890..194765087,194765856..194765944,\n    #                  194767641..194767743,194768400..194768583))\n    #                  /gene=\"Cd46\"\n    #                  /gene_synonym=\"Mcp\"\n    #                  /product=\"Membrane cofactor protein. /id=PRO_0000238971\"\n    #                  /note=\"propagated from UniProtKB/Swiss-Prot (O88174.1)\"\n\n    return mat_peptide_entry\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.convert_misc_feature","title":"<code>convert_misc_feature(feature, rec, id)</code>","text":"<p>Convert a misc feature to a Bakta-style feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The mRNA feature from the GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Bakta-style misc_feature feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_misc_feature(feature, rec, id):\n    \"\"\"\n    Convert a misc feature to a Bakta-style feature.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The mRNA feature from the GBK.\n        rec: Bio.SeqRecord\n            The record containing the sequence.\n\n    Returns:\n        dict: Bakta-style misc_feature feature.\n    \"\"\"\n\n    seq = str(rec.seq)\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    # Handle CompoundLocation (join)\n    starts = None\n    stops = None\n\n    if feature.location.__class__.__name__ == \"CompoundLocation\":\n        starts = []\n        stops = []\n        for part in feature.location.parts:\n            starts.append(int(part.start) + 1)\n            stops.append(int(part.end))\n\n    qualifiers = feature.qualifiers\n\n    so_code =  so.SO_MISC_REGION.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append so.SO_CDS.id only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n\n    misc_feature_entry = {\n            \"type\": \"misc_feature\",\n            \"sequence\": rec.id,\n            \"start\": start,\n            \"stop\": stop,\n            \"strand\": strand,\n            \"id\": id,\n\n            # Join support\n            \"starts\": starts,\n            \"stops\": stops,\n\n            # Multi-valued\n            \"db_xrefs\": db_xrefs,\n\n\n        }\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\", \"phenotype\"}\n    single_valued = {\"allele\", \"gene\", \"locus_tag\", \"map\",    \"old_locus_tag\", \"operon\", \"product\", \"standard_name\",  \"pseudogene\"}\n\n    add_optional_qualifiers(misc_feature_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"pseudo\",]:\n        if flag in qualifiers:\n            misc_feature_entry[flag] = True\n\n# Feature Key           misc_feature\n\n\n# Definition            region of biological interest which cannot be described\n#                       by any other feature key; a new or rare feature;\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /number=unquoted text (single token)\n#                       /old_locus_tag=\"text\" (single token)\n#                       /phenotype=\"text\"\n#                       /product=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /standard_name=\"text\"\n\n# Comment               this key should not be used when the need is merely to \n#                       mark a region in order to comment on it or to use it in \n#                       another feature's location\n\n    #  misc_feature    join(78488668..78488692,78499322..78499359)\n    #                  /gene=\"Mogat1\"\n    #                  /gene_synonym=\"0610030A14Rik; 1110064N14Rik; Dgat2l;\n    #                  Dgat2l1; mDC2; MGAT1; WI1-2612I11.1\"\n    #                  /note=\"propagated from UniProtKB/Swiss-Prot (Q91ZV4.2);\n    #                  transmembrane region\"\n\n    #  misc_feature    78179419..78180585\n    #                  /standard_name=\"Pax3 upstream hypaxial enhancer\"\n    #                  /note=\"Region: biological region; Derived by automated\n    #                  computational analysis using gene prediction method:\n    #                  RefSeqFE.\"\n    #                  /function=\"regulatory_interactions: LOC107980439 | Pax3\"\n    #                  /db_xref=\"GeneID:107980442\"    \n\n    return misc_feature_entry\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.convert_misc_rna_feature","title":"<code>convert_misc_rna_feature(feature, rec, id)</code>","text":"<p>Convert a GenBank misc_rna feature to a simplified Bakta-style 'misc_rna' feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The assembly_gap feature from the GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The full GenBank record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Simplified Bakta-style gap feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_misc_rna_feature(feature, rec, id):\n    \"\"\"\n    Convert a GenBank misc_rna feature to a simplified Bakta-style 'misc_rna' feature.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The assembly_gap feature from the GBK.\n        rec: Bio.SeqRecord\n            The full GenBank record containing the sequence.\n\n    Returns:\n        dict: Simplified Bakta-style gap feature.\n\n    \"\"\"\n\n        # from ensemble genomes\n        # misc_RNA        complement(437333..442742)\n        #             /gene=\"YPL060C-A\"\n        #             /note=\"transposable_element\"\n        #             /standard_name=\"YPL060C-A\"\n\n    # Coordinates (1-based)\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n\n    qualifiers = feature.qualifiers\n    gene = qualifiers.get(\"gene\", [None])[0]\n\n# Feature Key           misc_RNA\n\n\n# Definition            any transcript or RNA product that cannot be defined by\n#                       other RNA keys (prim_transcript, precursor_RNA, mRNA,\n#                       5'UTR, 3'UTR, exon, CDS, sig_peptide, transit_peptide,\n#                       mat_peptide, intron, polyA_site, ncRNA, rRNA and tRNA);\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /operon=\"text\"\n#                       /product=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /standard_name=\"text\"\n#                       /trans_splicing\n\n    misc_rna_entry = {\n        \"type\": \"misc_RNA\", # expects lowercase \n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand, # matches Bakta and is required\n        \"gene\": gene,\n        \"id\": id\n    }\n\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\" }\n    single_valued = {\"allele\", \"gene\",  \"locus_tag\", \"map\",  \"old_locus_tag\", \"operon\", \"product\", \"phenotype\", \"standard_name\"}\n\n    qualifiers = feature.qualifiers\n\n    add_optional_qualifiers(misc_rna_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"pseudo\", \"trans_splicing\"]:\n        if flag in qualifiers:\n            misc_rna_entry[flag] = flag in qualifiers\n\n    return misc_rna_entry\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.convert_mobile_element_feature","title":"<code>convert_mobile_element_feature(feature, rec, id)</code>","text":"<p>Convert a GenBank mobile_element feature to a Bakta-style feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_mobile_element_feature(feature, rec, id):\n    \"\"\"\n    Convert a GenBank mobile_element feature to a Bakta-style feature.\n    \"\"\"\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    qualifiers = feature.qualifiers\n\n    # Mandatory qualifier check (INSDC requirement)\n    mobile_element_type = qualifiers.get(\"mobile_element_type\", [None])[0]\n    if mobile_element_type is None:\n        raise ValueError(\n            f\"mobile_element feature {id} is missing mandatory \"\n            \"/mobile_element_type qualifier\"\n        )\n\n    so_code =  so.SO_MOBILE_ELEMENT.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n\n# Feature Key           mobile_element\n\n\n# Definition            region of genome containing mobile elements;\n\n# Mandatory qualifiers  /mobile_element_type=\"&lt;mobile_element_type&gt;\n#                       [:&lt;mobile_element_name&gt;]\"\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\" \n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /rpt_family=\"text\"\n#                       /rpt_type=&lt;repeat_type&gt;\n#                       /standard_name=\"text\"\n\n\n    # Extract commonly used INSDC qualifiers\n    mobile_element_entry = {\n            \"type\": \"mobile_element\",\n            \"sequence\": rec.id,\n            \"start\": start,\n            \"stop\": stop,\n            \"strand\": strand,\n            \"id\": id,\n            \"db_xrefs\": db_xrefs,\n                    # Mandatory\n            \"mobile_element_type\": mobile_element_type,\n        }\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\" }\n    single_valued = {\"allele\", \"gene\", \"locus_tag\", \"map\",    \"old_locus_tag\", \"standard_name\", \"rpt_family\", \"rpt_type\"}\n\n    add_optional_qualifiers(mobile_element_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    # for flag in [\"pseudo\"]:\n    #   if flag in qualifiers:\n    #     mobile_element_entry[flag] = True\n\n\n    #  mobile_element  57369551..57369723\n    #                  /note=\"Derived by automated computational analysis using\n    #                  gene prediction method: RefSeqFE.\"\n    #                  /mobile_element_type=\"SINE:AmnSINE1\"\n    #                  /db_xref=\"GeneID:106707176\"\n\n    return mobile_element_entry\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.convert_mrna_feature","title":"<code>convert_mrna_feature(feature, rec, id)</code>","text":"<p>Convert a funannotate mrna feature to a Bakta-style feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The mRNA feature from the GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Bakta-style mRNA feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_mrna_feature(feature, rec, id):\n    \"\"\"\n    Convert a funannotate mrna feature to a Bakta-style feature.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The mRNA feature from the GBK.\n        rec: Bio.SeqRecord\n            The record containing the sequence.\n\n    Returns:\n        dict: Bakta-style mRNA feature.\n    \"\"\"\n\n    # seq = str(rec.seq)\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    # Handle CompoundLocation (join)\n    starts = None\n    stops = None\n\n    if feature.location.__class__.__name__ == \"CompoundLocation\":\n        starts = []\n        stops = []\n        for part in feature.location.parts:\n            starts.append(int(part.start) + 1)\n            stops.append(int(part.end))\n\n    else:\n        starts = None\n        stops = None\n\n\n    qualifiers = feature.qualifiers\n\n\n    # fall back to start_stop_strand if there is no locus tag\n    if 'locus_tag' in qualifiers and qualifiers['locus_tag']:\n        locus_tag = qualifiers['locus_tag'][0]\n    else:\n        logger.warning(f\"No locus_tag found for feature {id}\")\n        locus_tag = f\"{GENOME_RANDOM_BACKUP_LOCUSTAG_STR}_{start}_{stop}\"\n        logger.warning(f\"Generating a locus_tag: {locus_tag}\")\n\n\n    mrna_entry = {\n        \"type\": \"mRNA\", \n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"starts\": starts,\n        \"stops\": stops,\n        \"strand\": strand,\n        \"db_xrefs\": [so.SO_MRNA.id],        \n        \"id\": id,\n        \"locus\": locus_tag\n    }\n\n\n\n# Feature Key           mRNA\n\n\n# Definition            messenger RNA; includes 5'untranslated region (5'UTR),\n#                       coding sequences (CDS, exon) and 3'untranslated region\n#                       (3'UTR);\n\n# Optional qualifiers   /allele=\"text\"\n#                       /artificial_location=\"[artificial_location_value]\"\n#                       /circular_RNA\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /operon=\"text\"\n#                       /product=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /standard_name=\"text\"\n#                       /trans_splicing\n\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\" }\n    single_valued = {\"allele\", \"artificial_location\", \"gene\",  \"locus_tag\", \"map\",  \"old_locus_tag\", \"operon\", \"phenotype\", \"product\", \"standard_name\"}\n\n    qualifiers = feature.qualifiers\n\n    add_optional_qualifiers(mrna_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"circular_RNA\", \"pseudo\", \"trans_splicing\"]:\n        if flag in qualifiers:\n            mrna_entry[flag] = flag in qualifiers\n\n    return mrna_entry\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.convert_ncrna_feature","title":"<code>convert_ncrna_feature(feature, rec, id)</code>","text":"<p>Convert a ncrna feature to a Bakta-style feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The mRNA feature from the GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Bakta-style misc_RNA feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_ncrna_feature(feature, rec, id):\n    \"\"\"\n    Convert a ncrna feature to a Bakta-style feature.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The mRNA feature from the GBK.\n        rec: Bio.SeqRecord\n            The record containing the sequence.\n\n    Returns:\n        dict: Bakta-style misc_RNA feature.\n    \"\"\"\n\n    # seq = str(rec.seq)\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    # Handle CompoundLocation (join)\n    starts = None\n    stops = None\n\n    if feature.location.__class__.__name__ == \"CompoundLocation\":\n        starts = []\n        stops = []\n        for part in feature.location.parts:\n            starts.append(int(part.start) + 1)\n            stops.append(int(part.end))\n\n    qualifiers = feature.qualifiers\n\n    so_code =  so.SO_NCRNA_GENE.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append so only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n    # Mandatory qualifier (INSDC requirement)\n    ncrna_class = qualifiers.get(\"ncRNA_class\", [None])[0]\n    if ncrna_class is None:\n        raise ValueError(\n            f\"ncRNA feature {id} is missing mandatory /ncRNA_class qualifier\"\n        )\n\n    ncrna_entry = {\n        \"type\": \"ncRNA\",\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"id\": id,\n\n        # Join support\n        \"starts\": starts,\n        \"stops\": stops,\n\n        # Mandatory\n        \"ncRNA_class\": ncrna_class,\n\n        # Multi-valued qualifiers\n        \"db_xrefs\": db_xrefs,\n\n\n    }\n\n# Feature Key           ncRNA\n\n# Definition            a non-protein-coding gene, other than ribosomal RNA and\n#                       transfer RNA, the functional molecule of which is the RNA\n#                       transcript;\n\n# Mandatory qualifiers  /ncRNA_class=\"TYPE\"\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /operon=\"text\"\n#                       /product=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /standard_name=\"text\"\n#                       /trans_splicing\n\n# Example               /ncRNA_class=\"miRNA\"\n#                       /ncRNA_class=\"siRNA\"\n#                       /ncRNA_class=\"scRNA\"       \n\n# Comment               the ncRNA feature is not used for ribosomal and transfer\n#                       RNA annotation, for which the rRNA and tRNA feature keys\n#                       should be used, respectively;\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\" }\n    single_valued = {\"allele\", \"gene\", \"locus_tag\", \"map\",    \"old_locus_tag\", \"operon\", \"product\", \"standard_name\", \"pseudogene\"}\n\n    add_optional_qualifiers(ncrna_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"pseudo\", \"trans_splicing\"]:\n        if flag in qualifiers:\n            ncrna_entry[flag] = flag in qualifiers\n\n    #  ncRNA           join(189791085..189791793,189798997..189799081,\n    #                  189819873..189820364,189821703..189822337)\n    #                  /ncRNA_class=\"lncRNA\"\n    #                  /gene=\"Gm30446\"\n    #                  /product=\"predicted gene, 30446, transcript variant X6\"\n    #                  /note=\"Derived by automated computational analysis using\n    #                  gene prediction method: Gnomon. Supporting evidence\n    #                  includes similarity to: 100% coverage of the annotated\n    #                  genomic feature by RNAseq alignments, including 2 samples\n    #                  with support for all annotated introns\"\n    #                  /transcript_id=\"XR_001779629.1\"\n    #                  /db_xref=\"GeneID:102632350\"\n    #                  /db_xref=\"MGI:MGI:5589605\"\n\n    return ncrna_entry\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.convert_precursor_rna_feature","title":"<code>convert_precursor_rna_feature(feature, rec, id)</code>","text":"<p>Convert a GenBank precursor_RNA feature to a Bakta-style feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_precursor_rna_feature(feature, rec, id):\n    \"\"\"\n    Convert a GenBank precursor_RNA feature to a Bakta-style feature.\n    \"\"\"\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    qualifiers = feature.qualifiers\n\n    so_code =  so.SO_PRECURSOR_RNA.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n    precursor_rna_entry = {\n            \"type\": \"precursor_RNA\",\n            \"sequence\": rec.id,\n            \"start\": start,\n            \"stop\": stop,\n            \"strand\": strand,\n            \"db_xrefs\": db_xrefs,\n            \"id\": id,\n        }\n\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\"}\n    single_valued = {\"allele\", \"gene\", \"locus_tag\", \"map\",  \"operon\",  \"old_locus_tag\", \"product\", \"standard_name\"}\n\n    add_optional_qualifiers(precursor_rna_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"trans_splicing\"]:\n        if flag in qualifiers:\n            precursor_rna_entry[flag] = True\n\n#     Feature Key           precursor_RNA\n\n\n# Definition            any RNA species that is not yet the mature RNA product;\n#                       may include ncRNA, rRNA, tRNA, 5' untranslated region\n#                       (5'UTR), coding sequences (CDS, exon), intervening\n#                       sequences (intron) and 3' untranslated region (3'UTR);\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"  \n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /operon=\"text\"\n#                       /product=\"text\"\n#                       /standard_name=\"text\"\n#                       /trans_splicing\n\n\n    #  precursor_RNA   194719348..194719428\n    #                  /gene=\"Mir29b-2\"\n    #                  /gene_synonym=\"mir-29b-2; Mirn29b-2\"\n    #                  /product=\"microRNA 29b-2\"\n    #                  /note=\"Derived by automated computational analysis using\n    #                  gene prediction method: BestRefSeq.\"\n    #                  /transcript_id=\"NR_029809.1\"\n    #                  /db_xref=\"GeneID:723963\"\n    #                  /db_xref=\"MGI:MGI:3619047\"\n    #                  /db_xref=\"miRBase:MI0000712\"\n\n    return precursor_rna_entry\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.convert_proprotein_propeptide_feature","title":"<code>convert_proprotein_propeptide_feature(feature, rec, id)</code>","text":"<p>Convert a proprotein or propeptide feature to a Bakta-style feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The mRNA feature from the GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Bakta-style proprotein feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_proprotein_propeptide_feature(feature, rec, id):\n    \"\"\"\n    Convert a proprotein or propeptide feature to a Bakta-style feature.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The mRNA feature from the GBK.\n        rec: Bio.SeqRecord\n            The record containing the sequence.\n\n    Returns:\n        dict: Bakta-style proprotein feature.\n    \"\"\"\n\n    # seq = str(rec.seq)\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    # Handle CompoundLocation (join)\n    starts = None\n    stops = None\n\n    if feature.location.__class__.__name__ == \"CompoundLocation\":\n        starts = []\n        stops = []\n        for part in feature.location.parts:\n            starts.append(int(part.start) + 1)\n            stops.append(int(part.end))\n\n\n    qualifiers = feature.qualifiers\n\n    so_code =  so.SO_PROPEPTIDE.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append so.SO_CDS.id only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n\n    propeptide_entry = {\n        \"type\": \"propeptide\",\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"id\": id,\n\n        # Join support\n        \"starts\": starts,\n        \"stops\": stops,\n\n        # Multi-valued\n        \"db_xrefs\": qualifiers.get(\"db_xref\", []),\n\n    }\n\n\n# Feature Key           propeptide\n\n\n# Definition            propeptide coding sequence; coding sequence for the domain of a \n#                       proprotein that is cleaved to form the mature protein product.\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /product=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /standard_name=\"text\"\n\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\"}\n    single_valued = {\"allele\", \"gene\", \"locus_tag\", \"map\",    \"old_locus_tag\", \"product\", \"standard_name\", \"pseudogene\"}\n\n    add_optional_qualifiers(propeptide_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"pseudo\",]:\n        if flag in qualifiers:\n            propeptide_entry[flag] = True\n\n    #  proprotein      join(171053237..171053367,171053712..171053832)\n    #                  /gene=\"Apoa2\"\n    #                  /gene_synonym=\"Alp-2; Apo-AII; Apoa-2; ApoA-II; ApoAII;\n    #                  Hdl-1\"\n    #                  /product=\"apolipoprotein A-II proprotein\"  \n\n    return propeptide_entry\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.convert_protein_bind_feature","title":"<code>convert_protein_bind_feature(feature, rec, id)</code>","text":"<p>Convert a GenBank protein_bind feature to a Bakta-style feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_protein_bind_feature(feature, rec, id):\n    \"\"\"\n    Convert a GenBank protein_bind feature to a Bakta-style feature.\n    \"\"\"\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    qualifiers = feature.qualifiers\n\n    # Mandatory qualifier\n    bound_moiety = qualifiers.get(\"bound_moiety\", [None])[0]\n    if bound_moiety is None:\n        raise ValueError(\n            f\"protein_bind feature {id} is missing mandatory /bound_moiety qualifier\"\n        )\n\n    so_code =  so.SO_PROTEINBIND.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n    protein_bind_entry = {\n        \"type\": \"protein_bind\",\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"bound_moiety\": bound_moiety,\n        \"db_xrefs\": db_xrefs,\n        \"id\": id,\n    }\n\n\n# Feature Key           protein_bind\n\n\n# Definition            non-covalent protein binding site on nucleic acid;\n\n# Mandatory qualifiers  /bound_moiety=\"text\"\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /operon=\"text\"\n#                       /standard_name=\"text\"\n\n# Comment               note that feature key regulatory with /regulatory_class=\"ribosome_binding_site\"\n#                       should be used for ribosome binding sites.\n\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\"}\n    single_valued = {\"allele\", \"gene\", \"locus_tag\", \"map\",  \"operon\",  \"old_locus_tag\", \"product\", \"standard_name\"}\n\n    add_optional_qualifiers(protein_bind_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    # for flag in [\"trans_splicing\"]:\n    #     if flag in qualifiers:\n    #         protein_bind_entry[flag] = True\n\n    return protein_bind_entry\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.convert_regulatory_feature","title":"<code>convert_regulatory_feature(feature, rec, id)</code>","text":"<p>Convert a GenBank regulatory feature to a Bakta-style feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_regulatory_feature(feature, rec, id):\n    \"\"\"\n    Convert a GenBank regulatory feature to a Bakta-style feature.\n    \"\"\"\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    qualifiers = feature.qualifiers\n\n    # Mandatory qualifier\n    regulatory_class = qualifiers.get(\"regulatory_class\", [None])[0]\n    if regulatory_class is None:\n        raise ValueError(\n            f\"regulatory feature {id} is missing mandatory /regulatory_class qualifier\"\n        )\n\n    so_code =  so.SO_REGULATORY_REGION.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n\n    regulatory_entry = {\n            \"type\": \"regulatory\",\n            \"sequence\": rec.id,\n            \"start\": start,\n            \"stop\": stop,\n            \"strand\": strand,\n            \"regulatory_class\": regulatory_class,\n            \"db_xrefs\": db_xrefs,\n            \"id\": id,\n        }\n\n\n# Feature Key           regulatory\n\n\n# Definition            any region of sequence that functions in the regulation of\n#                       transcription, translation, replication, recombination, or chromatin structure;\n\n# Mandatory qualifiers  /regulatory_class=\"TYPE\"\n\n# Optional qualifiers   /allele=\"text\"\n#                       /bound_moiety=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /operon=\"text\"\n#                       /phenotype=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /standard_name=\"text\"\n\n# Comment\t              This feature has replaced the following Feature Keys on 15-DEC-2014:\n#                       enhancer, promoter, CAAT_signal, TATA_signal, -35_signal, -10_signal,\n#                       RBS, GC_signal, polyA_signal, attenuator, terminator, misc_signal.\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\"}\n    single_valued = {\"allele\", \"bound_moiety\", \"gene\", \"locus_tag\", \"map\",  \"operon\",  \"old_locus_tag\", \"phenotype\", \"product\", \"pseudogene\", \"standard_name\"}\n\n    add_optional_qualifiers(regulatory_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"pseudo\"]:\n        if flag in qualifiers:\n            regulatory_entry[flag] = True\n\n    #  regulatory      195030925..195032349\n    #                  /regulatory_class=\"enhancer\"\n    #                  /experiment=\"EXISTENCE:reporter gene assay evidence\n    #                  [ECO:0000049][PMID:32912294]\"\n    #                  /note=\"C2 STARR-seq-only enhancer starr_03508\"\n    #                  /function=\"activates a minimal SCP1 promoter by STARR-seq\n    #                  in ground-state (2iL) and metastable (SL) mouse embryonic\n    #                  stem cells {active_cell/tissue: mESC(E14 +2i+LIF or\n    #                  +serum+LIF)}\"\n    #                  /db_xref=\"GeneID:131296982\"\n\n    return regulatory_entry\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.convert_repeat_region_feature","title":"<code>convert_repeat_region_feature(feature, rec, id)</code>","text":"<p>Convert a Prokka GenBank repeat_region (CRISPR) feature to a simplified Bakta-style feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The repeat_region feature (crispr) from the Prokka GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The full GenBank record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Simplified Bakta-style CRISPR feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_repeat_region_feature(feature, rec, id):\n    \"\"\"\n    Convert a Prokka GenBank repeat_region (CRISPR) feature to a simplified Bakta-style feature.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The repeat_region feature (crispr) from the Prokka GBK.\n        rec: Bio.SeqRecord\n            The full GenBank record containing the sequence.\n\n    Returns:\n        dict: Simplified Bakta-style CRISPR feature.\n    \"\"\"\n\n    # Coordinates (Bakta uses 1-based)\n    strand = \".\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n\n    qualifiers = feature.qualifiers\n    note = qualifiers.get(\"note\", [None])[0]\n    rpt_family = qualifiers.get(\"rpt_family\", [None])[0]\n    rpt_type = qualifiers.get(\"rpt_type\", [None])[0]\n    rpt_unit_seq = qualifiers.get(\"rpt_unit_seq\", [None])[0]\n\n    # always just take the positive strand to get the NT seq (crispr repeat region)\n    seq =  str(rec.seq)\n    nt_seq = seq[start-1:stop]\n\n\n# Feature Key           repeat_region\n\n\n# Definition            region of genome containing repeating units;\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\" \n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /rpt_family=\"text\"\n#                       /rpt_type=&lt;repeat_type&gt;\n#                       /rpt_unit_range=&lt;base_range&gt;\n#                       /rpt_unit_seq=\"text\"\n#                       /satellite=\"&lt;satellite_type&gt;[:&lt;class&gt;][ &lt;identifier&gt;]\"\n#                       /standard_name=\"text\"\n\n    so_code =  so.SO_REPEAT.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n    # Minimal Bakta-like CRISPR structure\n    repeat_region_entry = {\n        \"type\": \"repeat_region\",\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand, # matches Bakta and is required\n        \"family\": rpt_family,       # e.g., \"LINE1\" - should always be there\n        \"rpt_type\": rpt_type,   \n        \"repeat_unit\": rpt_unit_seq, # the actual consensus repeat if crispr\n        \"product\": note, # won't be the same as Bakta as different lookup method used - but needed for the gff writing\n        \"nt\": nt_seq, # needed for batka .ffn writeout\n        \"id\": id, # bakta_id needed \n        # \"locus\": None, # no locus tag like Bakta\n        \"db_xrefs\": db_xrefs\n    }\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\" }\n    single_valued = {\"satellite\", \"gene\",  \"locus_tag\", \"map\",  \"old_locus_tag\", \"operon\", \"phenotype\", \"product\", \"standard_name\"}\n\n    qualifiers = feature.qualifiers\n\n    add_optional_qualifiers(repeat_region_entry, qualifiers, single_valued, multi_valued)\n\n\n    return repeat_region_entry\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.convert_rrna_feature","title":"<code>convert_rrna_feature(feature, rec, id)</code>","text":"<p>Convert a GenBank rRNA feature to a Bakta-style feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_rrna_feature(feature, rec, id):\n    \"\"\"\n    Convert a GenBank rRNA feature to a Bakta-style feature.\n    \"\"\"\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    qualifiers = feature.qualifiers\n\n    so_code =  so.SO_RRNA.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n# Feature Key           rRNA\n\n\n# Definition            mature ribosomal RNA; RNA component of the\n#                       ribonucleoprotein particle (ribosome) which assembles\n#                       amino acids into proteins.\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /operon=\"text\"\n#                       /product=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /standard_name=\"text\"\n\n# Comment               rRNA sizes should be annotated with the /product\n#                       qualifier.  \n\n\n    rrna_entry = {\n            \"type\": \"rRNA\",\n            \"sequence\": rec.id,\n            \"start\": start,\n            \"stop\": stop,\n            \"strand\": strand,\n            \"db_xrefs\": db_xrefs,\n            \"id\": id,\n        }\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\"}\n    single_valued = {\"allele\", \"gene\", \"locus_tag\", \"map\",  \"operon\",  \"old_locus_tag\", \"product\", \"pseudogene\", \"standard_name\"}\n\n    add_optional_qualifiers(rrna_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"pseudo\"]:\n        if flag in qualifiers:\n            rrna_entry[flag] = True\n\n\n    #  rRNA            46413357..46413475\n    #                  /gene=\"n-R5s211\"\n    #                  /product=\"5S ribosomal RNA\"\n    #                  /inference=\"COORDINATES: nucleotide\n    #                  motif:Rfam:12.0:RF00001\"\n    #                  /inference=\"COORDINATES: profile:INFERNAL:1.1.1\"\n    #                  /note=\"Derived by automated computational analysis using\n    #                  gene prediction method: cmsearch.\"\n    #                  /transcript_id=\"XR_004936691.1\"\n    #                  /db_xref=\"GeneID:115487577\"\n    #                  /db_xref=\"RFAM:RF00001\"\n    #                  /db_xref=\"MGI:MGI:4422076\"\n\n    return rrna_entry\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.convert_sig_peptide_feature","title":"<code>convert_sig_peptide_feature(feature, rec, id)</code>","text":"<p>Convert a sig_peptide feature to a Bakta-style feature.</p> <p>mus musculus chrom 1 NC_000067</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The mRNA feature from the GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Bakta-style sig_peptide feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_sig_peptide_feature(feature, rec, id):\n    \"\"\"\n    Convert a sig_peptide feature to a Bakta-style feature.\n\n    mus musculus chrom 1 NC_000067\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The mRNA feature from the GBK.\n        rec: Bio.SeqRecord\n            The record containing the sequence.\n\n    Returns:\n        dict: Bakta-style sig_peptide feature.\n    \"\"\"\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    # Handle CompoundLocation (join)\n    starts = None\n    stops = None\n\n    if feature.location.__class__.__name__ == \"CompoundLocation\":\n        starts = []\n        stops = []\n        for part in feature.location.parts:\n            starts.append(int(part.start) + 1)\n            stops.append(int(part.end))\n\n\n    qualifiers = feature.qualifiers\n\n    so_code =  so.SO_SIGNAL_PEPTIDE.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n\n    sig_peptide_entry = {\n        \"type\": \"sig_peptide\",\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"id\": id,\n\n        # Join support\n        \"starts\": starts,\n        \"stops\": stops,\n\n        # Multi-valued\n        \"db_xrefs\": qualifiers.get(\"db_xref\", []),\n\n    }\n\n\n\n\n# Feature Key           sig_peptide\n\n\n# Definition            signal peptide coding sequence; coding sequence for an\n#                       N-terminal domain of a secreted protein; this domain is\n#                       involved in attaching nascent polypeptide to the\n#                       membrane leader sequence;\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /product=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /standard_name=\"text\"\n\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\"}\n    single_valued = {\"allele\", \"gene\", \"locus_tag\", \"map\",  \"operon\",  \"old_locus_tag\", \"phenotype\", \"product\", \"pseudogene\", \"standard_name\"}\n\n    add_optional_qualifiers(sig_peptide_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"pseudo\"]:\n        if flag in qualifiers:\n            sig_peptide_entry[flag] = True\n\n\n    #  sig_peptide     complement(join(194768584..194768588,\n    #                  194774407..194774533))\n    #                  /gene=\"Cd46\"\n    #                  /gene_synonym=\"Mcp\"\n    #                  /inference=\"COORDINATES: ab initio prediction:SignalP:6.0\"\n\n    return sig_peptide_entry\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.convert_transit_peptide_feature","title":"<code>convert_transit_peptide_feature(feature, rec, id)</code>","text":"<p>Convert a transit_peptide feature to a Bakta-style feature.</p> <p>mus musculus chrom 1 NC_000067</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The mRNA feature from the GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Bakta-style transit_peptide feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_transit_peptide_feature(feature, rec, id):\n    \"\"\"\n    Convert a transit_peptide feature to a Bakta-style feature.\n\n    mus musculus chrom 1 NC_000067\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The mRNA feature from the GBK.\n        rec: Bio.SeqRecord\n            The record containing the sequence.\n\n    Returns:\n        dict: Bakta-style transit_peptide feature.\n    \"\"\"\n\n    # seq = str(rec.seq)\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    # Handle CompoundLocation (join)\n    starts = None\n    stops = None\n\n    if feature.location.__class__.__name__ == \"CompoundLocation\":\n        starts = []\n        stops = []\n        for part in feature.location.parts:\n            starts.append(int(part.start) + 1)\n            stops.append(int(part.end))\n\n\n    qualifiers = feature.qualifiers\n\n    so_code =  so.SO_TRANSIT_PEPTIDE.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n\n    transit_peptide_entry = {\n            \"type\": \"transit_peptide\",\n            \"sequence\": rec.id,\n            \"start\": start,\n            \"stop\": stop,\n            \"strand\": strand,\n            \"id\": id,\n\n            # Join support\n            \"starts\": starts,\n            \"stops\": stops,\n\n            # Multi-valued\n            \"db_xrefs\": qualifiers.get(\"db_xref\", []),\n\n        }\n\n\n\n# Feature Key           transit_peptide\n\n\n# Definition            transit peptide coding sequence; coding sequence for an\n#                       N-terminal domain of a nuclear-encoded organellar\n#                       protein; this domain is involved in post-translational\n#                       import of the protein into the organelle;\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /product=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /standard_name=\"text\"\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\"}\n    single_valued = {\"allele\", \"gene\", \"locus_tag\", \"map\",  \"operon\",  \"old_locus_tag\", \"phenotype\", \"product\", \"pseudogene\", \"standard_name\"}\n\n    add_optional_qualifiers(transit_peptide_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"pseudo\"]:\n        if flag in qualifiers:\n            transit_peptide_entry[flag] = True\n\n    #  transit_peptide complement(join(180006550..180006849,\n    #                  180009627..180009803))\n    #                  /gene=\"Coq8a\"\n    #                  /gene_synonym=\"4632432J16Rik; Adck3; Cabc1; mKIAA0451\"\n    #                  /note=\"Mitochondrion.\n    #                  /evidence=ECO:0000250|UniProtKB:Q8NI60; propagated from\n    #                  UniProtKB/Swiss-Prot (Q60936.2)\"\n\n    return transit_peptide_entry\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.convert_trna_feature","title":"<code>convert_trna_feature(feature, seq_record, id)</code>","text":"<p>Convert a funannotate tRNA SeqFeature to a Bakta tRNA JSON entry.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_trna_feature(feature, seq_record, id):\n    \"\"\"\n    Convert a funannotate tRNA SeqFeature to a Bakta tRNA JSON entry.\n    \"\"\"\n\n    # ------------ Location ------------\n\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    # Handle CompoundLocation (join)\n    starts = None\n    stops = None\n\n    if feature.location.__class__.__name__ == \"CompoundLocation\":\n        starts = []\n        stops = []\n        for part in feature.location.parts:\n            starts.append(int(part.start) + 1)\n            stops.append(int(part.end))\n\n\n\n    # ------------ Extract nt sequence ------------\n    nt_seq = feature.extract(seq_record.seq)\n    nt = str(nt_seq)\n\n    # ------------ Basic qualifiers ------------\n    product = feature.qualifiers.get(\"product\", [None])[0]\n\n    qualifiers = feature.qualifiers\n\n    # fall back to start_stop_strand if there is no locus tag\n    if 'locus_tag' in qualifiers and qualifiers['locus_tag']:\n        locus_tag = qualifiers['locus_tag'][0]\n    else:\n        logger.warning(f\"No locus_tag found for feature {id}\")\n        locus_tag = f\"{GENOME_RANDOM_BACKUP_LOCUSTAG_STR}_{start}_{stop}\"\n        logger.warning(f\"Generating a locus_tag: {locus_tag}\")\n\n    # ------------ amino acid ------------\n    # Prokka product examples:\n    #   \"tRNA-Trp\"\n    #   \"tRNA-Leu\"\n    amino_acid = None\n    if product and product.startswith(\"tRNA-\"):\n        amino_acid = product.split(\"-\")[1]\n\n\n    # ------------ anticodon ------------\n    anti_codon = None\n\n    # anticodons are in notes\n\n    notes = feature.qualifiers.get(\"note\", [])\n\n    # Expect a note like: \"tRNA-Ser(gga)\"\n    for note in notes:\n        # Remove spaces for safety\n        n = note.replace(\" \", \"\")\n\n        # Extract part inside parentheses (anticodon)\n        if \"(\" in n and \")\" in n:\n            anti_codon = n.split(\"(\")[1].split(\")\")[0].lower()\n\n        # Extract amino acid:\n        # tRNA-Ser(gga) \u2192 \"Ser\"\n        if \"tRNA-\" in n:\n            try:\n                # tRNA-Ser(gga) \u2192 \"Ser(gga)\" \u2192 split('(')[0] \u2192 \"Ser\"\n                aa_section = n.split(\"tRNA-\")[1]\n                aa_clean = aa_section.split(\"(\")[0]\n                amino_acid = aa_clean\n            except Exception:\n                pass\n\n    # ------------ Anti-codon position detection ------------\n    # Prokka doesnt have it - dont include\n    # anti_codon_pos = None\n\n    # ------------ score ------------\n    # nothing in prokka\n    score = None\n\n    # ------------ db_xrefs ------------\n    # doesnt exist for prokka\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [])\n    # add so_term\n    so_term = AMINO_ACID_DICT.get(amino_acid.lower(), ('', None))[1]\n\n    if (so_term):\n        db_xrefs.append(so_term.id)\n\n    # ------------ final Bakta-form dict ------------\n    bakta_trna_entry = {\n        \"type\": \"tRNA\",\n        \"sequence\": seq_record.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"gene\": \"trn\" + (amino_acid[0].lower() if amino_acid else \"?\"),\n        \"product\": product,\n        \"amino_acid\": amino_acid,\n        \"anti_codon\": anti_codon,\n        \"score\": score,\n        \"nt\": nt,\n        \"db_xrefs\": db_xrefs,\n       #  \"anti_codon_pos\": anti_codon_pos,  dont include, not in output\n        \"locus\": locus_tag,\n        \"id\": id,\n    }\n\n# Feature Key           tRNA\n\n\n# Definition            mature transfer RNA, a small RNA molecule (75-85 bases\n#                       long) that mediates the translation of a nucleic acid\n#                       sequence into an amino acid sequence;\n\n# Optional qualifiers   /allele=\"text\"\n#                       /anticodon=(pos:&lt;location&gt;,aa:&lt;amino_acid&gt;,seq:&lt;text&gt;)\n#                       /circular_RNA\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /operon=\"text\"\n#                       /product=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /standard_name=\"text\"\n#                       /trans_splicing\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\" }\n    single_valued = {\"allele\",  \"map\",    \"old_locus_tag\", \"standard_name\"}\n\n    qualifiers = feature.qualifiers\n\n    add_optional_qualifiers(bakta_trna_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"circular_RNA\", \"pseudo\", \"trans_splicing\"]:\n        if flag in qualifiers:\n            bakta_trna_entry[flag] = flag in qualifiers\n\n    return bakta_trna_entry\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.convert_utr_region_feature","title":"<code>convert_utr_region_feature(feature, rec, id, three)</code>","text":"<p>Convert a UTR GenBank feature to a simplified Bakta-style feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The UTR feature from the GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The full GenBank record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Simplified Bakta-style feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_utr_region_feature(feature, rec, id, three):\n    \"\"\"\n    Convert a UTR GenBank feature to a simplified Bakta-style feature.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The UTR feature from the GBK.\n        rec: Bio.SeqRecord\n            The full GenBank record containing the sequence.\n\n    Returns:\n        dict: Simplified Bakta-style feature.\n    \"\"\"\n\n    if three:\n        type = \"3'UTR\"\n        so_code =  so.SO_3UTR.id\n    else:\n        type = \"5'UTR\"\n        so_code =  so.SO_5UTR.id\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n\n    qualifiers = feature.qualifiers\n    note = qualifiers.get(\"note\", [None])[0]\n\n\n    # fall back to start_stop_strand if there is no locus tag\n    if 'locus_tag' in qualifiers and qualifiers['locus_tag']:\n        locus_tag = qualifiers['locus_tag'][0]\n    else:\n        logger.warning(f\"No locus_tag found for feature {id}\")\n        locus_tag = f\"{GENOME_RANDOM_BACKUP_LOCUSTAG_STR}_{start}_{stop}\"\n        logger.warning(f\"Generating a locus_tag: {locus_tag}\")\n\n    # always just take the positive strand to get the NT seq (UTR region)\n    seq =  str(rec.seq)\n    nt_seq = seq[start-1:stop]\n\n\n# Feature Key           3'UTR\n\n\n# Definition            1) region at the 3' end of a mature transcript (following \n#                       the stop codon) that is not translated into a protein;\n#                       2) region at the 3' end of an RNA virus (following the last stop\n#                       codon) that is not translated into a protein;\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /standard_name=\"text\"\n#                       /trans_splicing\n\n\n\n# Feature Key           5'UTR\n\n\n# Definition            1) region at the 5' end of a mature transcript (preceding \n#                       the initiation codon) that is not translated into a protein;\n#                       2) region at the 5' end of an RNA virus genome (preceding the first \n#                       initiation codon) that is not translated into a protein;\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /standard_name=\"text\"\n#                       /trans_splicing\n\n    so_code =  so.SO_REPEAT.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n\n    # Minimal Bakta-like structure\n    utr_entry = {\n        \"type\": type,\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand, # matches Bakta and is required\n        \"product\": note, \n        \"nt\": nt_seq, # needed for batka .ffn writeout\n        \"id\": id, # bakta_id needed \n        \"db_xrefs\": db_xrefs,\n        \"locus\": locus_tag\n    }\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\" }\n    single_valued = {\"allele\", \"gene\",   \"map\",  \"old_locus_tag\", \"operon\", \"phenotype\", \"standard_name\"}\n\n    qualifiers = feature.qualifiers\n\n    add_optional_qualifiers(utr_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"pseudo\", \"trans_splicing\"]:\n        if flag in qualifiers:\n            utr_entry[flag] = flag in qualifiers\n\n    return utr_entry\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.get_bakta_style_id_from_locus_tag","title":"<code>get_bakta_style_id_from_locus_tag(records)</code>","text":"<p>Gets 10 char bakta-style ID tag based off the 8 char locus tag in first CDS on the first  record + 2 random chars</p> <p>Assumes all records will have the same locus tag prefix</p> <p>Will always add 2 chars to make ID unique vs locus tag</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def get_bakta_style_id_from_locus_tag(records):\n    \"\"\"\n    Gets 10 char bakta-style ID tag based off the 8 char locus tag in first CDS on the first  record + 2 random chars\n\n    Assumes all records will have the same locus tag prefix\n\n    Will always add 2 chars to make ID unique vs locus tag\n    \"\"\"\n\n    if not records:\n        raise ValueError(\"No GenBank records found.\")\n\n    for record in records:\n\n        for feat in record.features:\n            if feat.type == \"CDS\":\n                locus_tag_list = feat.qualifiers.get(\"locus_tag\") # returns None if doesn't exist\n\n                if locus_tag_list:\n                    locus_tag = locus_tag_list[0]\n\n                    if len(locus_tag) &gt; 7:\n\n                        locus_tag_prefix = locus_tag[:-7] # trims off _000001 from CDS\n\n                        rand_two_chars = random_n_letter_id(2)\n\n                        # by default  locus tag is 8 chars. So this returns a 10 char string (same as bakta defaults)\n\n                        id_tag = f\"{locus_tag_prefix}{rand_two_chars}\"\n\n                        return id_tag\n\n\n                    else:\n                        return random_n_letter_id(10)\n\n                # fallback if locus_tag missing or too short\n                return random_n_letter_id(10)\n\n    # No CDS feature found at all (shouldn't happen)\n    return random_n_letter_id(10)\n</code></pre>"},{"location":"reference/#src.baktfold.io.eukaryotic_to_json.random_n_letter_id","title":"<code>random_n_letter_id(n=4)</code>","text":"<p>generates a n letter id prefix </p> <p>n=2 to append to   locus tag  for bakta id to make it different n=10 if the locus tag is somehow missing (should never happen)</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def random_n_letter_id(n=4):\n    \"\"\"\n    generates a n letter id prefix \n\n    n=2 to append to   locus tag  for bakta id to make it different\n    n=10 if the locus tag is somehow missing (should never happen) \n    \"\"\"\n    return ''.join(random.choices(string.ascii_uppercase, k=n))\n</code></pre>"},{"location":"reference/#src.baktfold.io.io.write_bakta_outputs","title":"<code>write_bakta_outputs(data, features, features_by_sequence, output, prefix, custom_db, euk, has_duplicate_locus, fast, translation_table)</code>","text":"<p>Writes the bakta outputs to a given path.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>The dictionary containing the bakta outputs.</p> required <code>features</code> <code>Sequence[dict]</code> <p>The sequence of dictionaries containing the features.</p> required <code>features_by_sequence</code> <code>Sequence[dict]</code> <p>The sequence of dictionaries containing the features by sequence.</p> required <code>output</code> <code>Path</code> <p>The path to save the bakta outputs to.</p> required <code>prefix</code> <code>str</code> <p>The prefix to use for the bakta outputs.</p> required <code>custom_db</code> <code>bool</code> <p>A boolean indicating whether a custom database is used.</p> required <code>euk</code> <code>bool</code> <p>A boolean indicating whether the sequences are eukaryotic.</p> required <code>has_duplicate_locus</code> <code>bool</code> <p>A boolean indicating whether there are duplicate loci.</p> required <code>fast</code> <code>bool</code> <p>If True, skips AFDB step</p> required <code>translation_table</code> <code>str</code> <p>Translation table inferred from input JSON</p> required <p>Returns:</p> Type Description <p>None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; write_bakta_outputs(data, features, features_by_sequence, output, prefix, custom_db, euk, has_duplicate_locus)\n</code></pre> Source code in <code>src/baktfold/io/io.py</code> <pre><code>def write_bakta_outputs(data: dict, features: Sequence[dict], features_by_sequence: Sequence[dict] , \n                        output: Path, prefix: str, custom_db: bool, euk: bool, has_duplicate_locus: bool,\n                        fast: bool, translation_table: int):\n    \"\"\"\n    Writes the bakta outputs to a given path.\n\n    Args:\n      data (dict): The dictionary containing the bakta outputs.\n      features (Sequence[dict]): The sequence of dictionaries containing the features.\n      features_by_sequence (Sequence[dict]): The sequence of dictionaries containing the features by sequence.\n      output (Path): The path to save the bakta outputs to.\n      prefix (str): The prefix to use for the bakta outputs.\n      custom_db (bool): A boolean indicating whether a custom database is used.\n      euk (bool): A boolean indicating whether the sequences are eukaryotic.\n      has_duplicate_locus (bool): A boolean indicating whether there are duplicate loci.\n      fast (bool): If True, skips AFDB step\n      translation_table (str): Translation table inferred from input JSON\n\n    Returns:\n      None.\n\n    Examples:\n      &gt;&gt;&gt; write_bakta_outputs(data, features, features_by_sequence, output, prefix, custom_db, euk, has_duplicate_locus)\n    \"\"\"\n\n    #logger.info(f'selected features={len(features)}')\n\n    logger.info('writing human readable TSV...')\n    tsv_path: Path = Path(output) / f\"{prefix}.tsv\"\n    tsv.write_features(data['sequences'], features_by_sequence, tsv_path)\n\n    logger.info('writing GFF3...')\n    gff3_path: Path = Path(output) / f\"{prefix}.gff3\"\n    # fix later prokka\n    prokka = False\n    gff.write_features(data, features_by_sequence, gff3_path, prokka, euk)\n\n    logger.info('writing INSDC GenBank &amp; EMBL...')\n    genbank_path: Path = Path(output) / f\"{prefix}.gbff\"\n    embl_path: Path = Path(output) / f\"{prefix}.embl\"\n    insdc.write_features(data, features, genbank_path, embl_path, prokka, euk, translation_table)\n\n    logger.info('writing genome sequences...')\n    fna_path: Path = Path(output) / f\"{prefix}.fna\"\n    fasta.export_sequences(data['sequences'], fna_path, description=True, wrap=True)\n\n    logger.info('writing feature nucleotide sequences...')\n    ffn_path: Path = Path(output) / f\"{prefix}.ffn\"\n    fasta.write_ffn(features, ffn_path)\n\n    logger.info('writing translated CDS sequences...')\n    faa_path: Path = Path(output) / f\"{prefix}.faa\"\n    fasta.write_faa(features, faa_path)\n\n    # inference here is the different databases?\n    annotations_path: Path = Path(output) / f\"{prefix}.inference.tsv\"\n    if custom_db:\n        header_columns = ['Locus', 'Length', 'Product', 'Swissprot', 'AFDBClusters', 'PDB', 'CATH', 'Custom_DB']\n        if has_duplicate_locus:\n            header_columns = ['Locus', 'ID', 'Product', 'Swissprot', 'AFDBClusters', 'PDB', 'CATH', 'Custom_DB']\n    else:\n        header_columns = ['Locus', 'Length', 'Product', 'Swissprot', 'AFDBClusters', 'PDB', 'CATH']\n        if has_duplicate_locus:\n            header_columns = ['Locus', 'ID', 'Product', 'Swissprot', 'AFDBClusters', 'PDB', 'CATH']\n\n    # Remove 'AFDBClusters' if fast is True\n    if fast:\n        header_columns = [col for col in header_columns if col != 'AFDBClusters']\n\n    logger.info(f'Exporting annotations (TSV) to: {annotations_path}')\n\n    selected_features = []\n\n    for seq_id, features in features_by_sequence.items():\n        for feat in features:\n            # get() ensures we don't crash if the key doesn't exist\n            if 'hypothetical' in feat or 'baktfold' in feat:\n                selected_features.append(feat)\n\n\n    tsv.write_protein_features(selected_features, header_columns, annotations_path, custom_db, has_duplicate_locus, fast=fast)\n\n\n\n    cfg.skip_cds = False\n    if(cfg.skip_cds is False):\n\n        # no need to write the hypotheticals I think\n\n        # hypotheticals = [feat for feat in features if feat['type'] == bc.FEATURE_CDS and 'hypothetical' in feat]\n\n\n        # print('writing hypothetical TSV...')\n        # tsv_path: Path = Path(output) / f\"{prefix}.hypotheticals.tsv\"\n        # tsv.write_hypotheticals(hypotheticals, tsv_path)\n\n        # print('writing translated hypothetical CDS sequences...')\n        # print('writing translated CDS sequences...')\n        # faa_path: Path = Path(output) / f\"{prefix}.hypotheticals.faa\"\n        # fasta.write_faa(hypotheticals, faa_path)\n\n        # calc &amp; store runtime\n\n        # run_duration = (cfg.run_end - cfg.run_start).total_seconds()\n        # data['run'] = {\n        #     'start': cfg.run_start.strftime('%Y-%m-%d %H:%M:%S'),\n        #     'end': cfg.run_end.strftime('%Y-%m-%d %H:%M:%S'),\n        #     'duration': f'{(run_duration / 60):.2f} min'\n        # }\n\n        logger.info('write machine readable JSON...')\n        json_path: Path = Path(output) / f\"{prefix}.json\"\n        json.write_json(data, features, json_path)\n</code></pre>"},{"location":"reference/#src.baktfold.io.io.write_bakta_proteins_outputs","title":"<code>write_bakta_proteins_outputs(aas, output, prefix, custom_db, fast)</code>","text":"<p>Writes the bakta protein outputs to a given path.</p> <p>Parameters:</p> Name Type Description Default <code>aas</code> <code>Sequence[dict]</code> <p>The sequence of dictionaries containing the amino acids.</p> required <code>output</code> <code>Path</code> <p>The path to save the bakta protein outputs to.</p> required <code>prefix</code> <code>str</code> <p>The prefix to use for the bakta protein outputs.</p> required <code>custom_db</code> <code>bool</code> <p>A boolean indicating whether a custom database is used.</p> required <code>fast</code> <code>bool</code> <p>If True, skips AFDB step</p> required <p>Returns:</p> Type Description <p>None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; write_bakta_proteins_outputs(aas, output, prefix, custom_db)\n</code></pre> Source code in <code>src/baktfold/io/io.py</code> <pre><code>def write_bakta_proteins_outputs(aas: Sequence[dict], output: Path, prefix: str, custom_db: bool, fast: bool):\n    \"\"\"\n    Writes the bakta protein outputs to a given path.\n\n    Args:\n      aas (Sequence[dict]): The sequence of dictionaries containing the amino acids.\n      output (Path): The path to save the bakta protein outputs to.\n      prefix (str): The prefix to use for the bakta protein outputs.\n      custom_db (bool): A boolean indicating whether a custom database is used.\n      fast (bool): If True, skips AFDB step\n\n    Returns:\n      None.\n\n    Examples:\n      &gt;&gt;&gt; write_bakta_proteins_outputs(aas, output, prefix, custom_db)\n    \"\"\"\n\n\n    annotations_path: Path = Path(output) / f\"{prefix}.tsv\"\n    if custom_db:\n        header_columns = ['ID', 'Length', 'Product', 'Swissprot', 'AFDBClusters', 'PDB', 'CATH', 'Custom_DB']\n    else:\n        header_columns = ['ID', 'Length', 'Product', 'Swissprot', 'AFDBClusters', 'PDB', 'CATH']\n\n    if fast:\n        header_columns = [col for col in header_columns if col != 'AFDBClusters']\n\n\n    logger.info(f'Exporting annotations (TSV) to: {annotations_path}')\n    tsv.write_protein_features(aas, header_columns, annotations_path, custom_db, has_duplicate_locus=False, fast=fast)\n\n\n    # do i combine the tophits tsvs, sort by column, add a column for db and put out as one tsv\n\n    full_annotations_path: Path = Path(output) / f\"{prefix}.json\"\n    logger.info(f'Full annotations (JSON): {full_annotations_path}')\n    json.write_json({'features': aas}, aas, full_annotations_path)\n\n\n    #### don't write hyps I think\n\n    # hypotheticals_path = output_path.joinpath(f'{cfg.prefix}.hypotheticals.tsv')\n    # header_columns = ['ID', 'Length', 'Mol Weight [kDa]', 'Iso El. Point', 'Pfam hits']\n    # hypotheticals = hypotheticals = [aa for aa in aas if 'hypothetical' in aa]\n    # print(f'\\tinformation on hypotheticals (TSV): {hypotheticals_path}')\n    # tsv.write_protein_features(hypotheticals, header_columns, map_hypothetical_columns, hypotheticals_path)\n\n    aa_output_path: Path = Path(output) / f\"{prefix}.faa\"\n    logger.info(f'Annotated sequences (Fasta): {aa_output_path}')\n    fasta.write_faa(aas, aa_output_path)\n</code></pre>"},{"location":"reference/#src.baktfold.io.io.write_foldseek_tophit","title":"<code>write_foldseek_tophit(tophit_df, pdb_tophit_path)</code>","text":"<p>Writes the foldseek tophits to a given path.</p> <p>Parameters:</p> Name Type Description Default <code>tophit_df</code> <code>pd.DataFrame</code> <p>The dataframe containing the foldseek tophits.</p> required <code>pdb_tophit_path</code> <code>Path</code> <p>The path to save the foldseek tophits to.</p> required <p>Returns:</p> Type Description <p>None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; write_foldseek_tophit(tophit_df, pdb_tophit_path)\n</code></pre> Source code in <code>src/baktfold/io/io.py</code> <pre><code>def write_foldseek_tophit(tophit_df: pd.DataFrame, pdb_tophit_path: Path):\n    \"\"\"\n    Writes the foldseek tophits to a given path.\n\n    Args:\n      tophit_df (pd.DataFrame): The dataframe containing the foldseek tophits.\n      pdb_tophit_path (Path): The path to save the foldseek tophits to.\n\n    Returns:\n      None.\n\n    Examples:\n      &gt;&gt;&gt; write_foldseek_tophit(tophit_df, pdb_tophit_path)\n    \"\"\"\n    logger.info(f\"Saving foldseek tophits to {pdb_tophit_path}\")\n    tophit_df.to_csv(pdb_tophit_path, sep=\"\\t\", index=False)\n</code></pre>"},{"location":"reference/#src.baktfold.io.tsv.map_aa_columns","title":"<code>map_aa_columns(feat, custom_db, has_duplicate_locus, fast)</code>","text":"<p>Maps amino acid columns.</p> <p>Parameters:</p> Name Type Description Default <code>feat</code> <code>dict</code> <p>The dictionary containing the features.</p> required <code>custom_db</code> <code>bool</code> <p>A boolean indicating whether a custom database is used.</p> required <code>has_duplicate_locus</code> <code>bool</code> <p>A boolean indicating whether there are duplicate loci.</p> required <code>fast</code> <code>bool</code> <p>A boolean indicating whether AFDBclusters Foldseek search should be skipped</p> required <p>Returns:</p> Type Description <code>Sequence[str]</code> <p>Sequence[str]: A sequence of strings containing the mapped amino acid columns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; map_aa_columns({'locus': 'ABC', 'length': 100, 'product': 'protein'}, False, False)\n['ABC', '100', 'protein', '', '', '', '']\n</code></pre> Source code in <code>src/baktfold/io/tsv.py</code> <pre><code>def map_aa_columns(feat: dict, custom_db: bool, has_duplicate_locus: bool, fast: bool) -&gt; Sequence[str]:\n    \"\"\"\n    Maps amino acid columns.\n\n    Args:\n      feat (dict): The dictionary containing the features.\n      custom_db (bool): A boolean indicating whether a custom database is used.\n      has_duplicate_locus (bool): A boolean indicating whether there are duplicate loci.\n      fast (bool): A boolean indicating whether AFDBclusters Foldseek search should be skipped\n\n    Returns:\n      Sequence[str]: A sequence of strings containing the mapped amino acid columns.\n\n    Examples:\n      &gt;&gt;&gt; map_aa_columns({'locus': 'ABC', 'length': 100, 'product': 'protein'}, False, False)\n      ['ABC', '100', 'protein', '', '', '', '']\n    \"\"\"\n    # Ensure length exists\n    if 'length' not in feat:\n        feat['length'] = int(len(feat['nt']) / 3)\n\n    xrefs = feat.get('db_xrefs', [])\n\n    # Extract dbxref groups once\n    def join_filtered(prefix: str, replacement: str = None):\n        \"\"\"\n    Joins filtered database cross-references.\n\n    Args:\n      prefix (str): The prefix to filter by.\n      replacement (str): The string to replace the prefix with. Defaults to None.\n\n    Returns:\n      str: The joined filtered database cross-references.\n\n    Examples:\n      &gt;&gt;&gt; join_filtered('swissprot', 'afdb_v6:')\n      'afdb_v6:'\n    \"\"\"\n        if replacement is None:\n            replacement = prefix\n        return ','.join(\n            db.replace(replacement, '') for db in xrefs\n            if prefix in db\n        )\n\n    swissprot   = join_filtered('swissprot', 'afdb_v6:')\n    afdbclust   = join_filtered('afdbclusters_', 'afdb_v6:')\n    pdb         = join_filtered('pdb:')\n    cath        = join_filtered('cath:')\n    custom_refs = join_filtered('custom:', 'custom:custom_')\n\n    # Build the output row\n    row = [feat['locus']]\n\n    # add id if multiple CDS per Locus in that record (euks)\n    if has_duplicate_locus:\n        row.append(feat['id'])\n\n    row.extend([\n        str(feat['length']),\n        feat['product'],\n        swissprot,\n    ])\n\n    # Only add AFDBClusters if not in fast mode\n    if not fast:\n        row.append(afdbclust)\n\n    # Always add these\n    row.extend([\n        pdb,\n        cath,\n    ])\n\n    if custom_db:\n        row.append(custom_refs)\n\n    return row\n</code></pre>"},{"location":"reference/#src.baktfold.io.tsv.write_feature_inferences","title":"<code>write_feature_inferences(sequences, features_by_sequence, tsv_path)</code>","text":"<p>Export feature inference statistics in TSV format.</p> Source code in <code>src/baktfold/io/tsv.py</code> <pre><code>def write_feature_inferences(sequences: Sequence[dict], features_by_sequence: Dict[str, dict], tsv_path: Path):\n    \"\"\"Export feature inference statistics in TSV format.\"\"\"\n    logger.info('write tsv: path=%s', tsv_path)\n\n    with tsv_path.open('wt') as fh:\n        fh.write('# Annotated with Bakta\\n')\n        fh.write(f'# Software: v{cfg.version}\\n')\n        fh.write(f\"# Database: v{cfg.version}\\n\") # fix later\n        #fh.write(f\"# Database: v{cfg.db_info['major']}.{cfg.db_info['minor']}, {cfg.db_info['type']}\\n\")\n        fh.write(f'# DOI: {bc.BAKTFOLD_DOI}\\n')\n        fh.write(f'# URL: {bc.BAKTFOLD_URL}\\n')\n        fh.write('#Sequence Id\\tType\\tStart\\tStop\\tStrand\\tLocus Tag\\tScore\\tEvalue\\tQuery Cov\\tSubject Cov\\tId\\tAccession\\n')\n\n        for seq in sequences:\n            for feat in features_by_sequence[seq['id']]:\n                if(feat['type'] in [bc.FEATURE_CDS, bc.FEATURE_SORF]):\n                    score, evalue, query_cov, subject_cov, identity, accession = None, None, None, None, None, '-'\n                    if('ups' in feat or 'ips' in feat):\n                        query_cov = 1\n                        subject_cov = 1\n                        identity = 1\n                        evalue = 0\n                        accession = f\"{bc.DB_XREF_UNIREF}:{feat['ips'][DB_IPS_COL_UNIREF100]}\" if 'ips' in feat else f\"{bc.DB_XREF_UNIPARC}:{feat['ups'][DB_UPS_COL_UNIPARC]}\"\n                    elif('psc' in feat or 'pscc' in feat):\n                        psc_type = 'psc' if 'psc' in feat else 'pscc'\n                        query_cov = feat[psc_type]['query_cov']\n                        subject_cov = feat[psc_type].get('subject_cov', -1)\n                        identity = feat[psc_type]['identity']\n                        score = feat[psc_type].get('score', -1)\n                        evalue = feat[psc_type].get('evalue', -1)\n                        accession = f\"{bc.DB_XREF_UNIREF}:{feat['psc'][DB_PSC_COL_UNIREF90]}\" if 'psc' in feat else f\"{bc.DB_XREF_UNIREF}:{feat['pscc'][DB_PSCC_COL_UNIREF50]}\"\n                    fh.write('\\t'.join(\n                        [\n                            feat['sequence'] if 'sequence' in feat else feat['contig'],  # &lt;1.10.0 compatibility\n                            feat['type'],\n                            str(feat['start']),\n                            str(feat['stop']),\n                            feat['strand'],\n                            feat['locus'],\n                            f\"{score:0.1f}\" if score != None else '-',\n                            ('0.0' if evalue == 0 else f\"{evalue:1.1e}\") if evalue != None else '-',\n                            ('1.0' if query_cov == 1 else f\"{query_cov:0.3f}\") if query_cov != None else '-',\n                            ('1.0' if subject_cov == 1 else f\"{subject_cov:0.3f}\") if subject_cov != None else '-',\n                            ('1.0' if identity == 1 else f\"{identity:0.3f}\") if identity != None else '-',\n                            accession\n                        ])\n                    )\n                    fh.write('\\n')\n                elif(feat['type'] in [bc.FEATURE_T_RNA, bc.FEATURE_R_RNA, bc.FEATURE_NC_RNA, bc.FEATURE_NC_RNA_REGION]):\n                    accession = '-' if feat['type'] == bc.FEATURE_T_RNA else [xref for xref in feat['db_xrefs'] if bc.DB_XREF_RFAM in xref][0]\n                    fh.write('\\t'.join(\n                        [\n                            feat['sequence'] if 'sequence' in feat else feat['contig'],  # &lt;1.10.0 compatibility\n                            feat['type'],\n                            str(feat['start']),\n                            str(feat['stop']),\n                            feat['strand'],\n                            feat['locus'] if 'locus' in feat else '-',\n                            f\"{feat['score']:0.1f}\",\n                            ('0.0' if feat['evalue'] == 0 else f\"{feat['evalue']:1.1e}\") if 'evalue' in feat else '-',\n                            ('1.0' if feat['query_cov'] == 1 else f\"{feat['query_cov']:0.3f}\") if 'query_cov' in feat else '-',\n                            ('1.0' if feat['subject_cov'] == 1 else f\"{feat['subject_cov']:0.3f}\") if 'subject_cov' in feat else '-',\n                            ('1.0' if feat['identity'] == 1 else f\"{feat['identity']:0.3f}\") if 'identity' in feat else '-',\n                            accession\n                        ])\n                    )\n                    fh.write('\\n')\n    return\n</code></pre>"},{"location":"reference/#src.baktfold.io.tsv.write_features","title":"<code>write_features(sequences, features_by_sequence, tsv_path)</code>","text":"<p>Export features in TSV format.</p> Source code in <code>src/baktfold/io/tsv.py</code> <pre><code>def write_features(sequences: Sequence[dict], features_by_sequence: Dict[str, dict], tsv_path: Path):\n    \"\"\"Export features in TSV format.\"\"\"\n    logger.info(f'write feature tsv: path={tsv_path}')\n\n    with tsv_path.open('wt') as fh:\n        fh.write('# Annotated with Baktfold\\n')\n        fh.write(f'# Software: v{cfg.version}\\n')\n        fh.write(f\"# Database: v{cfg.version}\\n\") # fix later\n        #fh.write(f\"# Database: v{cfg.db_info['major']}.{cfg.db_info['minor']}, {cfg.db_info['type']}\\n\")\n        fh.write(f'# DOI: {bc.BAKTFOLD_DOI}\\n')\n        fh.write(f'# URL: {bc.BAKTFOLD_URL}\\n')\n        fh.write('#Sequence Id\\tType\\tStart\\tStop\\tStrand\\tLocus Tag\\tGene\\tProduct\\tDbXrefs\\n')\n\n        for seq in sequences:\n            for feat in features_by_sequence[seq['id']]:\n                seq_id = feat['sequence'] if 'sequence' in feat else feat['contig']  # &lt;1.10.0 compatibility\n                feat_type = feat['type']\n                if(feat_type == bc.FEATURE_GAP):\n                    feat_type = bc.INSDC_FEATURE_ASSEMBLY_GAP if feat['length'] &gt;= 100 else bc.INSDC_FEATURE_GAP\n\n                gene = feat['gene'] if feat.get('gene', None) else ''\n                product = feat.get('product', '')\n                if(bc.PSEUDOGENE in feat):\n                    product = f\"(pseudo) {product}\"\n                elif(feat.get('truncated', '') == bc.FEATURE_END_5_PRIME):\n                    product = f\"(5' truncated) {product}\"\n                elif(feat.get('truncated', '') == bc.FEATURE_END_3_PRIME):\n                    product = f\"(3' truncated) {product}\"\n                elif(feat.get('truncated', '') == bc.FEATURE_END_BOTH):\n                    product = f\"(partial) {product}\"\n\n                def s(x):\n                    return '' if x is None else str(x)\n\n                fh.write('\\t'.join(\n                    [\n                        seq_id,\n                        feat_type,\n                        str(feat['start']),\n                        str(feat['stop']),\n                        str(feat['strand']),\n                        s(feat.get('locus')), # handles None \u2192 ''\n                        s(gene),        # handles None \u2192 ''\n                        s(product),     # handles None \u2192 ''\n                        ', '.join(sorted(feat.get('db_xrefs', [])))\n                    ])\n                )\n                fh.write('\\n')\n                if(feat_type == bc.FEATURE_CRISPR):\n                    i = 0\n                    # spacers and repeats wont exist if Prokka input\n                    spacers = feat.get('spacers', [])\n                    repeat = feat.get('repeat', [])\n\n                    if len(spacers) &gt; 0 and len(repeat) &gt; 0: \n                    # if not - will just skip\n                        while i &lt; len(feat['spacers']):\n                            repeat = feat['repeats'][i]\n                            fh.write('\\t'.join([seq_id, bc.FEATURE_CRISPR_REPEAT, str(repeat['start']), str(repeat['stop']), repeat['strand'], '', '', f\"CRISPR repeat\", '']))\n                            fh.write('\\n')\n                            spacer = feat['spacers'][i]\n                            fh.write('\\t'.join([seq_id, bc.FEATURE_CRISPR_SPACER, str(spacer['start']), str(spacer['stop']), spacer['strand'], '', '', f\"CRISPR spacer, sequence {spacer['sequence']}\", '']))\n                            fh.write('\\n')\n                            i += 1\n                        if(len(feat['repeats']) - 1 == i):\n                            repeat = feat['repeats'][i]\n                            fh.write('\\t'.join([seq_id, bc.FEATURE_CRISPR_REPEAT, str(repeat['start']), str(repeat['stop']), repeat['strand'], '', '', f\"CRISPR repeat\", '']))\n                            fh.write('\\n')\n    return\n</code></pre>"},{"location":"reference/#src.baktfold.io.tsv.write_hypotheticals","title":"<code>write_hypotheticals(hypotheticals, tsv_path)</code>","text":"<p>Export hypothetical information in TSV format.</p> Source code in <code>src/baktfold/io/tsv.py</code> <pre><code>def write_hypotheticals(hypotheticals: Sequence[dict], tsv_path: Path):\n    \"\"\"Export hypothetical information in TSV format.\"\"\"\n    logger.info('write hypothetical tsv: path=%s', tsv_path)\n\n    with tsv_path.open('wt') as fh:\n        fh.write(f'#Annotated with Baktfold v{cfg.version}, https://github.com/oschwengers/bakta\\n')\n        #fh.write(f\"#Database v{cfg.db_info['major']}.{cfg.db_info['minor']}, https://doi.org/10.5281/zenodo.4247252\\n\")\n        fh.write('#Sequence Id\\tStart\\tStop\\tStrand\\tLocus Tag\\tMol Weight [kDa]\\tIso El. Point\\tPfam hits\\tDbxrefs\\n')\n        for hypo in hypotheticals:\n            pfams = [f\"{pfam['id']}|{pfam['name']}\" for pfam in hypo.get('pfams', [])]\n            seq_stats = hypo['seq_stats']\n            mol_weight = f\"{(seq_stats['molecular_weight']/1000):.1f}\" if seq_stats['molecular_weight'] else 'NA'\n            iso_point = f\"{seq_stats['isoelectric_point']:.1f}\" if seq_stats['isoelectric_point'] else 'NA'\n            seq_id = hypo['sequence'] if 'sequence' in hypo else hypo['contig']  # &lt;1.10.0 compatibility\n            fh.write(f\"{seq_id}\\t{hypo['start']}\\t{hypo['stop']}\\t{hypo['strand']}\\t{hypo.get('locus', '')}\\t{mol_weight}\\t{iso_point}\\t{', '.join(sorted(pfams))}\\t{', '.join(sorted(hypo.get('db_xrefs', [])))}\\n\")\n    return\n</code></pre>"},{"location":"reference/#src.baktfold.io.tsv.write_protein_features","title":"<code>write_protein_features(features, header_columns, tsv_path, custom_db, has_duplicate_locus, fast)</code>","text":"<p>Export protein features in TSV format.</p> Source code in <code>src/baktfold/io/tsv.py</code> <pre><code>def write_protein_features(features: Sequence[dict], header_columns: Sequence[str], tsv_path: Path, custom_db: bool, has_duplicate_locus: bool, fast: bool):\n    \"\"\"Export protein features in TSV format.\"\"\"\n    logger.info(f'write protein feature tsv: path={tsv_path}')\n\n    with tsv_path.open('wt') as fh:\n        fh.write(f'#Annotated with Baktfold (v{cfg.version}): https://github.com/gbouras13/baktfold\\n')\n        #fh.write(f\"#Database (v{cfg.db_info['major']}.{cfg.db_info['minor']}): https://doi.org/10.5281/zenodo.4247252\\n\")\n        fh.write('\\t'.join(header_columns))\n        fh.write('\\n')\n        for feat in features:\n            columns = map_aa_columns(feat, custom_db, has_duplicate_locus, fast)\n            fh.write('\\t'.join(columns))\n            fh.write('\\n')\n    return\n</code></pre>"},{"location":"reference/#src.baktfold.io.fasta_in.parse_protein_input","title":"<code>parse_protein_input(input_path, faa_path)</code>","text":"<p>handles regular FASTA and gzipped  returns cds_dict</p> Source code in <code>src/baktfold/io/fasta_in.py</code> <pre><code>def parse_protein_input(input_path, faa_path):\n    \"\"\"\n    handles regular FASTA and gzipped \n    returns cds_dict\n    \"\"\"\n\n    # handles regular FASTA and gzipped \n\n    try:\n        if input_path == '':\n            raise ValueError('File path argument must be non-empty')\n        input_path = Path(input_path).resolve()\n    except:\n        logger.error(f'ERROR: annotation file {input_path} not valid!')\n\n\n    try:\n        logger.info('Parsing input protein sequences...')\n        aas = fasta.import_sequences(input_path, False, False)\n        logger.info(f'Imported sequences={len(aas)}')\n    except:\n        logger.error('ERROR: wrong file format or unallowed characters in amino acid sequences!')\n\n    mock_start = 1\n    for aa in aas:  # rename and mock feature attributes to reuse existing functions\n        aa['type'] = bc.FEATURE_CDS\n        aa['locus'] = aa['id']\n        aa['sequence'] = '-'\n        aa['start'] = mock_start\n        aa['stop'] = mock_start + aa['length'] - 1\n        aa['strand'] = bc.STRAND_UNKNOWN\n        aa['frame'] = 1\n        mock_start += 100\n\n    # write hypothetical proteins to file\n    with faa_path.open('wt') as fh:\n        for aa in aas:\n            fh.write(f\"&gt;{aa['locus']}\\n{aa['aa']}\\n\")\n\n    logger.info('Parsing complete')\n\n    return aas\n</code></pre>"},{"location":"reference/#src.baktfold.io.gff.encode_annotations","title":"<code>encode_annotations(annotations)</code>","text":"<p>Encodes annotations into a string.</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>dict</code> <p>A dictionary containing the annotations.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The encoded annotations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; encode_annotations({\n    'ID': 'EHICP_3230_sigpep',\n    'Name': 'signal peptide',\n    'product': 'signal peptide',\n    'score': 0.5,\n    'Parent': 'EHICP_3230'\n})\n'ID=EHICP_3230_sigpep;Name=signal peptide;product=signal peptide;score=0.5;Parent=EHICP_3230'\n</code></pre> Source code in <code>src/baktfold/io/gff.py</code> <pre><code>def encode_annotations(annotations: Dict[str, Union[str, Sequence[str]]]) -&gt; str:\n    \"\"\"\n    Encodes annotations into a string.\n\n    Args:\n      annotations (dict): A dictionary containing the annotations.\n\n    Returns:\n      str: The encoded annotations.\n\n    Examples:\n      &gt;&gt;&gt; encode_annotations({\n          'ID': 'EHICP_3230_sigpep',\n          'Name': 'signal peptide',\n          'product': 'signal peptide',\n          'score': 0.5,\n          'Parent': 'EHICP_3230'\n      })\n      'ID=EHICP_3230_sigpep;Name=signal peptide;product=signal peptide;score=0.5;Parent=EHICP_3230'\n    \"\"\"\n    annotation_strings = []\n    for key, val in annotations.items():\n        if(type(val) is list):\n            if(len(val) &gt;= 1):\n                val = [encode_attribute(k) for k in val]\n                annotation = f\"{key}={','.join(val)}\"\n                annotation_strings.append(annotation)\n        else:\n            annotation_strings.append(f'{key}={encode_attribute(val)}')\n    return ';'.join(annotation_strings)\n</code></pre>"},{"location":"reference/#src.baktfold.io.gff.encode_attribute","title":"<code>encode_attribute(product)</code>","text":"<p>Replace special characters forbidden in column 9 of the GFF3 format: https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md</p> Source code in <code>src/baktfold/io/gff.py</code> <pre><code>def encode_attribute(product: str) -&gt; str:\n    \"\"\"Replace special characters forbidden in column 9 of the GFF3 format: https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md\"\"\"\n    product = str(product)\n    product = product.replace('%', '%25')\n    product = product.replace(';', '%3B')\n    product = product.replace('=', '%3D')\n    product = product.replace('&amp;', '%26')\n    product = product.replace(',', '%2C')\n    return product\n</code></pre>"},{"location":"reference/#src.baktfold.io.gff.write_euk_cds_feature","title":"<code>write_euk_cds_feature(fh, seq_id, feat)</code>","text":"<p>Write a eukaryotic CDS feature to GFF3 with multiple CDS parts.</p>"},{"location":"reference/#src.baktfold.io.gff.write_euk_cds_feature--parameters","title":"Parameters","text":"<p>fh : file-handle seq_id : str</p> dict-like feature with keys: <p>\"start\", \"stop\", \"strand\", \"locus\", \"starts\", \"stops\"</p> Source code in <code>src/baktfold/io/gff.py</code> <pre><code>def write_euk_cds_feature(fh, seq_id, feat):\n    \"\"\"\n    Write a eukaryotic CDS feature to GFF3 with multiple CDS parts.\n\n    Parameters\n    ----------\n    fh : file-handle\n    seq_id : str\n    feat : dict-like feature with keys:\n            \"start\", \"stop\", \"strand\", \"locus\", \"starts\", \"stops\"\n    \"\"\"\n\n    strand = feat.get(\"strand\", \"+\")\n    locus = feat.get(\"locus\", \"unknown\")\n\n    transcript_id = f\"{locus}-T1\"\n    cds_id = f\"{transcript_id}.cds\"\n\n    starts = feat.get(\"starts\")\n    stops = feat.get(\"stops\")\n\n    # -------------------------------\n    # 1. Determine CDS sub-coordinates\n    # -------------------------------\n    if (\n        isinstance(starts, list)\n        and isinstance(stops, list)\n        and len(starts) == len(stops)\n        and len(starts) &gt; 0\n    ):\n        cds_coords = list(zip(starts, stops))\n    else:\n        cds_coords = [(feat[\"start\"], feat[\"stop\"])]\n\n    # -------------------------------\n    # 2. Reverse order for negative strand\n    # -------------------------------\n    if strand == \"-\":\n        cds_coords.reverse()\n\n    # -------------------------------\n    # 3. Emit CDS lines with correct phase\n    # -------------------------------\n    offset = 0\n\n    for i, (cds_start, cds_stop) in enumerate(cds_coords, start=1):\n\n        length = cds_stop - cds_start + 1\n        phase = offset % 3\n        offset += length\n\n        attr = f\"ID={cds_id}-{i};Parent={transcript_id}\"\n\n        fh.write(\n            f\"{seq_id}\\tbaktfold\\tCDS\\t{cds_start}\\t{cds_stop}\"\n            f\"\\t.\\t{strand}\\t{phase}\\t{attr}\\n\"\n        )\n</code></pre>"},{"location":"reference/#src.baktfold.io.gff.write_euk_repeat_region_feature","title":"<code>write_euk_repeat_region_feature(fh, seq_id, feat)</code>","text":"<p>Writes a repeat region feature to a file.</p> <p>Parameters:</p> Name Type Description Default <code>fh</code> <code>file</code> <p>The file handle to write to.</p> required <code>seq_id</code> <code>str</code> <p>The sequence ID.</p> required <code>feat</code> <code>dict</code> <p>A dictionary containing the feature information.</p> required <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; write_euk_repeat_region_feature(fh, 'DS572673.1', {\n    \"type\": \"repeat_region\",\n    \"sequence\": \"DS571531.1\",\n    \"start\": 1470,\n    \"stop\": 1716,\n    \"strand\": \"?\",\n    \"family\": \"LINE2\",\n    \"rpt_type\": null,\n    \"repeat_unit\": null,\n    \"product\": null,\n    \"nt\": \"AATAAAATCATATCAGAAATAAAAAGAATGAAAATAAACAAATTAAAGAAAATAATTATAAAATTAATAAACGATATTTAAATGAAAGAAAATAGAGAATATGTAATAAGTACAAATGGTTCATTCATTAATAAGAAATTAACAATAATAAAATAGAGAATATTGATTATAAAAAGAAATATATTTCTCAAAACAGTAGAGATACAAAAAGAATAGATATGAAATAAATATTAATTCTAAAATACTC\",\n    \"id\": \"EHICP_3230\",\n    \"db_xrefs\": [\n        \"SO:0000657\"\n    ]\n})\n</code></pre> Source code in <code>src/baktfold/io/gff.py</code> <pre><code>def write_euk_repeat_region_feature(fh, seq_id, feat):\n    \"\"\"\n    Writes a repeat region feature to a file.\n\n    Args:\n      fh (file): The file handle to write to.\n      seq_id (str): The sequence ID.\n      feat (dict): A dictionary containing the feature information.\n\n    Returns:\n      None\n\n    Examples:\n      &gt;&gt;&gt; write_euk_repeat_region_feature(fh, 'DS572673.1', {\n          \"type\": \"repeat_region\",\n          \"sequence\": \"DS571531.1\",\n          \"start\": 1470,\n          \"stop\": 1716,\n          \"strand\": \"?\",\n          \"family\": \"LINE2\",\n          \"rpt_type\": null,\n          \"repeat_unit\": null,\n          \"product\": null,\n          \"nt\": \"AATAAAATCATATCAGAAATAAAAAGAATGAAAATAAACAAATTAAAGAAAATAATTATAAAATTAATAAACGATATTTAAATGAAAGAAAATAGAGAATATGTAATAAGTACAAATGGTTCATTCATTAATAAGAAATTAACAATAATAAAATAGAGAATATTGATTATAAAAAGAAATATATTTCTCAAAACAGTAGAGATACAAAAAGAATAGATATGAAATAAATATTAATTCTAAAATACTC\",\n          \"id\": \"EHICP_3230\",\n          \"db_xrefs\": [\n              \"SO:0000657\"\n          ]\n      })\n    \"\"\"\n\n    start = int(feat['start'])\n    stop  = int(feat['stop'])\n    strand = feat['strand']\n\n    id = feat['sequence']\n\n    attrs = {\n        \"ID\": f\"{id}:{start}..{stop}\",\n        \"gbkey\": \"repeat_region\"\n    }\n\n    if feat.get('family') is not None:\n        attrs[\"rpt_family\"] = feat.get('family')\n\n    attr_str = \";\".join(f\"{k}={v}\" for k, v in attrs.items())\n\n    fh.write(f\"{seq_id}\\tbaktfold\\trepeat_region\\t{start}\\t{stop}\\t.\\t{strand}\\t.\\t{attr_str}\\n\")\n</code></pre>"},{"location":"reference/#src.baktfold.io.gff.write_euk_trna_feature","title":"<code>write_euk_trna_feature(fh, seq_id, feat)</code>","text":"<p>Write a tRNA feature to GFF3 with a top-level line and single exon.</p>"},{"location":"reference/#src.baktfold.io.gff.write_euk_trna_feature--parameters","title":"Parameters","text":"file-like <p>Open file handle to write GFF lines.</p> str <p>Sequence/contig ID.</p> SeqFeature <p>Biopython SeqFeature object of type 'tRNA'.</p>"},{"location":"reference/#src.baktfold.io.gff.write_euk_trna_feature--notes","title":"Notes","text":"<ul> <li>Generates one tRNA line and one exon line.</li> <li>Includes optional 'product' qualifier.</li> </ul> Source code in <code>src/baktfold/io/gff.py</code> <pre><code>def write_euk_trna_feature(fh, seq_id, feat):\n    \"\"\"\n    Write a tRNA feature to GFF3 with a top-level line and single exon.\n\n    Parameters\n    ----------\n    fh : file-like\n        Open file handle to write GFF lines.\n    seq_id : str\n        Sequence/contig ID.\n    feat : SeqFeature\n        Biopython SeqFeature object of type 'tRNA'.\n\n    Notes\n    -----\n    - Generates one tRNA line and one exon line.\n    - Includes optional 'product' qualifier.\n    \"\"\"\n    start = int(feat['start'])\n    stop  = int(feat['stop'])\n\n    strand = feat['strand']\n\n    locus = feat['locus']\n\n    trna_id = f\"{locus}-T1\"\n\n    # Top-level tRNA attributes\n    attrs = {\n        \"ID\": trna_id,\n        \"Parent\": locus\n    }\n\n    attrs = {}\n\n    product = feat.get(\"product\", [])\n\n    if product:\n\n        key = \"product\"         \n        if isinstance(product, list):\n            if len(product) == 1:\n                attrs[key] = str(product[0])\n            else:\n                attrs[key] = \",\".join(str(v) for v in product)\n        else:\n            attrs[key] = str(product)\n\n\n    attr_str = \";\".join(f\"{k}={v}\" for k, v in attrs.items())\n\n    # Write top-level tRNA line\n    fh.write(f\"{seq_id}\\tbaktfold\\ttRNA\\t{start}\\t{stop}\\t.\\t{strand}\\t.\\t{attr_str}\\n\")\n\n    # Write exon line (tRNA single-exon)\n    exon_id = f\"{trna_id}.ex\u00dfon1\"\n    exon_attrs = f\"ID={exon_id};Parent={trna_id}\"\n    fh.write(f\"{seq_id}\\tbaktfold\\texon\\t{start}\\t{stop}\\t.\\t{strand}\\t.\\t{exon_attrs}\\n\")\n</code></pre>"},{"location":"reference/#src.baktfold.io.gff.write_euk_utr_feature","title":"<code>write_euk_utr_feature(fh, seq_id, feat, locus_counter, three=False)</code>","text":"<p>Write a 'utr' feature.</p> Source code in <code>src/baktfold/io/gff.py</code> <pre><code>def write_euk_utr_feature(fh, seq_id, feat, locus_counter, three=False):\n    \"\"\"Write a 'utr' feature.\"\"\"\n    start = int(feat['start'])\n    stop  = int(feat['stop'])\n    strand = feat['strand']\n\n    locus = feat['locus']\n\n    # Count occurrences for this locus\n    count = locus_counter.get(locus, 0) + 1\n    locus_counter[locus] = count\n\n    # Construct ID with suffix -2, -3, etc.\n    # For first entry we keep ID=locus (no -1)\n    if count == 1:\n        utr_id = locus\n    else:\n        utr_id = f\"{locus}-{count}\"\n\n    # Top-level mRNA line\n    attrs = {\n        \"ID\": f\"{utr_id}\",\n        \"Parent\": f\"{locus}\",\n    }\n\n# CAMXCT020000566.1\tEMBL\tthree_prime_UTR\t84568\t84617\t.\t-\t.\tID=id-C1SCF055_LOCUS8420;Parent=gene-C1SCF055_LOCUS8420;Note=ID:SCF055_s1507_g28601.utr3p1%3B~source:feature;gbkey=3'UTR;locus_tag=C1SCF055_LOCUS8420\n# CAMXCT020000566.1\tEMBL\tfive_prime_UTR\t136251\t136259\t.\t-\t.\tID=id-C1SCF055_LOCUS8420-2;Parent=gene-C1SCF055_LOCUS8420;Note=ID:SCF055_s1507_g28601.utr5p1%3B~source:feature;gbkey=5'UTR;locus_tag=C1SCF055_LOCUS8420\n\n    if feat.get('Note') is not None:\n        attrs[\"Note\"] = feat.get('note')\n\n\n    attrs[\"gbkey\"] = \"3'UTR\" if three else \"5'UTR\"\n\n    if feat.get('Note') is not None:\n        attrs[\"locus_tag\"] = feat.get('locus')\n\n    attr_str = \";\".join(f\"{k}={v}\" for k, v in attrs.items())\n\n    if three:\n        gene_tag = 'three_prime_UTR'\n    else:\n        gene_tag = 'five_prime_UTR'\n\n    fh.write(f\"{seq_id}\\tbaktfold\\t{gene_tag}\\t{start}\\t{stop}\\t.\\t{strand}\\t.\\t{attr_str}\\n\")\n</code></pre>"},{"location":"reference/#src.baktfold.io.gff.write_features","title":"<code>write_features(data, features_by_sequence, gff3_path, prokka=False, euk=False)</code>","text":"<p>Export features in GFF3 format.</p> Source code in <code>src/baktfold/io/gff.py</code> <pre><code>def write_features(data: dict, features_by_sequence: Dict[str, dict], gff3_path: Path, prokka: bool = False, euk: bool = False):\n    \"\"\"Export features in GFF3 format.\"\"\"\n    logger.info(f'write features: path={gff3_path}')\n\n    with gff3_path.open('wt') as fh:\n        fh.write('##gff-version 3\\n')  # GFF version\n        fh.write('##feature-ontology https://github.com/The-Sequence-Ontology/SO-Ontologies/blob/v3.1/so.obo\\n')  # SO feature version\n\n        if(data['genome'].get('taxon', None)):  # write organism info\n            fh.write(f\"# organism {data['genome']['taxon']}\\n\")\n\n        fh.write('# Annotated with Baktfold\\n')\n        fh.write(f'# Software: v{cfg.version}\\n')\n        fh.write(f\"# Database: v{cfg.version}\\n\") # fix later\n        #fh.write(f\"# Database: v{cfg.db_info['major']}.{cfg.db_info['minor']}, {cfg.db_info['type']}\\n\")\n        fh.write(f'# DOI: {bc.BAKTFOLD_DOI}\\n')\n        fh.write(f'# URL: {bc.BAKTFOLD_URL}\\n')\n\n        for seq in data['sequences']:  # write features\n            if euk:\n                locus_counter = {} # for UTRs\n\n            fh.write(f\"##sequence-region {seq['id']} 1 {seq['length']}\\n\")  # sequence region\n\n            # write landmark region\n            annotations = {\n                'ID': seq['id'],\n                'Name': seq['id']\n            }\n            if(seq['topology'] == bc.TOPOLOGY_CIRCULAR):\n                annotations['Is_circular'] = 'true'\n            annotations = encode_annotations(annotations)\n            fh.write(f\"{seq['id']}\\tBaktfold\\tregion\\t1\\t{str(seq['length'])}\\t.\\t+\\t.\\t{annotations}\\n\")\n\n            for feat in features_by_sequence[seq['id']]:\n                seq_id = feat['sequence'] if 'sequence' in feat else feat['contig']  # &lt;1.10.0 compatibility\n                start = feat['start']\n                stop = feat['stop']\n                if('edge' in feat):\n                    stop += seq['length']\n\n                # euks\n                if euk:\n                    if(feat['type'] == bc.FEATURE_REPEAT):\n                        write_euk_repeat_region_feature(fh, seq_id, feat)\n\n                    if(feat['type'] == bc.FEATURE_5UTR or feat['type'] == bc.FEATURE_3UTR):\n                        if feat['type'] == bc.FEATURE_3UTR:\n                            write_euk_utr_feature(fh, seq_id, feat, locus_counter, three=True)\n                        elif feat['type'] == bc.FEATURE_5UTR:\n                            write_euk_utr_feature(fh, seq_id, feat, locus_counter, three=False)\n\n                if(feat['type'] == bc.FEATURE_T_RNA):\n\n                    if euk:\n                        write_euk_trna_feature(fh, seq_id, feat)\n                    else:\n\n                        trna_tool = \"tRNAscan-SE\"\n                        if prokka:\n                            trna_tool = \"Aragorn\"\n                        annotations = {\n                            'ID': feat['locus'],\n                            'Name': feat['product'],\n                            'locus_tag': feat['locus'],\n                            'product': feat['product'],\n                            'Dbxref': feat['db_xrefs']\n                        }\n                        if(feat.get('gene', None)):  # add gene annotation if available\n                            annotations['gene'] = feat['gene']\n                        if(bc.PSEUDOGENE in feat):\n                            annotations[bc.INSDC_FEATURE_PSEUDOGENE] = bc.INSDC_FEATURE_PSEUDOGENE_TYPE_UNKNOWN\n                        elif('truncated' in feat):\n                            annotations[bc.INSDC_FEATURE_PSEUDO] = True\n                        if(feat.get('anti_codon', False)):\n                            annotations['anti_codon'] = feat['anti_codon']\n                        if(feat.get('amino_acid', False)):\n                            annotations['amino_acid'] = feat['amino_acid']\n                        if(cfg.compliant):\n                            gene_id = f\"{feat['locus']}_gene\"\n                            annotations['Parent'] = gene_id\n                            annotations['inference'] = 'profile:tRNAscan:2.0'\n                            annotations['Dbxref'], annotations['Note'] = insdc.revise_dbxref_insdc(feat['db_xrefs'])  # remove INSDC invalid DbXrefs\n                            gene_annotations = {\n                                'ID': gene_id,\n                                'locus_tag': feat['locus']\n                            }\n                            if(feat.get('gene', None)):\n                                gene_annotations['gene'] = feat['gene']\n                            if(bc.PSEUDOGENE in feat):\n                                gene_annotations[bc.INSDC_FEATURE_PSEUDOGENE] = bc.INSDC_FEATURE_PSEUDOGENE_TYPE_UNKNOWN\n                            gene_annotations = encode_annotations(gene_annotations)\n                            fh.write(f\"{seq_id}\\t{trna_tool}\\tgene\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{gene_annotations}\\n\")\n                        annotations = encode_annotations(annotations)\n                        fh.write(f\"{seq_id}\\t{trna_tool}\\t{so.SO_TRNA.name}\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{annotations}\\n\")\n                elif(feat['type'] == bc.FEATURE_TM_RNA):\n                    # both prokka and bakta use Aragorn\n                    annotations = {\n                        'ID': feat['locus'],\n                        'Name': feat['product'],\n                        'locus_tag': feat['locus'],\n                        'gene': feat['gene'],\n                        'product': feat['product'],\n                        'Dbxref': feat['db_xrefs']\n                    }\n                    if('tag' in feat):\n                        annotations['tag_peptide'] = feat['tag']['aa']\n                    if('truncated' in feat):\n                        annotations[bc.INSDC_FEATURE_PSEUDO] = True\n                    if(cfg.compliant):\n                        gene_id = f\"{feat['locus']}_gene\"\n                        annotations['Parent'] = gene_id\n                        annotations['inference'] = 'profile:aragorn:1.2'\n                        annotations['Dbxref'], annotations['Note'] = insdc.revise_dbxref_insdc(feat['db_xrefs'])  # remove INSDC invalid DbXrefs\n                        if('tag' in feat):\n                            annotations['tag_peptide'] = f\"{feat['tag']['start']}..{feat['tag']['stop']}\" if feat['strand'] == bc.STRAND_FORWARD else f\"complement({feat['tag']['start']}..{feat['tag']['stop']})\"\n                        gene_annotations = {\n                            'ID': gene_id,\n                            'locus_tag': feat['locus'],\n                            'gene': feat['gene']\n                        }\n                        if('truncated' in feat):\n                            gene_annotations[bc.INSDC_FEATURE_PSEUDO] = True\n                        gene_annotations = encode_annotations(gene_annotations)\n                        fh.write(f\"{seq_id}\\tAragorn\\tgene\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{gene_annotations}\\n\")\n                    annotations = encode_annotations(annotations)\n                    fh.write(f\"{seq_id}\\tAragorn\\t{so.SO_TMRNA.name}\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{annotations}\\n\")\n                elif(feat['type'] == bc.FEATURE_R_RNA):\n                    rrna_tool = \"Infernal\"\n                    if prokka:\n                        rrna_tool = \"barrnap\"\n                    annotations = {\n                        'ID': feat['locus'],\n                        'Name': feat['product'],\n                        'locus_tag': feat['locus'],\n                        'gene': feat['gene'],\n                        'product': feat['product'],\n                        'Dbxref': feat['db_xrefs']\n                    }\n                    if('truncated' in feat):\n                        annotations[bc.INSDC_FEATURE_PSEUDO] = True\n                    if(cfg.compliant):\n                        gene_id = f\"{feat['locus']}_gene\"\n                        annotations['Parent'] = gene_id\n                        annotations['Dbxref'], annotations['Note'] = insdc.revise_dbxref_insdc(feat['db_xrefs'])  # remove INSDC invalid DbXrefs\n                        for rfam_id in [dbxref.split(':')[1] for dbxref in feat['db_xrefs'] if dbxref.split(':')[0] == bc.DB_XREF_RFAM]:\n                            annotations['inference'] = f'profile:Rfam:{rfam_id}'\n                        gene_annotations = {\n                            'ID': gene_id,\n                            'locus_tag': feat['locus'],\n                            'gene': feat['gene']\n                        }\n                        if('truncated' in feat):\n                            gene_annotations[bc.INSDC_FEATURE_PSEUDO] = True\n                        gene_annotations = encode_annotations(gene_annotations)\n                        fh.write(f\"{seq_id}\\t{rrna_tool}\\tgene\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{gene_annotations}\\n\")\n                    annotations = encode_annotations(annotations)\n                    fh.write(f\"{seq_id}\\t{rrna_tool}\\t{so.SO_RRNA.name}\\t{start}\\t{stop}\\t{feat['evalue']}\\t{feat['strand']}\\t.\\t{annotations}\\n\")\n                elif(feat['type'] == bc.FEATURE_NC_RNA):\n                    # both prokka and bakta use infernal for ncrna\n                    annotations = {\n                        'ID': feat['locus'],\n                        'Name': feat['product'],\n                        'locus_tag': feat['locus'],\n                        'gene': feat['gene'],\n                        'product': feat['product'],\n                        'Dbxref': feat['db_xrefs']\n                    }\n                    if('truncated' in feat):\n                        annotations[bc.INSDC_FEATURE_PSEUDO] = True\n                    if(cfg.compliant):\n                        gene_id = f\"{feat['locus']}_gene\"\n                        annotations['Parent'] = gene_id\n                        annotations['Dbxref'], annotations['Note'] = insdc.revise_dbxref_insdc(feat['db_xrefs'])  # remove INSDC invalid DbXrefs\n                        annotations[bc.INSDC_FEATURE_NC_RNA_CLASS] = insdc.select_ncrna_class(feat)\n                        for rfam_id in [dbxref.split(':')[1] for dbxref in feat['db_xrefs'] if dbxref.split(':')[0] == bc.DB_XREF_RFAM]:\n                            annotations['inference'] = f'profile:Rfam:{rfam_id}'\n                        gene_annotations = {\n                            'ID': gene_id,\n                            'locus_tag': feat['locus'],\n                            'gene': feat['gene']\n                        }\n                        if(ba.RE_GENE_SYMBOL.fullmatch(feat['gene'])):  # discard non-standard ncRNA gene symbols\n                            gene_annotations['gene'] = feat['gene']\n                        else:\n                            annotations.pop('gene', None)\n                        if('truncated' in feat):\n                            gene_annotations[bc.INSDC_FEATURE_PSEUDO] = True\n                        gene_annotations = encode_annotations(gene_annotations)\n                        fh.write(f\"{seq_id}\\tInfernal\\tgene\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{gene_annotations}\\n\")\n                    annotations = encode_annotations(annotations)\n                    fh.write(f\"{seq_id}\\tInfernal\\t{so.SO_NCRNA_GENE.name}\\t{start}\\t{stop}\\t{feat['evalue']}\\t{feat['strand']}\\t.\\t{annotations}\\n\")\n                elif(feat['type'] == bc.FEATURE_NC_RNA_REGION):\n                    annotations = {\n                        'ID': feat['id'],\n                        'Name': feat['product'],\n                        'product': feat['product'],\n                        'Dbxref': feat['db_xrefs']\n                    }\n                    if('truncated' in feat):\n                        annotations[bc.INSDC_FEATURE_PSEUDO] = True\n                    if(cfg.compliant):\n                        for rfam_id in [dbxref.split(':')[1] for dbxref in feat['db_xrefs'] if dbxref.split(':')[0] == bc.DB_XREF_RFAM]:\n                            annotations['inference'] = f'profile:Rfam:{rfam_id}'\n                        annotations['Dbxref'], annotations['Note'] = insdc.revise_dbxref_insdc(feat['db_xrefs'])  # remove INSDC invalid DbXrefs\n                        annotations[bc.INSDC_FEATURE_REGULATORY_CLASS] = insdc.select_regulatory_class(feat)\n                    annotations = encode_annotations(annotations)\n                    fh.write(f\"{seq_id}\\tInfernal\\t{so.SO_REGULATORY_REGION.name}\\t{start}\\t{stop}\\t{feat['evalue']}\\t{feat['strand']}\\t.\\t{annotations}\\n\")\n                elif(feat['type'] == bc.FEATURE_CRISPR):\n                    crispr_tool = \"PILER-CR\"\n                    if prokka:\n                        crispr_tool = \"MinCED\"\n                    annotations = {\n                        'ID': feat['id'],\n                        'Name': feat['product'],\n                        'product': feat['product']\n                    }\n                    feat_type = so.SO_CRISPR.name\n                    if(cfg.compliant):\n                        feat_type = bc.INSDC_FEATURE_REPEAT_REGION\n                        annotations['inference'] = 'COORDINATES:alignment:pilercr:1.02'\n                        annotations['Dbxref'], annotations['Note'] = insdc.revise_dbxref_insdc(feat['db_xrefs'])  # remove INSDC invalid DbXrefs\n                        annotations[bc.INSDC_FEATURE_REPEAT_FAMILY] = 'CRISPR'\n                        annotations[bc.INSDC_FEATURE_REPEAT_TYPE] = 'direct'\n                        annotations[bc.INSDC_FEATURE_REPEAT_UNIT_SEQ] = feat['repeat_consensus']\n                    annotations = encode_annotations(annotations)\n                    fh.write(f\"{seq_id}\\t{crispr_tool}\\t{feat_type}\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{annotations}\\n\")\n                    if(not cfg.compliant):\n                        i = 0\n                        # spacers and repeats wont exist if Prokka input\n                        spacers = feat.get('spacers', [])\n                        repeat = feat.get('repeat', [])\n                        if len(spacers) &gt; 0 and len(repeat) &gt; 0: \n                            while i &lt; len(feat['spacers']):\n                                repeat = feat['repeats'][i]\n                                annotations = {\n                                    'ID': f\"{feat['id']}_repeat_{i+1}\",\n                                    'Parent': feat['id']\n                                }\n                                annotations = encode_annotations(annotations)\n                                # will always be PILER here as prokka won't have any\n                                fh.write(f\"{seq_id}\\tPILER-CR\\t{bc.FEATURE_CRISPR_REPEAT}\\t{repeat['start']}\\t{repeat['stop']}\\t.\\t{repeat['strand']}\\t.\\t{annotations}\\n\")\n                                spacer = feat['spacers'][i]\n                                annotations = {\n                                    'ID': f\"{feat['id']}_spacer_{i+1}\",\n                                    'Parent': feat['id'],\n                                    'sequence': spacer['sequence']\n                                }\n                                annotations = encode_annotations(annotations)\n                                fh.write(f\"{seq_id}\\tPILER-CR\\t{bc.FEATURE_CRISPR_SPACER}\\t{spacer['start']}\\t{spacer['stop']}\\t.\\t{spacer['strand']}\\t.\\t{annotations}\\n\")\n                                i += 1\n                            if(len(feat['repeats']) - 1 == i):\n                                repeat = feat['repeats'][i]\n                                annotations = { 'ID': f\"{feat['id']}_repeat_{i+1}\" }\n                                annotations = encode_annotations(annotations)\n                                fh.write(f\"{seq_id}\\tPILER-CR\\t{bc.FEATURE_CRISPR_REPEAT}\\t{repeat['start']}\\t{repeat['stop']}\\t.\\t{repeat['strand']}\\t.\\t{annotations}\\n\")\n                elif feat['type'] == bc.FEATURE_CDS:\n                    if euk:\n                        write_euk_cds_feature(fh, seq_id, feat)\n                    else:\n                        annotations = {\n                            'ID': feat['locus'],\n                            'Name': feat['product'],\n                            'locus_tag': feat['locus'],\n                            'product': feat['product'],\n                            'Dbxref': feat['db_xrefs']\n                        }\n                        if(bc.PSEUDOGENE in feat):\n                            annotations[bc.INSDC_FEATURE_PSEUDOGENE] = bc.INSDC_FEATURE_PSEUDOGENE_TYPE_UNPROCESSED if feat[bc.PSEUDOGENE]['paralog'] else bc.INSDC_FEATURE_PSEUDOGENE_TYPE_UNITARY\n                        elif('truncated' in feat):\n                            annotations[bc.INSDC_FEATURE_PSEUDO] = True\n                        if(feat.get('gene', None)):  # add gene annotation if available\n                            annotations['gene'] = feat['gene']\n                        source = '?' if feat.get('source', None) == bc.CDS_SOURCE_USER else 'Pyrodigal'\n                        if prokka: \n                            source = 'Prodigal'\n                        if(cfg.compliant):\n                            gene_id = f\"{feat['locus']}_gene\"\n                            annotations['Parent'] = gene_id\n                            annotations['inference'] = 'EXISTENCE:non-experimental evidence, no additional details recorded' if feat.get('source', None) == bc.CDS_SOURCE_USER else 'ab initio prediction:Pyrodigal:3.5'\n                            annotations['Dbxref'], annotations['Note'] = insdc.revise_dbxref_insdc(feat['db_xrefs'])  # remove INSDC invalid DbXrefs\n                            annotations['Note'], ec_number = insdc.extract_ec_from_notes_insdc(annotations, 'Note')\n                            if(ec_number is not None):\n                                annotations['ec_number'] = ec_number\n                            gene_annotations = {\n                                'ID': gene_id,\n                                'locus_tag': feat['locus']\n                            }\n                            if(feat.get('gene', None)):\n                                gene_annotations['gene'] = feat['gene']\n                            if(bc.PSEUDOGENE in feat):\n                                gene_annotations[bc.INSDC_FEATURE_PSEUDOGENE] = bc.INSDC_FEATURE_PSEUDOGENE_TYPE_UNPROCESSED if feat[bc.PSEUDOGENE]['paralog'] else bc.INSDC_FEATURE_PSEUDOGENE_TYPE_UNITARY\n                            gene_annotations = encode_annotations(gene_annotations)\n                            fh.write(f\"{seq_id}\\t{source}\\tgene\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{gene_annotations}\\n\")\n                        if('exception' in feat):\n                            ex = feat['exception']\n                            pos = f\"{ex['start']}..{ex['stop']}\"\n                            if(feat['strand'] == bc.STRAND_REVERSE):\n                                pos = f\"complement({pos})\"\n                            annotations['transl_except']=f\"(pos:{pos},aa:{ex['aa']})\"\n                            notes = annotations.get('Note', [])\n                            notes.append(f\"codon on position {ex['codon_position']} is a {ex['type']} codon\")\n                            if('Notes' not in annotations):\n                                annotations['Note'] = notes\n                        annotations = encode_annotations(annotations)\n                        fh.write(f\"{seq_id}\\t{source}\\t{so.SO_CDS.name}\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t0\\t{annotations}\\n\")\n                        if(bc.FEATURE_SIGNAL_PEPTIDE in feat):\n                            write_signal_peptide(fh, feat)\n                elif(feat['type'] == bc.FEATURE_SORF):\n                    annotations = {\n                        'ID': feat['locus'],\n                        'Name': feat['product'],\n                        'locus_tag': feat['locus'],\n                        'product': feat['product'],\n                        'Dbxref': feat['db_xrefs']\n                    }\n                    if(feat.get('gene', None)):  # add gene annotation if available\n                        annotations['gene'] = feat['gene']\n                    if(cfg.compliant):\n                        gene_id = f\"{feat['locus']}_gene\"\n                        annotations['Parent'] = gene_id\n                        annotations['Dbxref'], annotations['Note'] = insdc.revise_dbxref_insdc(feat['db_xrefs'])  # remove INSDC invalid DbXrefs\n                        annotations['Note'], ec_number = insdc.extract_ec_from_notes_insdc(annotations, 'Note')\n                        if(ec_number is not None):\n                            annotations['ec_number'] = ec_number\n                        gene_annotations = {\n                            'ID': gene_id,\n                            'locus_tag': feat['locus'],\n                            'inference': 'ab initio prediction:Bakta'\n                        }\n                        if(feat.get('gene', None)):\n                            gene_annotations['gene'] = feat['gene']\n                        gene_annotations = encode_annotations(gene_annotations)\n                        fh.write(f\"{seq_id}\\tBakta\\tgene\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{gene_annotations}\\n\")\n                    annotations = encode_annotations(annotations)\n                    fh.write(f\"{seq_id}\\tBakta\\t{so.SO_CDS.name}\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t0\\t{annotations}\\n\")\n                    if(bc.FEATURE_SIGNAL_PEPTIDE in feat):\n                        write_signal_peptide(fh, feat)\n                elif(feat['type'] == bc.FEATURE_GAP):\n                    gap_tool=\"Bakta\"\n                    if prokka:\n                        gap_tool=\"Prokka\"\n                    annotations = {\n                        'ID': feat['id'],\n                        'Name': f\"gap ({feat['length']} bp)\",\n                        'product': f\"gap ({feat['length']} bp)\"\n                    }\n                    annotations = encode_annotations(annotations)\n                    fh.write(f\"{seq_id}\\t{gap_tool}\\t{so.SO_GAP.name}\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{annotations}\\n\")\n                elif(feat['type'] == bc.FEATURE_ORIC):\n                    annotations = {\n                        'ID': feat['id'],\n                        'Name': feat['product']\n                    }\n                    if(cfg.compliant):\n                        annotations['Note'] = feat['product']\n                    else:\n                        annotations['product'] = feat['product']\n                        annotations['inference'] = 'similar to DNA sequence'\n                    annotations = encode_annotations(annotations)\n                    feat_type = bc.INSDC_FEATURE_ORIGIN_REPLICATION if cfg.compliant else so.SO_ORIC.name\n                    fh.write(f\"{seq_id}\\tBLAST+\\t{feat_type}\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{annotations}\\n\")\n                elif(feat['type'] == bc.FEATURE_ORIV):\n                    annotations = {\n                        'ID': feat['id'],\n                        'Name': feat['product']\n                    }\n                    if(cfg.compliant):\n                        annotations['Note'] = feat['product']\n                    else:\n                        annotations['product'] = feat['product']\n                        annotations['inference'] = 'similar to DNA sequence'\n                    annotations = encode_annotations(annotations)\n                    feat_type = bc.INSDC_FEATURE_ORIGIN_REPLICATION if cfg.compliant else so.SO_ORIC.name\n                    fh.write(f\"{seq_id}\\tBLAST+\\t{feat_type}\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{annotations}\\n\")\n                elif(feat['type'] == bc.FEATURE_ORIT):\n                    annotations = {\n                        'ID': feat['id'],\n                        'Name': feat['product']\n                    }\n                    if(cfg.compliant):\n                        annotations['Note'] = feat['product']\n                    else:\n                        annotations['product'] = feat['product']\n                        annotations['inference'] = 'similar to DNA sequence'\n                    annotations = encode_annotations(annotations)\n                    feat_type = bc.INSDC_FEATURE_ORIGIN_TRANSFER if cfg.compliant else so.SO_ORIT.name\n                    fh.write(f\"{seq_id}\\tBLAST+\\t{feat_type}\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{annotations}\\n\")\n                elif(feat['type'] == bc.FEATURE_GENE):\n                    write_gene_feature(fh, seq_id, feat)\n                elif(feat['type'] == bc.FEATURE_MRNA):\n                    write_mrna_feature(fh, seq_id, feat)\n\n        if(not cfg.compliant):\n            fh.write('##FASTA\\n')\n            for seq in data['sequences']:  # write sequences\n                fh.write(f\"&gt;{seq['id']}\\n\")\n                seq_nt = seq['nt'] if 'nt' in seq else seq['sequence']  # &lt;1.10.0 compatibility\n                fh.write(fasta.wrap_sequence(seq_nt))\n    return\n</code></pre>"},{"location":"reference/#src.baktfold.io.gff.write_gene_feature","title":"<code>write_gene_feature(fh, seq_id, feat)</code>","text":"<p>Write a 'gene' feature including fuzzy boundaries.</p> Source code in <code>src/baktfold/io/gff.py</code> <pre><code>def write_gene_feature(fh, seq_id, feat):\n    \"\"\"Write a 'gene' feature including fuzzy boundaries.\"\"\"\n    start = int(feat['start'])\n    stop  = int(feat['stop'])\n    strand = feat['strand']\n\n    # fall back if there is no locus tag\n    locus = feat.get('locus') or f\"{seq_id}_{start}_{stop}_{strand}\"\n\n    attrs = {\n        \"ID\": f\"{locus}\"\n    }\n\n    if feat.get('gene') is not None:\n        attrs[\"Name\"] = feat.get('gene')\n\n    attr_str = \";\".join(f\"{k}={v}\" for k, v in attrs.items())\n\n    fh.write(f\"{seq_id}\\tbaktfold\\tgene\\t{start}\\t{stop}\\t.\\t{strand}\\t.\\t{attr_str}\\n\")\n</code></pre>"},{"location":"reference/#src.baktfold.io.gff.write_mrna_feature","title":"<code>write_mrna_feature(fh, seq_id, feat)</code>","text":"<p>Write mRNA + implied exons based on join() structure.</p> Source code in <code>src/baktfold/io/gff.py</code> <pre><code>def write_mrna_feature(fh, seq_id, feat):\n    \"\"\"Write mRNA + implied exons based on join() structure.\"\"\"\n\n    start = int(feat['start'])\n    stop  = int(feat['stop'])\n    strand = feat['strand']\n\n    # fall back if there is no locus tag\n    locus = feat.get('locus') or f\"{seq_id}_{start}_{stop}_{strand}\"\n\n    mrna_id = f\"{locus}-T1\"\n\n    # Top-level mRNA line\n    attrs = {\n        \"ID\": mrna_id,\n        \"Parent\": f\"{locus}\",\n    }\n\n    product = feat.get(\"product\", [])\n\n    if product:\n\n        key = \"product\"         \n        if isinstance(product, list):\n            if len(product) == 1:\n                attrs[key] = str(product[0])\n            else:\n                attrs[key] = \",\".join(str(v) for v in product)\n        else:\n            attrs[key] = str(product)\n\n\n    # Ensure db_xrefs exists and is a list\n    db_xrefs = feat.get(\"db_xrefs\", [])\n\n    # Access note safely\n    note = feat.get(\"note\", None)\n\n\n    if db_xrefs:\n\n        key = \"Dbxref\"         \n        if isinstance(db_xrefs, list):\n            if len(db_xrefs) == 1:\n                attrs[key] = str(db_xrefs[0])\n            else:\n                attrs[key] = \",\".join(str(v) for v in db_xrefs)\n        else:\n            # if somehow not a list, just convert to string\n            attrs[key] = str(db_xrefs)\n\n    if note:\n\n        key = \"note\"         # &lt;-- you must define this\n        if isinstance(db_xrefs, list):\n            if len(db_xrefs) == 1:\n                attrs[key] = str(db_xrefs[0])\n            else:\n                attrs[key] = \",\".join(str(v) for v in db_xrefs)\n        else:\n            # if somehow not a list, just convert to string\n            attrs[key] = str(db_xrefs)\n\n\n    attr_str = \";\".join(f\"{k}={v}\" for k, v in attrs.items())\n\n    fh.write(f\"{seq_id}\\tbaktfold\\tmRNA\\t{start}\\t{stop}\\t.\\t{strand}\\t.\\t{attr_str}\\n\")\n\n    starts = feat.get(\"starts\")\n    stops  = feat.get(\"stops\")\n    strand = feat.get(\"strand\")\n    seq_id = feat.get(\"sequence\")\n\n    if (\n        isinstance(starts, list)\n        and isinstance(stops, list)\n        and len(starts) == len(stops)\n        and len(starts) &gt; 0\n    ):\n        # For minus strand, exons must be written in reverse order (5'\u21923')\n        if strand == \"-\":\n            exon_parts = list(zip(starts, stops))\n        else:\n            exon_parts = list(zip(starts, stops))\n\n        # Exons must be numbered in biological order (5' to 3')\n        if strand == \"-\":\n            exon_parts = exon_parts[::-1]   # reverse order\n\n        # Write each exon to GFF\n        for idx, (ex_start, ex_stop) in enumerate(exon_parts, start=1):\n            exon_id = f\"{mrna_id}.exon{idx}\"\n            exon_attrs = f\"ID={exon_id};Parent={mrna_id}\"\n            fh.write(\n                f\"{seq_id}\\tbaktfold\\texon\\t{ex_start}\\t{ex_stop}\\t.\\t{strand}\\t.\\t{exon_attrs}\\n\"\n            )\n    else:\n        # Single exon (no starts/stops provided)\n        exon_start = feat[\"start\"]\n        exon_stop = feat[\"stop\"]\n        exon_id = f\"{mrna_id}.exon1\"\n        exon_attrs = f\"ID={exon_id};Parent={mrna_id}\"\n\n        fh.write(\n            f\"{seq_id}\\tbaktfold\\texon\\t{exon_start}\\t{exon_stop}\"\n            f\"\\t.\\t{feat['strand']}\\t.\\t{exon_attrs}\\n\"\n        )\n</code></pre>"},{"location":"reference/#src.baktfold.io.gff.write_signal_peptide","title":"<code>write_signal_peptide(fh, feat)</code>","text":"<p>Writes a signal peptide feature to a file.</p> <p>Parameters:</p> Name Type Description Default <code>fh</code> <code>file</code> <p>The file handle to write to.</p> required <code>feat</code> <code>dict</code> <p>A dictionary containing the feature information.</p> required <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; write_signal_peptide(fh, {\n    'locus': 'EHICP_3230',\n    'sequence': 'DS571531.1',\n    'strand': '+',\n    'signal_peptide': {\n        'start': 1,\n        'stop': 20,\n        'score': 0.5\n    }\n})\n</code></pre> Source code in <code>src/baktfold/io/gff.py</code> <pre><code>def write_signal_peptide(fh, feat: dict):  # &lt;1.10.0 compatibility\n    \"\"\"\n    Writes a signal peptide feature to a file.\n\n    Args:\n      fh (file): The file handle to write to.\n      feat (dict): A dictionary containing the feature information.\n\n    Returns:\n      None\n\n    Examples:\n      &gt;&gt;&gt; write_signal_peptide(fh, {\n          'locus': 'EHICP_3230',\n          'sequence': 'DS571531.1',\n          'strand': '+',\n          'signal_peptide': {\n              'start': 1,\n              'stop': 20,\n              'score': 0.5\n          }\n      })\n    \"\"\"\n    sig_peptide = feat[bc.FEATURE_SIGNAL_PEPTIDE]\n    annotations = {\n        'ID': f\"{feat['locus']}_sigpep\",\n        'Name': 'signal peptide',\n        'product': 'signal peptide',\n        'score': sig_peptide['score'],\n        'Parent': feat['locus']\n    }\n    annotations = encode_annotations(annotations)\n    seq_id = feat['sequence'] if 'sequence' in feat else feat['contig']  # &lt;1.10.0 compatibility\n    fh.write(f\"{seq_id}\\tDeepSig\\t{so.SO_SIGNAL_PEPTIDE.name}\\t{sig_peptide['start']}\\t{sig_peptide['stop']}\\t{sig_peptide['score']:.2f}\\t{feat['strand']}\\t.\\t{annotations}\\n\")\n</code></pre>"},{"location":"reference/#src.baktfold.io.fasta.export_sequences","title":"<code>export_sequences(sequences, fasta_path, description=False, wrap=False)</code>","text":"<p>Write sequences to Fasta file.</p> Source code in <code>src/baktfold/io/fasta.py</code> <pre><code>def export_sequences(sequences: Sequence[dict], fasta_path: Path, description: bool=False, wrap: bool=False):\n    \"\"\"Write sequences to Fasta file.\"\"\"\n    logger.info(f'write genome sequences: path={fasta_path}, description={description}, wrap={wrap}')\n\n    with fasta_path.open('wt') as fh:\n        for seq in sequences:\n            if(description):\n                fh.write(f\"&gt;{seq['id']} {seq['description']}\\n\")\n            else:\n                fh.write(f\"&gt;{seq['id']}\\n\")\n            if(wrap):\n                fh.write(wrap_sequence(seq['nt'] if 'nt' in seq else seq['sequence']))  # &lt;1.10.0 compatibility\n            else:\n                fh.write(seq['nt'])\n                fh.write('\\n')\n</code></pre>"},{"location":"reference/#src.baktfold.io.fasta.import_sequences","title":"<code>import_sequences(sequences_path, is_genomic=True, is_dna=True)</code>","text":"<p>Import raw sequences from Fasta file.</p> Source code in <code>src/baktfold/io/fasta.py</code> <pre><code>def import_sequences(sequences_path: Path, is_genomic: bool=True, is_dna: bool=True) -&gt; Sequence[dict]:\n    \"\"\"Import raw sequences from Fasta file.\"\"\"\n    sequences = []\n    with xopen(str(sequences_path), threads=0) as fh:\n        for record in SeqIO.parse(fh, 'fasta'):\n            sequence = {\n                'id': record.id,\n                'description': record.description.split(' ', maxsplit=1)[1] if ' ' in record.description else ''\n            }\n\n            raw_sequence = str(record.seq).upper()\n            if('-' in raw_sequence):\n                dash_count = raw_sequence.count('-')\n                raw_sequence = raw_sequence.replace('-', '')\n                logger.info('import: Discarded alignment gaps (dashes): id=%s, occurences=%i', record.id, dash_count)\n            if(is_dna):\n                if(FASTA_DNA_SEQUENCE_PATTERN.fullmatch(raw_sequence) is None):\n                    logger.error('import: Fasta sequence contains invalid DNA characters! id=%s', record.id)\n                    raise ValueError(f'Fasta sequence contains invalid DNA characters! id={record.id}')\n                sequence['nt'] = raw_sequence\n            else:\n                if(raw_sequence[-1] == '*'):  # remove trailing stop asterik\n                    raw_sequence = raw_sequence[:-1]\n                    logger.warning('import: Removed trailing asterik! id=%s, seq=%s', record.id, raw_sequence)\n                if(FASTA_AA_SEQUENCE_PATTERN.fullmatch(raw_sequence) is None):\n                    logger.error('import: Fasta sequence contains invalid AA characters! id=%s, seq=%s', record.id, raw_sequence)\n                    raise ValueError(f'Fasta sequence contains invalid AA characters! id={record.id}')\n                sequence['aa'] = raw_sequence\n            sequence['length'] = len(raw_sequence)\n            if(is_genomic):\n                sequence['complete'] = False\n                sequence['type'] = bc.REPLICON_CONTIG\n                sequence['topology'] = bc.TOPOLOGY_LINEAR\n            logger.info(\n                f\"imported: id={sequence['id']}, length={sequence['length']}, description={sequence['description']}, genomic={is_genomic}, dna={is_dna}\"\n            )   \n            sequences.append(sequence)\n    return sequences\n</code></pre>"},{"location":"reference/#src.baktfold.io.fasta.wrap_sequence","title":"<code>wrap_sequence(sequence)</code>","text":"<p>Wraps a sequence into lines of 60 characters.</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <code>str</code> <p>The sequence to wrap.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The wrapped sequence.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; wrap_sequence('ARNDCQEGHILKMFPOSUTWYVBZXJ')\n'ARNDCQEGHILKMFPOSUTWYVBZXJ\\n'\n</code></pre> Notes <p>This function is used to format sequences in FASTA files.</p> Source code in <code>src/baktfold/io/fasta.py</code> <pre><code>def wrap_sequence(sequence: str):\n    \"\"\"\n    Wraps a sequence into lines of 60 characters.\n\n    Args:\n      sequence (str): The sequence to wrap.\n\n    Returns:\n      str: The wrapped sequence.\n\n    Examples:\n      &gt;&gt;&gt; wrap_sequence('ARNDCQEGHILKMFPOSUTWYVBZXJ')\n      'ARNDCQEGHILKMFPOSUTWYVBZXJ\\\\n'\n\n    Notes:\n      This function is used to format sequences in FASTA files.\n    \"\"\"\n    lines = []\n    for i in range(0, len(sequence), FASTA_LINE_WRAPPING):\n        lines.append(sequence[i:i + FASTA_LINE_WRAPPING])\n    return '\\n'.join(lines) + '\\n'\n</code></pre>"},{"location":"reference/#src.baktfold.io.fasta.write_faa","title":"<code>write_faa(features, faa_path)</code>","text":"<p>Write translated CDS sequences to Fasta file.</p> Source code in <code>src/baktfold/io/fasta.py</code> <pre><code>def write_faa(features: Sequence[dict], faa_path: Path):\n    \"\"\"Write translated CDS sequences to Fasta file.\"\"\"\n    logger.info(f'write translated CDS/sORF: path={faa_path}')\n    with faa_path.open('wt') as fh:\n        for feat in features:\n            if(feat['type'] == bc.FEATURE_CDS or feat['type'] == bc.FEATURE_SORF):\n                fh.write(f\"&gt;{feat['locus']} {feat['product']}\\n{feat['aa']}\\n\")\n</code></pre>"},{"location":"reference/#src.baktfold.io.fasta.write_ffn","title":"<code>write_ffn(features, ffn_path)</code>","text":"<p>Write annotated nucleotide sequences to Fasta file.</p> Source code in <code>src/baktfold/io/fasta.py</code> <pre><code>def write_ffn(features: Sequence[dict], ffn_path: Path):\n    \"\"\"Write annotated nucleotide sequences to Fasta file.\"\"\"\n    logger.info(f'write feature nucleotide sequences: path={ffn_path}')\n    with ffn_path.open('wt') as fh:\n        for feat in features:\n            if(feat['type'] in [bc.FEATURE_T_RNA, bc.FEATURE_TM_RNA, bc.FEATURE_R_RNA, bc.FEATURE_NC_RNA, bc.FEATURE_NC_RNA_REGION, bc.FEATURE_CRISPR, bc.FEATURE_CDS, bc.FEATURE_SORF, bc.FEATURE_ORIC, bc.FEATURE_ORIV, bc.FEATURE_ORIT]):\n                identifier = feat['locus'] if 'locus' in feat else feat['id']\n                if(feat.get('product', '') != ''):\n                    fh.write(f\"&gt;{identifier} {feat['product']}\\n{feat['nt']}\\n\")\n                else:\n                    fh.write(f\"&gt;{identifier}\\n{feat['nt']}\\n\")\n</code></pre>"},{"location":"reference/#src.baktfold.features.create_foldseek_db.create_foldseek_prostt5_gpu_db","title":"<code>create_foldseek_prostt5_gpu_db(fasta_aa, foldseek_db_path, db_dir, logdir)</code>","text":"<p>Convert a Foldseek DB with ProstT5 3Di predictions using Foldseek-GPU</p> <p>Parameters:</p> Name Type Description Default <code>fasta_aa</code> <code>Path</code> <p>Path to the amino-acid FASTA file.</p> required <code>foldseek_db_path</code> <code>Path</code> <p>Path to the directory where Foldseek database will be stored.</p> required <code>db_dir</code> <code>Path</code> <p>Path to the baktfold DB</p> required <code>logdir</code> <code>Path</code> <p>Path to the directory where logs will be stored.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/features/create_foldseek_db.py</code> <pre><code>def create_foldseek_prostt5_gpu_db(\n    fasta_aa: Path, foldseek_db_path: Path, db_dir: Path, logdir: Path\n) -&gt; None:\n    \"\"\"\n    Convert a Foldseek DB with ProstT5 3Di predictions using Foldseek-GPU\n\n    Args:\n        fasta_aa (Path): Path to the amino-acid FASTA file.\n        foldseek_db_path (Path): Path to the directory where Foldseek database will be stored.\n        db_dir (Path): Path to the baktfold DB\n        logdir (Path): Path to the directory where logs will be stored.\n    Returns:\n        None\n    \"\"\"\n\n    prostt5_db_path = Path(db_dir) / \"prostt5_weights\"\n\n    foldseek_createdb_prostt5 = ExternalTool(\n        tool=\"foldseek\",\n        input=f\"\",\n        output=f\"\",\n        params=f\"createdb {fasta_aa} {foldseek_db_path}  --prostt5-model {prostt5_db_path}  \",\n        logdir=logdir,\n    )\n\n    ExternalTool.run_tool(foldseek_createdb_prostt5)\n</code></pre>"},{"location":"reference/#src.baktfold.features.create_foldseek_db.foldseek_tsv2db","title":"<code>foldseek_tsv2db(in_tsv, out_db_name, db_type, logdir)</code>","text":"<p>Convert a Foldseek TSV file to a Foldseek database.</p> <p>Parameters:</p> Name Type Description Default <code>in_tsv</code> <code>Path</code> <p>Path to the input TSV file.</p> required <code>out_db_name</code> <code>Path</code> <p>Path for the output Foldseek database.</p> required <code>db_type</code> <code>int</code> <p>Type of the output database.</p> required <code>logdir</code> <code>Path</code> <p>Path to the directory where logs will be stored.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/features/create_foldseek_db.py</code> <pre><code>def foldseek_tsv2db(\n    in_tsv: Path, out_db_name: Path, db_type: int, logdir: Path\n) -&gt; None:\n    \"\"\"\n    Convert a Foldseek TSV file to a Foldseek database.\n\n    Args:\n        in_tsv (Path): Path to the input TSV file.\n        out_db_name (Path): Path for the output Foldseek database.\n        db_type (int): Type of the output database.\n        logdir (Path): Path to the directory where logs will be stored.\n\n    Returns:\n        None\n    \"\"\"\n    foldseek_tsv2db = ExternalTool(\n        tool=\"foldseek\",\n        input=f\"\",\n        output=f\"\",\n        params=f\"tsv2db {in_tsv} {out_db_name}  --output-dbtype {str(db_type)} \",\n        logdir=logdir,\n    )\n\n    ExternalTool.run_tool(foldseek_tsv2db)\n</code></pre>"},{"location":"reference/#src.baktfold.features.create_foldseek_db.generate_foldseek_db_from_aa_3di","title":"<code>generate_foldseek_db_from_aa_3di(fasta_aa, fasta_3di, foldseek_db_path, logdir, prefix)</code>","text":"<p>Generate Foldseek database from amino-acid and 3Di sequences.</p> <p>Parameters:</p> Name Type Description Default <code>fasta_aa</code> <code>Path</code> <p>Path to the amino-acid FASTA file.</p> required <code>fasta_3di</code> <code>Path</code> <p>Path to the 3Di FASTA file.</p> required <code>foldseek_db_path</code> <code>Path</code> <p>Path to the directory where Foldseek database will be stored.</p> required <code>logdir</code> <code>Path</code> <p>Path to the directory where logs will be stored.</p> required <code>prefix</code> <code>str</code> <p>Prefix for the Foldseek database.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/features/create_foldseek_db.py</code> <pre><code>def generate_foldseek_db_from_aa_3di(\n    fasta_aa: Path, fasta_3di: Path, foldseek_db_path: Path, logdir: Path, prefix: str\n) -&gt; None:\n    \"\"\"\n    Generate Foldseek database from amino-acid and 3Di sequences.\n\n    Args:\n        fasta_aa (Path): Path to the amino-acid FASTA file.\n        fasta_3di (Path): Path to the 3Di FASTA file.\n        foldseek_db_path (Path): Path to the directory where Foldseek database will be stored.\n        logdir (Path): Path to the directory where logs will be stored.\n        prefix (str): Prefix for the Foldseek database.\n\n    Returns:\n        None\n    \"\"\"\n    # read in amino-acid sequences\n    sequences_aa = {}\n    for record in SeqIO.parse(fasta_aa, \"fasta\"):\n        sequences_aa[record.id] = str(record.seq)\n\n    # read in 3Di strings\n    sequences_3di = {}\n    for record in SeqIO.parse(fasta_3di, \"fasta\"):\n        if not record.id in sequences_aa.keys():\n            logger.warning(\n                \"Warning: ignoring 3Di entry {}, since it is not in the amino-acid FASTA file\".format(\n                    record.id\n                )\n            )\n        else:\n            sequences_3di[record.id] = str(record.seq)  #no upper if masked\n\n    # assert that we parsed 3Di strings for all sequences in the amino-acid FASTA file\n    for id in sequences_aa.keys():\n        if not id in sequences_3di.keys():\n            logger.warning(\n                \"Warning: entry {} in amino-acid FASTA file has no corresponding 3Di string\".format(\n                    id\n                )\n            )\n            logger.warning(\"Removing: entry {} from the Foldseek database \".format(id))\n            sequences_aa = {\n                id: sequence\n                for id, sequence in sequences_aa.items()\n                if id in sequences_3di\n            }\n\n    # generate TSV file contents\n    tsv_aa = \"\"\n    tsv_3di = \"\"\n    tsv_header = \"\"\n    for i, id in enumerate(sequences_aa.keys()):\n        tsv_aa += \"{}\\t{}\\n\".format(str(i + 1), sequences_aa[id])\n        tsv_3di += \"{}\\t{}\\n\".format(str(i + 1), sequences_3di[id])\n        tsv_header += \"{}\\t{}\\n\".format(str(i + 1), id)\n\n    #### write temp tsv files\n\n    # write TSV files\n    temp_aa_tsv: Path = Path(foldseek_db_path) / \"aa.tsv\"\n    temp_3di_tsv: Path = Path(foldseek_db_path) / \"3di.tsv\"\n    temp_header_tsv: Path = Path(foldseek_db_path) / \"header.tsv\"\n    with open(temp_aa_tsv, \"w\") as f:\n        f.write(tsv_aa)\n    with open(temp_3di_tsv, \"w\") as f:\n        f.write(tsv_3di)\n    with open(temp_header_tsv, \"w\") as f:\n        f.write(tsv_header)\n\n    # create foldseek db names\n\n    short_db_name = f\"{prefix}\"\n    aa_db_name: Path = Path(foldseek_db_path) / short_db_name\n    tsv_db_name: Path = Path(foldseek_db_path) / f\"{short_db_name}_ss\"\n    header_db_name: Path = Path(foldseek_db_path) / f\"{short_db_name}_h\"\n\n    # create Foldseek database with foldseek tsv2db\n\n    foldseek_tsv2db(temp_aa_tsv, aa_db_name, 0, logdir)\n    foldseek_tsv2db(temp_3di_tsv, tsv_db_name, 0, logdir)\n    foldseek_tsv2db(temp_header_tsv, header_db_name, 12, logdir)\n\n    # clean up\n    remove_file(temp_aa_tsv)\n    remove_file(temp_3di_tsv)\n    remove_file(temp_header_tsv)\n</code></pre>"},{"location":"reference/#src.baktfold.features.create_foldseek_db.generate_foldseek_db_from_structures","title":"<code>generate_foldseek_db_from_structures(fasta_aa, foldseek_db_path, structure_dir, logdir, prefix, proteins_flag)</code>","text":"<p>Generate Foldseek database from PDB files.</p> <p>Parameters:</p> Name Type Description Default <code>fasta_aa</code> <code>Path</code> <p>Path to the amino-acid FASTA file.</p> required <code>foldseek_db_path</code> <code>Path</code> <p>Path to the directory where Foldseek database will be stored.</p> required <code>structure_dir</code> <code>Path</code> <p>Path to the directory containing .pdb or .cif structure files.</p> required <code>logdir</code> <code>Path</code> <p>Path to the directory where logs will be stored.</p> required <code>prefix</code> <code>str</code> <p>Prefix for the Foldseek database.</p> required <code>proteins_flag</code> <code>bool</code> <p>Flag - True if proteins-compare is run</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/features/create_foldseek_db.py</code> <pre><code>def generate_foldseek_db_from_structures(\n    fasta_aa: Path,\n    foldseek_db_path: Path,\n    structure_dir: Path,\n    logdir: Path,\n    prefix: str,\n    proteins_flag: bool,\n) -&gt; None:\n    \"\"\"\n    Generate Foldseek database from PDB files.\n\n    Args:\n        fasta_aa (Path): Path to the amino-acid FASTA file.\n        foldseek_db_path (Path): Path to the directory where Foldseek database will be stored.\n        structure_dir (Path): Path to the directory containing .pdb or .cif structure files.\n        logdir (Path): Path to the directory where logs will be stored.\n        prefix (str): Prefix for the Foldseek database.\n        proteins_flag (bool): Flag - True if proteins-compare is run\n\n    Returns:\n        None\n    \"\"\"\n\n    # read in amino-acid sequences\n    sequences_aa = {}\n    for record in SeqIO.parse(fasta_aa, \"fasta\"):\n        sequences_aa[record.id] = str(record.seq)\n\n    # lists all the pdb files\n\n    structure_files = [\n        file\n        for file in os.listdir(structure_dir)\n        if file.endswith(\".pdb\") or file.endswith(\".cif\")\n    ]\n\n    num_structures = len(structure_files)\n\n    num_structures = 0\n\n    # Checks that ID is in the pdbs\n\n    no_structure_cds_ids = []\n\n    for cds_id in sequences_aa.keys():\n\n        matching_files = [\n            file\n            for file in structure_files\n            if f\"{cds_id}.pdb\" == file or f\"{cds_id}.cif\" == file\n        ]\n\n        if len(matching_files) == 1:\n            num_structures += 1\n\n        # should neve happen but in case\n        if len(matching_files) &gt; 1:\n            logger.warning(f\"More than 1 structures found for {cds_id}\")\n            logger.warning(\"Taking the first one\")\n            num_structures += 1\n        elif len(matching_files) == 0:\n            logger.warning(f\"No structure found for {cds_id}\")\n            logger.warning(f\"{cds_id} will be ignored in annotation\")\n            no_structure_cds_ids.append(cds_id)\n\n    if num_structures == 0:\n        logger.error(\n            f\"No structures with matching CDS ids were found at all. Check the {structure_dir} directory\"\n        )\n\n    # generate the db\n    short_db_name = f\"{prefix}\"\n    structure_db_name: Path = Path(foldseek_db_path) / short_db_name\n    query_structure_dir = structure_dir\n\n\n    foldseek_createdb_from_structures = ExternalTool(\n        tool=\"foldseek\",\n        input=f\"\",\n        output=f\"\",\n        params=f\"createdb {query_structure_dir} {structure_db_name} \",\n        logdir=logdir,\n    )\n\n    ExternalTool.run_tool(foldseek_createdb_from_structures)\n</code></pre>"},{"location":"reference/#src.baktfold.features.run_foldseek.create_result_tsv","title":"<code>create_result_tsv(query_db, target_db, result_db, result_tsv, logdir, foldseek_gpu, structures, threads)</code>","text":"<p>Create a TSV file containing the results of a Foldseek search.</p> <p>Parameters:</p> Name Type Description Default <code>query_db</code> <code>Path</code> <p>Path to the query database.</p> required <code>target_db</code> <code>Path</code> <p>Path to the target database.</p> required <code>result_db</code> <code>Path</code> <p>Path to the result database generated by the search.</p> required <code>result_tsv</code> <code>Path</code> <p>Path to save the resulting TSV file.</p> required <code>logdir</code> <code>Path</code> <p>Path to the directory where logs will be stored.</p> required <code>foldseek_gpu</code> <code>bool</code> <p>Run Foldseek-GPU with accelerate ungapped prefilter</p> required <code>structures</code> <code>bool</code> <p>Whether structures were input (not ProstT5)</p> required <code>threads</code> <code>int</code> <p>Number of threads to use.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/features/run_foldseek.py</code> <pre><code>def create_result_tsv(\n    query_db: Path, target_db: Path, result_db: Path, result_tsv: Path, logdir: Path, foldseek_gpu: bool, structures: bool, threads: int\n) -&gt; None:\n    \"\"\"\n    Create a TSV file containing the results of a Foldseek search.\n\n    Args:\n        query_db (Path): Path to the query database.\n        target_db (Path): Path to the target database.\n        result_db (Path): Path to the result database generated by the search.\n        result_tsv (Path): Path to save the resulting TSV file.\n        logdir (Path): Path to the directory where logs will be stored.\n        foldseek_gpu (bool): Run Foldseek-GPU with accelerate ungapped prefilter\n        structures (bool): Whether structures were input (not ProstT5)\n        threads (int): Number of threads to use.\n\n    Returns:\n        None\n    \"\"\"\n    if structures:\n        format_string= \"--format-output query,target,bits,fident,evalue,qstart,qend,qlen,tstart,tend,tlen,alntmscore,lddt\"\n    else:\n        format_string = \"--format-output query,target,bits,fident,evalue,qstart,qend,qlen,tstart,tend,tlen\"\n    if foldseek_gpu:\n        target_db = f\"{target_db}_gpu\"\n\n\n    cmd = f\"convertalis {query_db} {target_db} {result_db} {result_tsv} {format_string} --threads {threads}\"\n\n    foldseek_createtsv = ExternalTool(\n        tool=\"foldseek\",\n        input=f\"\",\n        output=f\"\",\n        params=f\"{cmd}\",\n        logdir=logdir,\n    )\n\n\n    ExternalTool.run_tool(foldseek_createtsv)\n</code></pre>"},{"location":"reference/#src.baktfold.features.run_foldseek.run_foldseek_search","title":"<code>run_foldseek_search(query_db, target_db, result_db, temp_db, threads, logdir, evalue, sensitivity, max_seqs, ultra_sensitive, extra_foldseek_params, foldseek_gpu, structures)</code>","text":"<p>Run a Foldseek search using given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>query_db</code> <code>Path</code> <p>Path to the query database.</p> required <code>target_db</code> <code>Path</code> <p>Path to the target database.</p> required <code>result_db</code> <code>Path</code> <p>Path to store the result database.</p> required <code>temp_db</code> <code>Path</code> <p>Path to store temporary files.</p> required <code>threads</code> <code>int</code> <p>Number of threads to use for the search.</p> required <code>logdir</code> <code>Path</code> <p>Path to the directory where logs will be stored.</p> required <code>evalue</code> <code>float</code> <p>E-value threshold for the search.</p> required <code>sensitivity</code> <code>float</code> <p>Sensitivity threshold for the search.</p> required <code>max_seqs</code> <code>int</code> <p>Maximum results per query sequence allowed to pass the prefilter for foldseek.</p> required <code>ultra_sensitive</code> <code>bool</code> <p>Whether to skip foldseek prefilter for maximum sensitivity</p> required <code>extra_foldseek_params</code> <code>str</code> <p>Extra foldseek search params</p> required <code>foldseek_gpu</code> <code>bool</code> <p>Run Foldseek-GPU with accelerate ungapped prefilter</p> required <code>structures</code> <code>bool</code> <p>Run Foldseek with structures, not ProstT5 3Dis</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/features/run_foldseek.py</code> <pre><code>def run_foldseek_search(\n    query_db: Path,\n    target_db: Path,\n    result_db: Path,\n    temp_db: Path,\n    threads: int,\n    logdir: Path,\n    evalue: float,\n    sensitivity: float,\n    max_seqs: int,\n    ultra_sensitive: bool,\n    extra_foldseek_params: str,\n    foldseek_gpu: bool,\n    structures: bool,\n) -&gt; None:\n    \"\"\"\n    Run a Foldseek search using given parameters.\n\n    Args:\n        query_db (Path): Path to the query database.\n        target_db (Path): Path to the target database.\n        result_db (Path): Path to store the result database.\n        temp_db (Path): Path to store temporary files.\n        threads (int): Number of threads to use for the search.\n        logdir (Path): Path to the directory where logs will be stored.\n        evalue (float): E-value threshold for the search.\n        sensitivity (float): Sensitivity threshold for the search.\n        max_seqs (int): Maximum results per query sequence allowed to pass the prefilter for foldseek.\n        ultra_sensitive (bool): Whether to skip foldseek prefilter for maximum sensitivity\n        extra_foldseek_params (str): Extra foldseek search params\n        foldseek_gpu (bool): Run Foldseek-GPU with accelerate ungapped prefilter\n        structures (bool): Run Foldseek with structures, not ProstT5 3Dis\n\n    Returns:\n        None\n    \"\"\"\n\n\n\n    if ultra_sensitive:\n        cmd = f\"search {query_db} {target_db} {result_db} {temp_db} --threads {str(threads)} -e {evalue} -s {sensitivity} --exhaustive-search\"\n    else:\n        cmd = f\"search {query_db} {target_db} {result_db} {temp_db} --threads {str(threads)} -e {evalue} -s {sensitivity} --max-seqs {max_seqs}\"\n\n    # support foldseek gpu only for the regular DB search for now\n    if foldseek_gpu:\n        cmd = f\"search {query_db} {target_db}_gpu {result_db} {temp_db} --threads {str(threads)} -e {evalue}  --gpu 1 --prefilter-mode 1 --max-seqs {max_seqs}\"\n\n    if extra_foldseek_params:\n        cmd += f\" {extra_foldseek_params}\"\n\n    # need -a 1 to compute the alignment so tmscore and lddt can be output (if using --structures)\n    if structures:\n        cmd += f\" -a 1\"\n\n    foldseek_search = ExternalTool(\n        tool=\"foldseek\",\n        input=f\"\",\n        output=f\"\",\n        params=f\"{cmd}\",\n        logdir=logdir,\n    )\n\n    ExternalTool.run_tool(foldseek_search)\n</code></pre>"},{"location":"reference/#src.baktfold.features.run_foldseek.summarise_hits","title":"<code>summarise_hits(result_db, result_db_greedy_best_hits, logdir, threads)</code>","text":"<p>Get all non-overlapping tophits covering a query (designed for CATH)</p> <p>Parameters:</p> Name Type Description Default <code>result_db</code> <code>Path</code> <p>Path to the result database generated by the search.</p> required <code>result_db_greedy_best_hits</code> <code>Path</code> <p>Path to save the greedy best hits results db.</p> required <code>logdir</code> <code>Path</code> <p>Path to the directory where logs will be stored.</p> required <code>threads</code> <code>int</code> <p>Number of threads to use.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/features/run_foldseek.py</code> <pre><code>def summarise_hits(result_db: Path, result_db_greedy_best_hits: Path, logdir: Path, threads: int) -&gt; None:\n    \"\"\"\n    Get all non-overlapping tophits covering a query (designed for CATH)\n\n    Args:\n        result_db (Path): Path to the result database generated by the search.\n        result_db_greedy_best_hits (Path): Path to save the greedy best hits results db.\n        logdir (Path): Path to the directory where logs will be stored.\n        threads (int): Number of threads to use.\n\n    Returns:\n        None\n    \"\"\"\n\n    cmd = f\"summarizeresult  {result_db} {result_db_greedy_best_hits} --threads {threads} -a 1\"\n\n    foldseek_summarizeresult = ExternalTool(\n        tool=\"foldseek\",\n        input=f\"\",\n        output=f\"\",\n        params=f\"{cmd}\",\n        logdir=logdir,\n    )\n\n    ExternalTool.run_tool(foldseek_summarizeresult)\n</code></pre>"},{"location":"reference/#src.baktfold.features.predict_3Di.CNN","title":"<code>CNN</code>","text":"<p>             Bases: <code>nn.Module</code></p> <p>Convolutional neural network (two convolutional layers).</p> <p>Parameters:</p> Name Type Description Default <code>nn.Module</code> <p>The base class for all neural network modules.</p> required <p>Returns:</p> Type Description <p>None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; CNN()\nNone\n</code></pre> Source code in <code>src/baktfold/features/predict_3Di.py</code> <pre><code>class CNN(nn.Module):\n    \"\"\"\n    Convolutional neural network (two convolutional layers).\n\n    Args:\n      nn.Module: The base class for all neural network modules.\n\n    Returns:\n      None.\n\n    Examples:\n      &gt;&gt;&gt; CNN()\n      None\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the Convolutional Neural Network (CNN) model.\n        \"\"\"\n        super(CNN, self).__init__()\n\n        self.classifier = nn.Sequential(\n            nn.Conv2d(1024, 32, kernel_size=(7, 1), padding=(3, 0)),  # 7x32\n            nn.ReLU(),\n            nn.Dropout(0.0),\n            nn.Conv2d(32, 20, kernel_size=(7, 1), padding=(3, 0)),\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Perform forward pass through the CNN.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, embedding_size).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, sequence_length, num_classes).\n\n        L = protein length\n        B = batch-size\n        F = number of features (1024 for embeddings)\n        N = number of classes (20 for 3Di)\n        \"\"\"\n\n        # Permute input tensor to match expected shape\n        # Input shape: (batch_size, sequence_length, embedding_size)\n        # Output shape: (batch_size, embedding_size, sequence_length, 1)\n\n        x = x.permute(0, 2, 1).unsqueeze(\n            dim=-1\n        )  # IN: X = (B x L x F); OUT: (B x F x L, 1)\n\n        # Pass the input through the classifier\n        # Output shape: (batch_size, num_classes, sequence_length, 1)\n        Yhat = self.classifier(x)  # OUT: Yhat_consurf = (B x N x L x 1)\n\n        # Remove the singleton dimension from the output tensor\n        # Output shape: (batch_size, num_classes, sequence_length)\n        Yhat = Yhat.squeeze(dim=-1)  # IN: (B x N x L x 1); OUT: ( B x L x N )\n        return Yhat\n</code></pre>"},{"location":"reference/#src.baktfold.features.predict_3Di.CNN.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the Convolutional Neural Network (CNN) model.</p> Source code in <code>src/baktfold/features/predict_3Di.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize the Convolutional Neural Network (CNN) model.\n    \"\"\"\n    super(CNN, self).__init__()\n\n    self.classifier = nn.Sequential(\n        nn.Conv2d(1024, 32, kernel_size=(7, 1), padding=(3, 0)),  # 7x32\n        nn.ReLU(),\n        nn.Dropout(0.0),\n        nn.Conv2d(32, 20, kernel_size=(7, 1), padding=(3, 0)),\n    )\n</code></pre>"},{"location":"reference/#src.baktfold.features.predict_3Di.CNN.forward","title":"<code>forward(x)</code>","text":"<p>Perform forward pass through the CNN.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>Input tensor of shape (batch_size, sequence_length, embedding_size).</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>torch.Tensor: Output tensor of shape (batch_size, sequence_length, num_classes).</p> <p>L = protein length B = batch-size F = number of features (1024 for embeddings) N = number of classes (20 for 3Di)</p> Source code in <code>src/baktfold/features/predict_3Di.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Perform forward pass through the CNN.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, embedding_size).\n\n    Returns:\n        torch.Tensor: Output tensor of shape (batch_size, sequence_length, num_classes).\n\n    L = protein length\n    B = batch-size\n    F = number of features (1024 for embeddings)\n    N = number of classes (20 for 3Di)\n    \"\"\"\n\n    # Permute input tensor to match expected shape\n    # Input shape: (batch_size, sequence_length, embedding_size)\n    # Output shape: (batch_size, embedding_size, sequence_length, 1)\n\n    x = x.permute(0, 2, 1).unsqueeze(\n        dim=-1\n    )  # IN: X = (B x L x F); OUT: (B x F x L, 1)\n\n    # Pass the input through the classifier\n    # Output shape: (batch_size, num_classes, sequence_length, 1)\n    Yhat = self.classifier(x)  # OUT: Yhat_consurf = (B x N x L x 1)\n\n    # Remove the singleton dimension from the output tensor\n    # Output shape: (batch_size, num_classes, sequence_length)\n    Yhat = Yhat.squeeze(dim=-1)  # IN: (B x N x L x 1); OUT: ( B x L x N )\n    return Yhat\n</code></pre>"},{"location":"reference/#src.baktfold.features.predict_3Di.get_T5_model","title":"<code>get_T5_model(model_dir, model_name, cpu, threads)</code>","text":"<p>Loads a T5 model and tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>Path</code> <p>Directory where the model and tokenizer is be stored.</p> required <code>model_name</code> <code>str</code> <p>Name of the pre-trained T5 model.</p> required <code>cpu</code> <code>bool</code> <p>Whether to use CPU only.</p> required <code>threads</code> <code>int</code> <p>Number of cpu threads.</p> required <p>Returns:</p> Type Description <code>(T5EncoderModel, T5Tokenizer)</code> <p>Tuple[T5EncoderModel, T5Tokenizer]: Tuple containing the loaded T5 model and tokenizer.</p> Source code in <code>src/baktfold/features/predict_3Di.py</code> <pre><code>def get_T5_model(\n    model_dir: Path, model_name: str, cpu: bool, threads: int\n) -&gt; (T5EncoderModel, T5Tokenizer):\n    \"\"\"\n    Loads a T5 model and tokenizer.\n\n    Args:\n        model_dir (Path): Directory where the model and tokenizer is be stored.\n        model_name (str): Name of the pre-trained T5 model.\n        cpu (bool): Whether to use CPU only.\n        threads (int): Number of cpu threads.\n\n    Returns:\n        Tuple[T5EncoderModel, T5Tokenizer]: Tuple containing the loaded T5 model and tokenizer.\n    \"\"\"\n\n    # sets the device\n\n    # Torch load will map back to device from state, which often is GPU:0.\n    # to overcome, need to explicitly map to active device\n\n    global device\n\n    torch.set_num_threads(threads)\n\n    if cpu is True:\n        device = torch.device(\"cpu\")\n        dev_name = \"cpu\"\n    else:\n        # check for NVIDIA/cuda\n        if torch.cuda.is_available():\n            device = torch.device(\"cuda:0\")\n            dev_name = \"cuda:0\"\n        # check for apple silicon/metal\n        elif torch.backends.mps.is_available():\n            device = torch.device(\"mps\")\n            dev_name = \"mps\"\n        else:\n            device = torch.device(\"cpu\")\n            dev_name = \"cpu\"\n            if cpu is not True:\n                logger.warning(\n                    \"No available GPU was found, but --cpu was not specified\"\n                )\n                logger.warning(\"ProstT5 will be run with CPU only\")\n\n    # logger device only if the function is called\n    logger.info(\"Using device: {}\".format(dev_name))\n\n    # make dir if doesnt exist\n    Path(model_dir).mkdir(parents=True, exist_ok=True)\n\n    # load\n    logger.info(f\"Loading T5 from: {model_dir}/{model_name}\")\n    logger.info(f\"If {model_dir}/{model_name} is not found, it will be downloaded\")\n\n    # check ProstT5 is downloaded\n    # flag assumes transformers takes from local file (see #44)\n    localfile = True\n    download = False\n\n    download = check_prostT5_download(model_dir, model_name)\n    if download:\n        localfile = False\n        logger.info(\"ProstT5 not found. Downloading ProstT5 from Hugging Face\")\n    try:\n        model = T5EncoderModel.from_pretrained(\n            model_name,\n            cache_dir=f\"{model_dir}/\",\n            force_download=download,\n            local_files_only=localfile,\n        ).to(device)\n\n    except:\n        logger.warning(\"Download from Hugging Face failed. Trying backup from Zenodo.\")\n        logdir = f\"{model_dir}/logdir\"\n        download_zenodo_prostT5(model_dir, logdir, threads )\n\n        model = T5EncoderModel.from_pretrained(\n            model_name,\n            cache_dir=f\"{model_dir}/\",\n            force_download=False,\n            local_files_only=True,\n        ).to(device)\n\n    model = model.eval()\n    vocab = T5Tokenizer.from_pretrained(\n        model_name, cache_dir=f\"{model_dir}/\", do_lower_case=False,\n        use_fast=False\n    )\n\n    model = T5EncoderModel.from_pretrained(\n                model_name,\n                cache_dir=f\"{model_dir}/\",\n                force_download=False,\n                local_files_only=True,\n            ).to(device)\n\n    logger.info(f\"{model_name} loaded\")\n\n    return model, vocab\n</code></pre>"},{"location":"reference/#src.baktfold.features.predict_3Di.get_embeddings","title":"<code>get_embeddings(hypotheticals, cds_dict, out_path, prefix, model_dir, model_name, checkpoint_path, output_3di, output_h5_per_residue, output_h5_per_protein, half_precision, max_residues=100000, max_seq_len=30000, max_batch=10000, cpu=False, output_probs=True, save_per_residue_embeddings=False, save_per_protein_embeddings=False, threads=1, mask_threshold=0, has_duplicate_locus=False)</code>","text":"<p>Generate embeddings and predictions for protein sequences using ProstT5 encoder &amp; CNN prediction head.</p> <p>Parameters:</p> Name Type Description Default <code>hypotheticals</code> <code> Dict[str, Tuple[str, ...]]</code> <p>dictionary containing CDS IDs feature information</p> required <code>cds_dict</code> <code> Dict[str, Tuple[str, ...]]</code> <p>dictionary containing CDS IDs and corresponding protein sequences.</p> required <code>out_path</code> <code>Path</code> <p>Path to the output directory.</p> required <code>prefix</code> <code>str</code> <p>Prefix for the output files.</p> required <code>model_dir</code> <code>Path</code> <p>Directory containing the pre-trained model.</p> required <code>model_name</code> <code>str</code> <p>Name of the pre-trained model.</p> required <code>output_3di</code> <code>Path</code> <p>Path to the output 3Di file.</p> required <code>output_h5_per_residue</code> <code>Path</code> <p>Path to the output h5 per residue embeddings file.</p> required <code>output_h5_per_protein</code> <code>Path</code> <p>Path to the output h5 per proteins embeddings file.</p> required <code>half_precision</code> <code>bool</code> <p>Whether to use half precision for the models.</p> required <code>max_residues</code> <code>int</code> <p>Maximum number of residues allowed in a batch. Defaults to 3000.</p> <code>100000</code> <code>max_seq_len</code> <code>int</code> <p>Maximum sequence length allowed. Defaults to 1000.</p> <code>30000</code> <code>max_batch</code> <code>int</code> <p>Maximum batch size. Defaults to 100.</p> <code>10000</code> <code>cpu</code> <code>bool</code> <p>Whether to use CPU for processing. Defaults to False.</p> <code>False</code> <code>output_probs</code> <code>bool</code> <p>Whether to output probabilities. Defaults to True.</p> <code>True</code> <code>save_embeddings</code> <code>bool</code> <p>Whether to save embeddings to h5 file. Defaults to False. Will  save per residue embeddings</p> required <code>per_protein_embeddings</code> <code>bool</code> <p>Whether to save per protein mean embeddings to h5 file. Defaults to False.</p> required <code>threads</code> <code>int</code> <p>number of cpu threads</p> <code>1</code> <code>mask_threshold</code> <code>float) </code> <p>0-100 - below this ProstT5 confidence threshold, these residues are masked</p> <code>0</code> <code>has_duplicate_locus</code> <code>bool) </code> <p>some euks have dupe locus tags</p> <code>False</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if embeddings and predictions are generated successfully.</p> Source code in <code>src/baktfold/features/predict_3Di.py</code> <pre><code>def get_embeddings(\n    hypotheticals: Dict[str, Tuple[str, ...]],\n    cds_dict: Dict[str, Tuple[str, ...]],\n    out_path: Path,\n    prefix: str,\n    model_dir: Path,\n    model_name: str,\n    checkpoint_path: Path,\n    output_3di: Path,\n    output_h5_per_residue: Path,\n    output_h5_per_protein: Path,\n    half_precision: bool,\n    max_residues: int = 100000,\n    max_seq_len: int = 30000,\n    max_batch: int = 10000,\n    cpu: bool = False,\n    output_probs: bool = True,\n    save_per_residue_embeddings: bool = False,\n    save_per_protein_embeddings: bool = False,\n    threads: int = 1,\n    mask_threshold: float = 0,\n    has_duplicate_locus: bool = False\n) -&gt; bool:\n    \"\"\"\n    Generate embeddings and predictions for protein sequences using ProstT5 encoder &amp; CNN prediction head.\n\n    Args:\n        hypotheticals ( Dict[str, Tuple[str, ...]]):  dictionary containing CDS IDs feature information\n        cds_dict ( Dict[str, Tuple[str, ...]]):  dictionary containing CDS IDs and corresponding protein sequences.\n        out_path (Path): Path to the output directory.\n        prefix (str): Prefix for the output files.\n        model_dir (Path): Directory containing the pre-trained model.\n        model_name (str): Name of the pre-trained model.\n        output_3di (Path): Path to the output 3Di file.\n        output_h5_per_residue (Path): Path to the output h5 per residue embeddings file.\n        output_h5_per_protein (Path): Path to the output h5 per proteins embeddings file.\n        half_precision (bool): Whether to use half precision for the models.\n        max_residues (int, optional): Maximum number of residues allowed in a batch. Defaults to 3000.\n        max_seq_len (int, optional): Maximum sequence length allowed. Defaults to 1000.\n        max_batch (int, optional): Maximum batch size. Defaults to 100.\n        cpu (bool, optional): Whether to use CPU for processing. Defaults to False.\n        output_probs (bool, optional): Whether to output probabilities. Defaults to True.\n        save_embeddings (bool, optional): Whether to save embeddings to h5 file. Defaults to False. Will  save per residue embeddings\n        per_protein_embeddings (bool, optional): Whether to save per protein mean embeddings to h5 file. Defaults to False.\n        threads (int): number of cpu threads\n        mask_threshold (float) : 0-100 - below this ProstT5 confidence threshold, these residues are masked\n        has_duplicate_locus (bool) : some euks have dupe locus tags\n\n    Returns:\n        bool: True if embeddings and predictions are generated successfully.\n    \"\"\"\n\n    predictions = {}\n    batch_predictions = {}\n\n    if save_per_residue_embeddings:\n        embeddings_per_residue = {}\n        batch_embeddings_per_residue = {}\n    if save_per_protein_embeddings:\n        embeddings_per_protein = {}\n        batch_embeddings_per_protein = {}\n\n\n    prostt5_prefix = \"&lt;AA2fold&gt;\"\n\n\n    model, vocab = get_T5_model(model_dir, model_name, cpu, threads)\n    predictor = load_predictor(checkpoint_path)\n\n    logger.info(\"Beginning ProstT5 predictions\")\n\n    if half_precision:\n        model = model.half()\n        predictor = predictor.half()\n        logger.info(\"Using models in half-precision\")\n    else:\n        logger.info(\"Using models in full-precision\")\n\n    # 3Di predictions\n\n    fail_ids = []\n\n    for k, v in list(cds_dict.items()):\n        if len(v) == 0:\n            logger.info(f\"Skipping empty CDS entry as it has no amino acid string associated (likely pseudo): key={k}\")\n            del cds_dict[k]\n\n    # sort sequences by length\n    seq_dict = []\n    fail_ids = []\n\n    for k, seq in cds_dict.items():\n        if isinstance(seq, str) and seq:\n            clean_seq = (\n                seq.replace(\"U\", \"X\")\n                .replace(\"Z\", \"X\")\n                .replace(\"O\", \"X\")\n            )\n            seq_dict.append((k, clean_seq, len(clean_seq))) # in the correct format for the remainder of the code\n        else:\n            logger.warning(\n                f\"Protein header {k} is corrupt. It will be saved in fails.tsv\"\n            )\n            fail_ids.append(k)\n\n\n    original_keys = list(cds_dict.keys())\n        # --- sort once ---\n    seq_dict.sort(key=lambda x: x[2], reverse=True)\n\n    batch = list()\n    for seq_idx, (pdb_id, seq, slen) in enumerate(tqdm(seq_dict, desc=f\"Predicting 3Di\"), 1):\n\n        # replace non-standard AAs\n        seq = seq.replace(\"U\", \"X\").replace(\"Z\", \"X\").replace(\"O\", \"X\")\n        seq_len = len(seq)\n        seq = prostt5_prefix + \" \" + \" \".join(list(seq))\n        batch.append((pdb_id, seq, seq_len))\n\n        # count residues in current batch and add the last sequence length to\n        # avoid that batches with (n_res_batch &gt; max_residues) get processed\n        n_res_batch = sum([s_len for _, _, s_len in batch]) + seq_len\n        if (\n            len(batch) &gt;= max_batch\n            or n_res_batch &gt;= max_residues\n            or seq_idx == len(seq_dict)\n            or seq_len &gt; max_seq_len\n        ):\n            pdb_ids, seqs, seq_lens = zip(*batch)\n            batch = list()\n\n            token_encoding = vocab(\n                seqs,\n                add_special_tokens=True,\n                padding=\"longest\",\n                truncation=False,\n                return_tensors=\"pt\"\n                ).to(device)\n\n            try:\n                with torch.no_grad():\n                    embedding_repr = model(\n                        token_encoding.input_ids,\n                        attention_mask=token_encoding.attention_mask,\n                    )\n            except RuntimeError:\n                logger.warning(f\" number of residues in batch {n_res_batch}\")\n                logger.warning(f\" seq length is {seq_len}\")\n                logger.warning(f\" ids are {pdb_ids}\")\n                logger.warning(\n                    \"RuntimeError during embedding for {} (L={})\".format(\n                        pdb_id, seq_len\n                    )\n                )\n                for id in pdb_ids:\n                    fail_ids.append(id)\n                continue\n\n\n            # ProtT5 appends a special tokens at the end of each sequence\n            # Mask this also out during inference while taking into account the prostt5 prefix\n            try:\n                for idx, s_len in enumerate(seq_lens):\n                    token_encoding.attention_mask[idx, s_len + 1] = 0\n\n                # extract last hidden states (=embeddings)\n                residue_embedding = embedding_repr.last_hidden_state.detach()\n                # mask out padded elements in the attention output (can be non-zero) for further processing/prediction\n                residue_embedding = (\n                    residue_embedding\n                    * token_encoding.attention_mask.unsqueeze(dim=-1)\n                )\n                # slice off embedding of special token prepended before to each sequence\n                residue_embedding = residue_embedding[:, 1:]\n                prediction = predictor(residue_embedding)\n\n\n                # compute max probabilities per token/residue\n                probabilities = toCPU(\n                    torch.max(\n                        F.softmax(prediction, dim=1), dim=1, keepdim=True\n                    )[0]\n                )\n\n                prediction = toCPU(\n                    torch.max(prediction, dim=1, keepdim=True)[1]\n                ).astype(np.byte)\n\n\n                    # to get logits\n                # t = prediction.transpose(1, 2)  # changes ( B x L x N ) to ( B x N x L ) \n                # logits = t.detach().cpu().numpy()\n                # print(logits[0])\n                # print(logits[0].shape)\n\n                    # prob_tensor = F.softmax(prediction, dim=1).cpu()\n                    # #print(prob_tensor.shape)\n                    # prob_matrix = prob_tensor.squeeze(0).transpose(0, 1)\n                    # #print(prob_matrix.shape)\n                    # all_sampled_states = {}\n                    # i = 1\n                    # while i &lt; 11:\n                    #     all_sampled_states[i] = torch.multinomial(prob_matrix, 1).squeeze(1)\n                    #     i += 1\n\n                    #sampled_states = torch.multinomial(prob_matrix, 1).squeeze(1)\n                    #sampled_states = torch.multinomial(prob_matrix, 100, replacement=True)\n                    #print(sampled_states)\n                    #print(sampled_states.shape)\n\n                # batch-size x seq_len x embedding_dim\n                # extra token is added at the end of the seq\n                for batch_idx, identifier in enumerate(pdb_ids):\n                    s_len = seq_lens[batch_idx]\n\n                    # save embeddings\n                    if save_per_residue_embeddings or save_per_protein_embeddings:\n                        try:\n                            # account for prefix in offset\n                            emb = embedding_repr.last_hidden_state[\n                                batch_idx, 1 : s_len + 1\n                            ]\n\n                            if save_per_residue_embeddings:\n                                batch_embeddings_per_residue[identifier] = (\n                                    emb.detach().cpu().numpy().squeeze()\n                                )\n\n                            if save_per_protein_embeddings:\n                                batch_embeddings_per_protein[identifier] = (\n                                    emb.mean(dim=0).detach().cpu().numpy().squeeze()\n                                )\n\n                        except:\n                            logger.warning(\n                                f\"Saving embeddings failed for {identifier}\"\n                            )\n\n                    # slice off padding and special token appended to the end of the sequence\n                    pred = prediction[batch_idx, :, 0:s_len].squeeze()\n\n                    # always return the mean probs\n                    mean_prob = round(\n                            100 * np.mean(probabilities[batch_idx, :, 0:s_len]), 2\n                        )\n\n                    if output_probs:  # if you want the per-residue probs\n                        all_prob = probabilities[batch_idx, :, 0:s_len]\n                        batch_predictions[identifier] = (\n                            pred,\n                            mean_prob,\n                            all_prob,\n                        )\n                    else:\n                        batch_predictions[identifier] = (pred, mean_prob, None)\n\n                    try:\n                        len(batch_predictions[identifier][0])\n                    except:\n                        logger.warning(\n                            f\"{identifier} prediction has length 0\"\n                        )\n                        fail_ids.append(identifier)\n                        continue\n\n                    if s_len != len(batch_predictions[identifier][0]):\n                        logger.warning(\n                            f\"Length mismatch for {identifier}: is:{len(batch_predictions[identifier][0])} vs should:{s_len}\"\n                        )\n\n                    for k in original_keys:\n                        if k in batch_predictions:\n                            predictions[k] = batch_predictions[k]\n\n                    if save_per_residue_embeddings:\n                        for k in original_keys:\n                            if k in batch_predictions:\n                                embeddings_per_residue[k] = batch_embeddings_per_residue[k]\n\n                    if save_per_protein_embeddings:\n                        for k in original_keys:\n                            if k in batch_predictions:\n                                embeddings_per_protein[k] = batch_embeddings_per_protein[k]\n\n            except IndexError:\n                logger.warning(\n                    \"Index error during prediction for {} (L={})\".format(\n                        pdb_id, seq_len\n                    )\n                )\n                for id in pdb_ids:\n                    fail_ids.append(id)\n                continue\n\n    # write list of fails if length &gt; 0\n    if len(fail_ids) &gt; 0:\n        fail_tsv: Path = Path(out_path) / \"fails.tsv\"\n\n        # Convert the list to a list of lists\n        data_as_list_of_lists = [[str(item)] for item in fail_ids]\n\n        # Write the list to a TSV file\n        with open(fail_tsv, \"w\", newline=\"\") as file:\n            tsv_writer = csv.writer(file, delimiter=\"\\t\")\n            tsv_writer.writerows(data_as_list_of_lists)\n\n    write_predictions(hypotheticals, predictions, output_3di,  mask_threshold, has_duplicate_locus)\n\n    if save_per_residue_embeddings:\n        write_embeddings(embeddings_per_residue, output_h5_per_residue)\n\n    if save_per_protein_embeddings:\n        write_embeddings(embeddings_per_protein, output_h5_per_protein)\n\n    # always write the mean embeddings\n    mean_probs_out_path: Path = (\n        Path(out_path) / f\"{prefix}_prostT5_3di_mean_probabilities.csv\"\n    )\n\n    # output per residue probs\n    if output_probs:\n        all_probs_out_path: Path = (\n            Path(out_path) / f\"{prefix}_prostT5_3di_all_probabilities.json\"\n        )\n    else:\n        all_probs_out_path = None\n\n    write_probs(predictions, mean_probs_out_path, all_probs_out_path, original_keys)\n\n    return predictions\n</code></pre>"},{"location":"reference/#src.baktfold.features.predict_3Di.load_predictor","title":"<code>load_predictor(checkpoint_path)</code>","text":"<p>Load a pre-trained CNN ProstT5 prediction head weights from a checkpoint file.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_path</code> <code>Union[str, Path]</code> <p>Path to the checkpoint file.</p> required <p>Returns:</p> Name Type Description <code>CNN</code> <code>CNN</code> <p>Loaded CNN model.</p> Source code in <code>src/baktfold/features/predict_3Di.py</code> <pre><code>def load_predictor(checkpoint_path: Union[str, Path]) -&gt; CNN:\n    \"\"\"\n    Load a pre-trained CNN ProstT5 prediction head weights from a checkpoint file.\n\n    Args:\n        checkpoint_path (Union[str, Path]): Path to the checkpoint file.\n\n    Returns:\n        CNN: Loaded CNN model.\n    \"\"\"\n\n    model = CNN()\n\n    # checkpoint_path = Path(CNN_DIR) / \"cnn_chkpnt\" / \"model.pt\"\n\n    state = torch.load(checkpoint_path, map_location=device)\n\n    # regular ProstT5 CNN \n    if checkpoint_path.suffix == '.pt':\n        model.load_state_dict(state[\"state_dict\"])\n    # finetuned\n    else:\n        model.load_state_dict(state)\n\n\n    model = model.eval()\n    model = model.to(device)\n\n    return model\n</code></pre>"},{"location":"reference/#src.baktfold.features.predict_3Di.toCPU","title":"<code>toCPU(tensor)</code>","text":"<p>Move a tensor to CPU and convert it to a NumPy array.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>torch.Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: NumPy array.</p> Source code in <code>src/baktfold/features/predict_3Di.py</code> <pre><code>def toCPU(tensor: torch.Tensor) -&gt; np.ndarray:\n    \"\"\"\n    Move a tensor to CPU and convert it to a NumPy array.\n\n    Args:\n        tensor (torch.Tensor): Input tensor.\n\n    Returns:\n        np.ndarray: NumPy array.\n    \"\"\"\n    if len(tensor.shape) &gt; 1:\n        return tensor.detach().cpu().squeeze(dim=-1).numpy()\n    else:\n        return tensor.detach().cpu().numpy()\n</code></pre>"},{"location":"reference/#src.baktfold.features.predict_3Di.write_embeddings","title":"<code>write_embeddings(embeddings, out_path)</code>","text":"<p>Write embeddings to an output file.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>Dict[str, Dict[str, Tuple[List[str], Any, Any]]]</code> <p>Predictions dictionary containing contig IDs, sequence IDs, predictions, and additional information.</p> required <code>out_path</code> <code>Path</code> <p>Path to the output file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/features/predict_3Di.py</code> <pre><code>def write_embeddings(\n    embeddings: Dict[str, Dict[str, Tuple[List[str], Any, Any]]],\n    out_path: Path,\n) -&gt; None:\n    \"\"\"\n    Write embeddings to an output file.\n\n    Args:\n        embeddings (Dict[str, Dict[str, Tuple[List[str], Any, Any]]]): Predictions dictionary containing contig IDs, sequence IDs, predictions, and additional information.\n        out_path (Path): Path to the output file.\n\n    Returns:\n        None\n    \"\"\"\n\n    with h5py.File(str(out_path), \"w\") as hf:\n        for sequence_id, embedding in embeddings.items():\n            hf.create_dataset(sequence_id, data=embedding)\n</code></pre>"},{"location":"reference/#src.baktfold.features.predict_3Di.write_predictions","title":"<code>write_predictions(hypotheticals, predictions, out_path, mask_threshold, has_duplicate_locus)</code>","text":"<p>Write predictions to an output file.</p> <p>Parameters:</p> Name Type Description Default <code>hypotheticals</code> <code>Dict</code> <p>Hypothetical protein feature dictionary from Bakta</p> required <code>predictions</code> <code>Dict[str, Tuple[List[str], Any, Any]]</code> <p>Predictions dictionary containing sequence IDs, predictions, and additional information.</p> required <code>out_path</code> <code>Path</code> <p>Path to the output file.</p> required <code>mask_threshold</code> <code>float</code> <p>between 0 and 100 - below this ProstT5 confidence, 3Di predictions are masked</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/features/predict_3Di.py</code> <pre><code>def write_predictions(\n    hypotheticals: Dict,\n    predictions: Dict[str, Tuple[List[str], Any, Any]],\n    out_path: Path,\n    mask_threshold: float,\n    has_duplicate_locus: bool\n) -&gt; None:\n    \"\"\"\n    Write predictions to an output file.\n\n    Args:\n        hypotheticals Dict: Hypothetical protein feature dictionary from Bakta\n        predictions (Dict[str,  Tuple[List[str], Any, Any]]): Predictions dictionary containing sequence IDs, predictions, and additional information.\n        out_path (Path): Path to the output file.\n        mask_threshold (float): between 0 and 100 - below this ProstT5 confidence, 3Di predictions are masked\n\n\n    Returns:\n        None\n    \"\"\"\n    ss_mapping = {\n        0: \"A\",\n        1: \"C\",\n        2: \"D\",\n        3: \"E\",\n        4: \"F\",\n        5: \"G\",\n        6: \"H\",\n        7: \"I\",\n        8: \"K\",\n        9: \"L\",\n        10: \"M\",\n        11: \"N\",\n        12: \"P\",\n        13: \"Q\",\n        14: \"R\",\n        15: \"S\",\n        16: \"T\",\n        17: \"V\",\n        18: \"W\",\n        19: \"Y\",\n        20: \"X\" # fully mask the low confidence 3Di residues with X not lower case (not working for Foldseek v10, but X does)\n    }\n\n    mask_prop_threshold = mask_threshold/100\n\n    # Filter out entries where the length of the value is 0\n    # Issue #47\n    predictions = {\n        k: v for k, v in predictions.items() if len(v[0]) &gt; 0\n    }\n\n\n    # mask low confidence residues\n    for seq_id, (pred, mean_prob, all_prob) in predictions.items():\n\n        # mask to X\n        for i in range(len(pred)):\n            if all_prob[0][i] &lt; mask_prop_threshold:\n                pred[i] = 20\n\n    with open(out_path, \"w+\") as out_f:\n        for feat in hypotheticals:\n            if has_duplicate_locus:\n                seq_id = feat[\"id\"]\n            else:\n                seq_id = feat[\"locus\"]\n            pred = predictions.get(seq_id)  # predictions = {seq_id: (yhats, _, _)} or None\n\n            if pred is not None:\n                yhats = pred[0]  \n                threedi_seq = \"\".join(ss_mapping[int(yhat)] for yhat in yhats)\n                feat[\"3di\"] = threedi_seq  # update the feature dictionary\n                out_f.write(f\"&gt;{seq_id}\\n{threedi_seq}\\n\")\n            else:\n                feat[\"3di\"] = None  # missing prediction\n\n    logger.info(f\"Finished writing results to {out_path}\")\n    return None\n</code></pre>"},{"location":"reference/#src.baktfold.features.predict_3Di.write_probs","title":"<code>write_probs(predictions, output_path_mean, output_path_all, original_keys)</code>","text":"<p>Write all ProstT5 encoder + CNN probabilities and mean probabilities to output files.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>Dict[str, Tuple[int, float, Union[int, np.ndarray]]]</code> required <code>output_path_mean</code> <code>str</code> <p>Path to the output file for mean probabilities.</p> required <code>output_path_all</code> <code>str</code> <p>Path to the output file for all probabilities.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/features/predict_3Di.py</code> <pre><code>def write_probs(\n    predictions: Dict[str, Tuple[int, float, Union[int, np.ndarray]]],\n    output_path_mean: Path,\n    output_path_all: Path,\n    original_keys: list[str],\n) -&gt; None:\n    \"\"\"\n    Write all ProstT5 encoder + CNN probabilities and mean probabilities to output files.\n\n    Args:\n        predictions (Dict[str, Tuple[int, float, Union[int, np.ndarray]]]):\n        Predictions dictionary containing  sequence IDs, probabilities, and additional information.\n        output_path_mean (str): Path to the output file for mean probabilities.\n        output_path_all (str): Path to the output file for all probabilities.\n\n    Returns:\n        None\n    \"\"\"\n\n    with open(output_path_mean, \"w+\") as out_f:\n        for seq_id in original_keys:\n            if seq_id not in predictions:\n                logger.warning(f\"Missing ProstT5 mean confidence for {seq_id}\")\n                continue\n            _, mean_prob, _ = predictions[seq_id]\n            out_f.write(f\"{seq_id},{mean_prob}\\n\")\n\n    if output_path_all is not None:\n        with open(output_path_all, \"w+\") as out_f:\n            for seq_id in original_keys:\n                if seq_id not in predictions:\n                    logger.warning(f\"Missing ProstT5 confidence for {seq_id}\")\n                    continue\n\n                _, _, all_probs = predictions[seq_id]\n\n                # convert to percentage\n                all_probs = all_probs * 100\n\n                # flatten\n                if isinstance(all_probs, np.ndarray):\n                    all_probs_list = all_probs.flatten().tolist()\n                else:\n                    all_probs_list = all_probs\n\n                rounded_list = [round(num, 2) for num in all_probs_list]\n\n                json_data = json.dumps(\n                    {\"seq_id\": seq_id, \"probability\": rounded_list}\n                )\n                out_f.write(json_data + \"\\n\")\n</code></pre>"},{"location":"reference/#src.baktfold.features.autotune.autotune_batching_real_data","title":"<code>autotune_batching_real_data(model_dir, model_name, cpu, threads, probe_seqs, start_bs=1, max_bs=100, step=5)</code>","text":"<p>Autotunes the batch size for a given model and set of sequences.</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>str</code> <p>The directory where the model is stored.</p> required <code>model_name</code> <code>str</code> <p>The name of the model.</p> required <code>cpu</code> <code>bool</code> <p>Whether to use the CPU or not.</p> required <code>threads</code> <code>int</code> <p>The number of threads to use.</p> required <code>probe_seqs</code> <code>list</code> <p>A list of sequences to use for probing.</p> required <code>start_bs</code> <code>int</code> <p>The starting batch size to use.</p> <code>1</code> <code>max_bs</code> <code>int</code> <p>The maximum batch size to use.</p> <code>100</code> <code>step</code> <code>int</code> <p>The step size to use when increasing the batch size.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>int</code> <p>The optimal batch size.</p> <code>int</code> <p>The maximum number of residues per batch.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; autotune_batching_real_data(\"model_dir\", \"model_name\", True, 4, [\"ATCG\", \"GCTA\"], 1, 100, 5)\n(10, 100)\n</code></pre> Source code in <code>src/baktfold/features/autotune.py</code> <pre><code>def autotune_batching_real_data(\n    model_dir,\n    model_name,\n    cpu,\n    threads,\n    probe_seqs,\n    start_bs=1,\n    max_bs=100,\n    step=5 # step size\n):\n    \"\"\"\n    Autotunes the batch size for a given model and set of sequences.\n\n    Args:\n      model_dir (str): The directory where the model is stored.\n      model_name (str): The name of the model.\n      cpu (bool): Whether to use the CPU or not.\n      threads (int): The number of threads to use.\n      probe_seqs (list): A list of sequences to use for probing.\n      start_bs (int): The starting batch size to use.\n      max_bs (int): The maximum batch size to use.\n      step (int): The step size to use when increasing the batch size.\n\n    Returns:\n      int: The optimal batch size.\n      int: The maximum number of residues per batch.\n\n    Examples:\n      &gt;&gt;&gt; autotune_batching_real_data(\"model_dir\", \"model_name\", True, 4, [\"ATCG\", \"GCTA\"], 1, 100, 5)\n      (10, 100)\n    \"\"\"\n\n    model, tokenizer = get_T5_model(model_dir, model_name, cpu, threads)\n    model.eval()\n    model.half()\n\n    bs = start_bs\n    results = []\n\n    # get device\n    device = None\n\n    if cpu is True:\n        device = torch.device(\"cpu\")\n    else:\n        # check for NVIDIA/cuda\n        if torch.cuda.is_available():\n            device = torch.device(\"cuda:0\")\n        # check for apple silicon/metal\n        elif torch.backends.mps.is_available():\n            device = torch.device(\"mps\")\n        else:\n            device = torch.device(\"cpu\")\n\n\n    while bs &lt;= max_bs:\n        try:\n\n            # seqs = probe_seqs\n            n_tokens = sum(len(s) for s in probe_seqs)\n\n            logger.info(f\"Running with batch size {bs}\")\n\n            model.eval()\n\n            total_tokens = 0\n            total_time = 0.0\n            batches = 0\n\n            # iterate over real sequences in batches\n            for i in tqdm(range(0, len(probe_seqs), bs), desc=\"Processing\"):\n                batch_seqs = probe_seqs[i : i + bs]\n\n                n_tokens = sum(len(s) for s in batch_seqs)\n                total_tokens += n_tokens\n\n                inputs = tokenizer(\n                    batch_seqs,\n                    padding=True,\n                    return_tensors=\"pt\",\n                )\n                inputs.pop(\"token_type_ids\", None)\n                inputs = {k: v.to(device) for k, v in inputs.items()}\n\n                # timing\n                torch.cuda.synchronize()\n                t0 = time.perf_counter()\n                with torch.no_grad():\n                    _ = model(**inputs)\n                torch.cuda.synchronize()\n\n                total_time += time.perf_counter() - t0\n\n                batches += 1\n\n            time_per_token = total_time / total_tokens\n\n\n            token_per_batch = math.floor(total_tokens / batches)\n\n\n            results.append({\n                \"bs\": bs,\n                \"tokens_per_batch\": token_per_batch,\n                \"time\": total_time,\n                \"time_per_token\": time_per_token,\n            })\n\n            logger.info(f\"Time elapsed {round(total_time,5)}\")\n            logger.info(f\"Tokens per batch {token_per_batch}\")\n\n            bs += step\n\n        except torch.cuda.OutOfMemoryError:\n            torch.cuda.empty_cache()\n            break\n\n\n    if not results:\n        raise RuntimeError(\"No batch size fits on this GPU\")\n\n    best_entry = min(results, key=lambda x: x[\"time_per_token\"])\n\n    best_bs = best_entry[\"bs\"]\n    best_residues = best_entry[\"tokens_per_batch\"]\n    # best_tpt = best_bs[\"time_per_token\"]\n\n    logger.info(f\"##########################\")\n    logger.info(f\"Best batch size: {best_bs}\")\n    # logger.info(f\"best max residues: {best_residues}\")\n\n    return best_bs, best_residues\n</code></pre>"},{"location":"reference/#src.baktfold.features.autotune.run_autotune","title":"<code>run_autotune(input_path, model_dir, model_name, cpu, threads, step, min_batch, max_batch, sample_seqs)</code>","text":"<p>Runs the batch size autotuning process.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>The path to the input file.</p> required <code>model_dir</code> <code>str</code> <p>The directory where the model is stored.</p> required <code>model_name</code> <code>str</code> <p>The name of the model.</p> required <code>cpu</code> <code>bool</code> <p>Whether to use the CPU or not.</p> required <code>threads</code> <code>int</code> <p>The number of threads to use.</p> required <code>step</code> <code>int</code> <p>The step size to use when increasing the batch size.</p> required <code>min_batch</code> <code>int</code> <p>The minimum batch size to use.</p> required <code>max_batch</code> <code>int</code> <p>The maximum batch size to use.</p> required <code>sample_seqs</code> <code>int</code> <p>The number of sequences to sample for probing.</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>The optimal batch size.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; run_autotune(\"input_path\", \"model_dir\", \"model_name\", True, 4, 5, 1, 100, 10)\n10\n</code></pre> Source code in <code>src/baktfold/features/autotune.py</code> <pre><code>def run_autotune(    \n    input_path,\n    model_dir,\n    model_name,\n    cpu,\n    threads,\n    step, \n    min_batch,\n    max_batch, \n    sample_seqs):\n    \"\"\"\n    Runs the batch size autotuning process.\n\n    Args:\n      input_path (str): The path to the input file.\n      model_dir (str): The directory where the model is stored.\n      model_name (str): The name of the model.\n      cpu (bool): Whether to use the CPU or not.\n      threads (int): The number of threads to use.\n      step (int): The step size to use when increasing the batch size.\n      min_batch (int): The minimum batch size to use.\n      max_batch (int): The maximum batch size to use.\n      sample_seqs (int): The number of sequences to sample for probing.\n\n    Returns:\n      int: The optimal batch size.\n\n    Examples:\n      &gt;&gt;&gt; run_autotune(\"input_path\", \"model_dir\", \"model_name\", True, 4, 5, 1, 100, 10)\n      10\n    \"\"\"\n\n    # Dictionary to store the records\n    cds_dict = {}\n\n\n    with open_protein_fasta_file(input_path) as handle:  # handles gzip too\n        records = list(SeqIO.parse(handle, \"fasta\"))\n        if not records:\n            logger.warning(f\"No proteins were found in your input file {input_path}.\")\n            logger.error(\n                f\"Your input file {input_path} is likely not a amino acid FASTA file. Please check this.\"\n            )\n        for record in records:\n            prot_id = record.id\n            feature_location = FeatureLocation(0, len(record.seq))\n            # Seq needs to be saved as the first element in list hence the closed brackets [str(record.seq)]\n            seq_feature = SeqFeature(\n                feature_location,\n                type=\"CDS\",\n                qualifiers={\n                    \"ID\": record.id,\n                    \"description\": record.description,\n                    \"translation\": str(record.seq),\n                },\n            )\n\n            cds_dict[prot_id] = seq_feature\n\n    if not cds_dict:\n        logger.error(f\"Error: no AA protein sequences found in {input_path} file\")\n\n\n    seqs = []\n    for feat in cds_dict.values():\n        v = feat.qualifiers.get(\"translation\")\n        if v and isinstance(v, str):\n            seqs.append(v)\n\n    logger.info(\"Beginning batch size tuning\")\n    logger.info(f\"Using minimum batch size of 1 and maximum batch size of {max_batch}\")\n\n    # define the sampling\n\n    probe_seqs = sample_probe_sequences(seqs, n=sample_seqs)\n\n    batch_size, max_residues = autotune_batching_real_data(\n        model_dir,\n        model_name,\n        cpu,\n        threads,\n        probe_seqs,\n        start_bs=min_batch,\n        max_bs=max_batch,\n        step=step # step size\n    )\n\n    logger.info(f\"Optimal batch size is {batch_size} (residues per batch {max_residues})\")\n\n    return batch_size\n</code></pre>"},{"location":"reference/#src.baktfold.features.autotune.sample_probe_sequences","title":"<code>sample_probe_sequences(seqs, n=5000, seed=0)</code>","text":"<p>samples sequences</p> Source code in <code>src/baktfold/features/autotune.py</code> <pre><code>def sample_probe_sequences(seqs, n=5000, seed=0):\n    \"\"\"\n    samples sequences \n\n    \"\"\"\n\n    rng = random.Random(seed)\n\n    if n &gt;= len(seqs):\n        sampled = list(seqs)\n    else:\n        sampled = rng.sample(seqs, n)\n\n    # sort by sequence length\n    sampled.sort(key=len, reverse=True)\n\n    return sampled\n</code></pre>"},{"location":"reference/#src.baktfold.bakta.annotation.annotate_aa","title":"<code>annotate_aa(aas)</code>","text":"<p>Combines IPS and PSC annotations and marks hypotheticals.</p> <p>Parameters:</p> Name Type Description Default <code>aas</code> <code>Sequence[dict]</code> <p>A sequence of amino acid dictionaries to annotate.</p> required <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; aas = [{'sequence': 'ATG', 'start': 1, 'stop': 3, 'strand': '+'}]\n&gt;&gt;&gt; annotate_aa(aas)\n&gt;&gt;&gt; aas\n[{'sequence': 'ATG', 'start': 1, 'stop': 3, 'strand': '+'}]\n</code></pre> Source code in <code>src/baktfold/bakta/annotation.py</code> <pre><code>def annotate_aa(aas: Sequence[dict]):\n    \"\"\"\n    Combines IPS and PSC annotations and marks hypotheticals.\n\n    Args:\n      aas (Sequence[dict]): A sequence of amino acid dictionaries to annotate.\n\n    Returns:\n      None\n\n    Examples:\n      &gt;&gt;&gt; aas = [{'sequence': 'ATG', 'start': 1, 'stop': 3, 'strand': '+'}]\n      &gt;&gt;&gt; annotate_aa(aas)\n      &gt;&gt;&gt; aas\n      [{'sequence': 'ATG', 'start': 1, 'stop': 3, 'strand': '+'}]\n    \"\"\"\n\n    print('\\tcombine annotations and mark hypotheticals...')\n\n    for aa in aas:\n        print(aa)\n        combine_annotation(aa)  # combine IPS &amp; PSC annotations and mark hypothetical\n    logger.debug('analyze hypotheticals')\n    hypotheticals = [aa for aa in aas if 'hypothetical' in aa]\n    if(len(hypotheticals) &gt; 0):\n        print(f'\\tanalyze hypothetical proteins: {len(hypotheticals)}')\n        print('\\tcalculated proteins statistics')\n</code></pre>"},{"location":"reference/#src.baktfold.bakta.annotation.calc_annotation_score","title":"<code>calc_annotation_score(orf)</code>","text":"<p>Calculates the annotation score for a given ORF.</p> <p>Parameters:</p> Name Type Description Default <code>orf</code> <code>dict</code> <p>The ORF to calculate the annotation score for.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The annotation score for the given ORF.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; calc_annotation_score(orf)\n</code></pre> Source code in <code>src/baktfold/bakta/annotation.py</code> <pre><code>def calc_annotation_score(orf:dict) -&gt; int:\n    \"\"\"\n    Calculates the annotation score for a given ORF.\n\n    Args:\n      orf (dict): The ORF to calculate the annotation score for.\n\n    Returns:\n      int: The annotation score for the given ORF.\n\n    Examples:\n      &gt;&gt;&gt; calc_annotation_score(orf)\n    \"\"\"\n    score = 0\n    if(orf.get('gene', None)):\n        score += 1\n    if(orf.get('product', None)):\n        score += 1\n    return score\n</code></pre>"},{"location":"reference/#src.baktfold.bakta.annotation.combine_annotation","title":"<code>combine_annotation(feature, fast)</code>","text":"<p>Combines annotation information from different sources into a single feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>dict</code> <p>The feature to combine annotation for.</p> required <code>fast</code> <code>bool</code> <p>If True, skips AFDB</p> required <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; combine_annotation(feature)\n</code></pre> Source code in <code>src/baktfold/bakta/annotation.py</code> <pre><code>def combine_annotation(feature: dict, fast: bool):\n    \"\"\"\n    Combines annotation information from different sources into a single feature.\n\n    Args:\n      feature (dict): The feature to combine annotation for.\n      fast (bool): If True, skips AFDB\n    Returns:\n      None\n\n    Examples:\n      &gt;&gt;&gt; combine_annotation(feature)\n    \"\"\"\n\n\n    # ups = feature.get('ups', None)\n    # ips = feature.get('ips', None)\n    # psc = feature.get('psc', None)\n    # pscc = feature.get('pscc', None)\n    pstc = feature.get('pstc', None)\n    # expert_hits = feature.get('expert', [])\n\n    # gene = None\n    # genes = set()\n    # product = None\n\n    product = feature.get('product', None)\n    db_xrefs = feature.get('db_xrefs', [])\n\n    if(pstc):\n\n        # Always normalize pstc to a list\n        if isinstance(pstc, dict):\n            pstc = [pstc]\n        elif isinstance(pstc, str):\n            pstc = [pstc]\n\n        # afdb\n        afdb_entry = None if fast else next(\n            (p for p in pstc if isinstance(p, dict) and p.get('source') == 'afdb'),\n            None\n        )\n        # swissprot\n        swissprot_entry = next((p for p in pstc if isinstance(p, dict) and p.get('source') == 'swissprot'), None)\n        # pdb\n        pdb_entry = next((p for p in pstc if isinstance(p, dict) and p.get('source') == 'pdb'), None)\n        # cath\n        cath_entry = next((p for p in pstc if isinstance(p, dict) and p.get('source') == 'cath'), None)\n        # custom\n        custom_entry = next((p for p in pstc if isinstance(p, dict) and p.get('source') == 'custom_db'), None)\n\n        ####\n        # hierarchy\n        # if it exists, custom is at the top\n        # custom\n        # if not\n        # 1. SwissProt\n        # 2. AFDB\n        # 3. PDB\n        # 4. CATH\n        ####\n\n        if custom_entry:\n            pstc_product = custom_entry['description'] \n        elif swissprot_entry:\n            pstc_product = swissprot_entry['description']\n        elif afdb_entry:\n            pstc_product = afdb_entry['description'] \n        elif pdb_entry:\n            pstc_product = pdb_entry['description'] \n        elif cath_entry:\n            pstc_product = cath_entry['description'] \n        else:\n            pstc_product = None\n\n        if(pstc_product):\n            product = pstc_product\n\n        # Collect all db_xref IDs\n        for entry in pstc:\n            if isinstance(entry, dict):\n                src = entry.get('source', '').lower()\n                eid = entry.get('id')\n                if eid:\n                    if src == 'afdb':\n                        if not fast:\n                            db_xrefs.append(f\"afdb_v6:afdbclusters_{eid}\")\n                    elif src == 'swissprot':\n                        db_xrefs.append(f\"afdb_v6:swissprot_{eid}\")\n                    elif src == 'pdb':\n                        db_xrefs.append(f\"pdb:pdb_{eid}\")\n                    elif src == 'cath':\n                        db_xrefs.append(f\"cath:cath_{eid}\")\n                    elif src == 'custom_db':\n                        db_xrefs.append(f\"custom:custom_{eid}\")\n                    else:\n                        db_xrefs.append(eid)\n            elif isinstance(entry, str):\n                # Preserve any existing string cross-references\n                db_xrefs.append(entry)\n\n        # mark as baktfold\n        mark_as_baktfold(feature)\n\n\n\n\n    # if(len(expert_hits) &gt; 0):\n    #     top_expert_hit = sorted(expert_hits,key=lambda k: (k['rank'], k.get('score', 0), calc_annotation_score(k)), reverse=True)[0]\n    #     expert_genes = top_expert_hit.get('gene', None)\n    #     if(expert_genes):\n    #         expert_genes = expert_genes.replace('/', ',').split(',')\n    #         genes.update(expert_genes)\n    #         gene = expert_genes[0]\n    #     product = top_expert_hit.get('product', None)\n    #     for hit in expert_hits:\n    #         db_xrefs.update(hit.get('db_xrefs', []))\n\n    if product and \"hypothetical protein\" not in product.lower():\n        product = revise_cds_product(product)\n        if(product):\n            if(cfg.compliant):\n                product = insdc.revise_product_insdc(product)\n            feature['product'] = product\n\n            unmark_as_hypothetical(feature)\n\n            # protein_gene_symbol = extract_protein_gene_symbol(product)\n            # if(protein_gene_symbol):\n            #     genes.add(protein_gene_symbol)\n            # revised_genes = revise_cds_gene_symbols(genes)\n            # revised_gene = None\n            # if gene is not None:\n            #     revised_gene = revise_cds_gene_symbols([gene])  # special treatment for selected gene symbol\n            #     revised_gene = revised_gene[0] if len(revised_gene) &gt; 0 else None\n            # if(revised_gene is None  and  len(revised_genes) &gt;= 1):  # select first from gene symbol list if no symbol was selected before\n            #     revised_gene = revised_genes[0]\n\n            # feature['gene'] = revised_gene\n            # feature['genes'] = sorted(revised_genes)\n        else:\n            mark_as_hypothetical(feature)\n    else:\n        mark_as_hypothetical(feature)\n\n    feature['db_xrefs'] = sorted(list(db_xrefs))\n</code></pre>"},{"location":"reference/#src.baktfold.bakta.annotation.extract_protein_gene_symbol","title":"<code>extract_protein_gene_symbol(product)</code>","text":"<p>Extracts a valid gene symbol from a protein name.</p> <p>Parameters:</p> Name Type Description Default <code>product</code> <code>str</code> <p>The protein name to extract a gene symbol from.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The extracted gene symbol.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; extract_protein_gene_symbol(product)\n</code></pre> Source code in <code>src/baktfold/bakta/annotation.py</code> <pre><code>def extract_protein_gene_symbol(product: str) -&gt; str:\n    \"\"\"\n    Extracts a valid gene symbol from a protein name.\n\n    Args:\n      product (str): The protein name to extract a gene symbol from.\n\n    Returns:\n      str: The extracted gene symbol.\n\n    Examples:\n      &gt;&gt;&gt; extract_protein_gene_symbol(product)\n    \"\"\"\n    gene_symbols = []\n    for part in product.split(' '):  # try to extract valid gene symbols\n        m = RE_GENE_SYMBOL.fullmatch(part)\n        if(m):\n            symbol = m[0]\n            logger.info('fix gene: extract symbol from protein name. symbol=%s', symbol)\n            gene_symbols.append(symbol)\n        else:\n            m = RE_PROTEIN_SYMBOL.fullmatch(part)  # extract protein names\n            if(m):\n                symbol = m[0]\n                symbol = symbol[0].lower() + symbol[1:]\n                logger.info('fix gene: extract symbol from protein name. symbol=%s', symbol)\n                gene_symbols.append(symbol)\n    if(len(gene_symbols) == 0):  # None found\n        return None\n    elif(len(gene_symbols) == 1):  # found 1\n        return gene_symbols[0]\n    else:  # found more than one, take the 2nd as the 1st often describes a broader gene family like \"xyz family trancsriptional regulator ...\"\n        return gene_symbols[1]\n</code></pre>"},{"location":"reference/#src.baktfold.bakta.annotation.mark_as_baktfold","title":"<code>mark_as_baktfold(feature)</code>","text":"<p>Adds the baktfold key to the given feature dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>dict</code> <p>The feature dictionary to add the baktfold key to.</p> required <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; feature = {'sequence': 'ATG', 'start': 1, 'stop': 3, 'strand': '+'}\n&gt;&gt;&gt; mark_as_baktfold(feature)\n&gt;&gt;&gt; feature\n{'sequence': 'ATG', 'start': 1, 'stop': 3, 'strand': '+', 'baktfold': True}\n</code></pre> Source code in <code>src/baktfold/bakta/annotation.py</code> <pre><code>def mark_as_baktfold(feature: dict):\n    \"\"\"\n    Adds the baktfold key to the given feature dictionary.\n\n    Args:\n      feature (dict): The feature dictionary to add the baktfold key to.\n\n    Returns:\n      None\n\n    Examples:\n      &gt;&gt;&gt; feature = {'sequence': 'ATG', 'start': 1, 'stop': 3, 'strand': '+'}\n      &gt;&gt;&gt; mark_as_baktfold(feature)\n      &gt;&gt;&gt; feature\n      {'sequence': 'ATG', 'start': 1, 'stop': 3, 'strand': '+', 'baktfold': True}\n    \"\"\"\n    # logger.info(\n    #     f'baktfold found hit(s) for: seq={feature['sequence']}, start={feature['start']}, stop={feature['stop']}, strand={feature['strand']}'\n    # )\n    feature['baktfold'] = True\n</code></pre>"},{"location":"reference/#src.baktfold.bakta.annotation.mark_as_hypothetical","title":"<code>mark_as_hypothetical(feature)</code>","text":"<p>Marks a feature as hypothetical.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>dict</code> <p>The feature to mark as hypothetical.</p> required <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mark_as_hypothetical(feature)\n</code></pre> Source code in <code>src/baktfold/bakta/annotation.py</code> <pre><code>def mark_as_hypothetical(feature: dict):\n    \"\"\"\n    Marks a feature as hypothetical.\n\n    Args:\n      feature (dict): The feature to mark as hypothetical.\n\n    Returns:\n      None\n\n    Examples:\n      &gt;&gt;&gt; mark_as_hypothetical(feature)\n    \"\"\"\n    # no need to actually print this I think\n    # logger.info(\n    #     f'marked as hypothetical: seq={feature['sequence']}, start={feature['start']}, stop={feature['stop']}, strand={feature['strand']}'\n    # )\n    feature['hypothetical'] = True\n    feature['gene'] = None\n    feature['genes'] = []\n    feature['product'] = bc.HYPOTHETICAL_PROTEIN\n</code></pre>"},{"location":"reference/#src.baktfold.bakta.annotation.revise_cds_gene_symbols","title":"<code>revise_cds_gene_symbols(raw_genes)</code>","text":"<p>Revises a list of gene symbols to ensure they are valid.</p> <p>Parameters:</p> Name Type Description Default <code>raw_genes</code> <code>Sequence[str]</code> <p>The list of gene symbols to revise.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>The revised list of gene symbols.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; revise_cds_gene_symbols(raw_genes)\n</code></pre> Source code in <code>src/baktfold/bakta/annotation.py</code> <pre><code>def revise_cds_gene_symbols(raw_genes: Sequence[str]):\n    \"\"\"\n    Revises a list of gene symbols to ensure they are valid.\n\n    Args:\n      raw_genes (Sequence[str]): The list of gene symbols to revise.\n\n    Returns:\n      list: The revised list of gene symbols.\n\n    Examples:\n      &gt;&gt;&gt; revise_cds_gene_symbols(raw_genes)\n    \"\"\"\n    revised_genes = set()\n    for gene in raw_genes:\n        old_gene = gene\n        if(RE_GENE_SUSPECT_CHARS.search(gene)):  # check for suspect characters -&gt; remove gene symbol\n            logger.info('fix gene: remove gene symbol containing suspect chars. old=%s', old_gene)\n            continue\n\n        old_gene = gene\n        gene = gene.replace('gene', '')\n        if(gene != old_gene):  # remove gene literal\n            logger.info('fix gene: remove gene literal. new=%s, old=%s', gene, old_gene)\n\n        old_gene = gene\n        if(gene[-1] == '-'):  # remove orphan hyphen\n            gene = gene[:-1]\n            logger.info('fix gene: remove orphan hypen. new=%s, old=%s', gene, old_gene)\n\n        old_gene = gene\n        gene = RE_MULTIWHITESPACE.sub(' ', gene).strip()  # revise whitespaces\n        if(gene != old_gene):\n            logger.info('fix gene: revise whitespaces. new=%s, old=%s', gene, old_gene)\n\n        old_gene = gene\n        if(RE_GENE_CAPITALIZED.fullmatch(gene)):\n            gene = gene[0].lower() + gene[1:]\n            logger.info('fix gene: lowercase first char. new=%s, old=%s', gene, old_gene)\n\n        if(len(gene) &gt;= 3):\n            if(len(gene) &lt;= 12):\n                revised_genes.add(gene)\n            else:\n                old_gene = gene\n                gene = extract_protein_gene_symbol(gene)\n                if(gene):\n                    revised_genes.add(gene)\n    return list(revised_genes)\n</code></pre>"},{"location":"reference/#src.baktfold.bakta.annotation.revise_cds_product","title":"<code>revise_cds_product(product)</code>","text":"<p>Revise product name for INSDC compliant submissions</p> Source code in <code>src/baktfold/bakta/annotation.py</code> <pre><code>def revise_cds_product(product: str):\n    \"\"\"Revise product name for INSDC compliant submissions\"\"\"\n\n    # from gb \n    # grep \"Uncharacterized protein\" AFDBClusters.tsv | wc -l\n    #     805448\n\n    if \"Uncharacterized protein\" in product:\n        old_product = product\n        product = \"hypothetical protein\"\n        if product != old_product:\n            logger.info(f'fix product: renamed uncharacterized protein as hypothetical. new={product}, old={old_product}')\n\n    # from bakta\n\n    old_product = product\n    product = RE_PROTEIN_WEIGHT.sub(' ', product)  # remove protein weight in (k)Da\n    if(product != old_product):\n        logger.info('fix product: remove protein weight in (k)Da. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    product = re.sub(RE_PROTEIN_PERIOD_SEPARATOR, r'\\1-\\2', product)  # replace separator periods\n    if(product != old_product):\n        logger.info('fix product: replace separator periods. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    if(product[0] in RE_PROTEIN_SUSPECT_CHARS_BEGINNING):  # remove suspect first character\n        product = product[1:]\n        logger.info('fix product: replace invalid first character. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    product = RE_PROTEIN_SUSPECT_CHARS_DISCARD.sub('', product)  # remove suspect characters\n    if(product != old_product):\n        logger.info('fix product: replace invalid characters. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    product = RE_PROTEIN_SUSPECT_CHARS_REPLACE.sub(' ', product)  # replace suspect characters by single whitespace\n    if(product != old_product):\n        logger.info('fix product: replace invalid characters. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    product = RE_PROTEIN_WRONG_PRIMES.sub('\\u0027', product)  # replace wrong prime characters with single quote (U+0027) (') according to https://www.ncbi.nlm.nih.gov/genome/doc/internatprot_nomenguide/\n    if(product != old_product):\n        logger.info('fix product: replace wrong prime characters. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    product = product.replace('FOG:', '')  # remove FOG ids\n    if(product != old_product):\n        logger.info('fix product: replace FOG ids. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    product = RE_PROTEIN_REMNANT.sub('', product)  # remove 'Remnant of's\n    if(product != old_product):\n        logger.info('fix product: replace remnant ofs. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    dufs = []  # replace DUF-containing products\n    for m in RE_DOMAIN_OF_UNKNOWN_FUNCTION.finditer(product):\n        dufs.append(m.group(1).upper())\n    if(len(dufs) &gt;= 1):\n        product = f\"{' '.join(dufs)} domain{'s' if len(dufs) &gt; 1 else ''}-containing protein\"\n        if(product != old_product):\n            logger.info('fix product: revise DUF. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    if('conserved' in product.lower()):  # replace conserved UPF proteins\n        upfs = []\n        for m in RE_UNCHARACTERIZED_PROTEIN_FAMILY.finditer(product):\n            upfs.append(m.group(1).upper())\n        if(len(upfs) &gt;= 1):\n            product = f\"{' '.join(upfs)} protein\"\n            if(product != old_product):\n                logger.info('fix product: revise UPF. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    product = RE_PROTEIN_HOMOLOG.sub('-like protein', product)  # replace Homologs\n    if(product != old_product):\n        if(product.count('protein') == 2):\n            product = product.replace('protein', '', 1)  # remove former protein term if existing\n        logger.info('fix product: replace Homolog. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    product = RE_MULTIWHITESPACE.sub(' ', product).strip()  # revise whitespaces\n    if(product != old_product):\n        logger.info('fix product: revise whitespaces. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    product = RE_PROTEIN_PUTATIVE.sub('putative', product)  # replace putative synonyms)\n    if(product != old_product):\n        logger.info('fix product: replace putative synonyms. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    if(RE_PROTEIN_DOMAIN_CONTAINING.search(product)):  # replace domain name underscores in domain names\n        product = product.replace('_', '-')\n        if(product != old_product):\n            logger.info('fix product: replace domain name underscores. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    if(RE_PROTEIN_TMRNA.fullmatch(product)):\n        product = ''\n        logger.info('fix product: discard pure tmRNA product descriptions. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    if(\n        RE_PROTEIN_CONTIG.search(product) or  # protein containing 'sequence'\n        RE_PROTEIN_NODE.search(product) or  # potential contig name (SPAdes)\n        RE_PROTEIN_POTENTIAL_CONTIG_NAME.search(product) or  # potential contig name (SPAdes)\n        RE_PROTEIN_NO_LETTERS.fullmatch(product)  # no letters -&gt; set to Hypothetical\n        ):  # remove suspect products and mark as hypothetical\n        product = None\n        logger.info('remove product: mark proteins with suspect products as hypothetical. old=%s', old_product)\n\n    return product\n</code></pre>"},{"location":"reference/#src.baktfold.bakta.annotation.unmark_as_hypothetical","title":"<code>unmark_as_hypothetical(feature)</code>","text":"<p>Removes the hypothetical key from the given feature dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>dict</code> <p>The feature dictionary to remove the hypothetical key from.</p> required <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; feature = {'sequence': 'ATG', 'start': 1, 'stop': 3, 'strand': '+'}\n&gt;&gt;&gt; unmark_as_hypothetical(feature)\n&gt;&gt;&gt; feature\n{'sequence': 'ATG', 'start': 1, 'stop': 3, 'strand': '+'}\n</code></pre> Source code in <code>src/baktfold/bakta/annotation.py</code> <pre><code>def unmark_as_hypothetical(feature: dict):\n    \"\"\"\n    Removes the hypothetical key from the given feature dictionary.\n\n    Args:\n      feature (dict): The feature dictionary to remove the hypothetical key from.\n\n    Returns:\n      None\n\n    Examples:\n      &gt;&gt;&gt; feature = {'sequence': 'ATG', 'start': 1, 'stop': 3, 'strand': '+'}\n      &gt;&gt;&gt; unmark_as_hypothetical(feature)\n      &gt;&gt;&gt; feature\n      {'sequence': 'ATG', 'start': 1, 'stop': 3, 'strand': '+'}\n    \"\"\"\n    # logger.info(\n    #     f'unmarked as hypothetical: seq={feature['sequence']}, start={feature['start']}, stop={feature['stop']}, strand={feature['strand']}'\n    # )\n    feature.pop('hypothetical', None)  # remove completely\n</code></pre>"},{"location":"reference/#src.baktfold.bakta.config.check_content_size","title":"<code>check_content_size(file_name, file_path)</code>","text":"<p>Checks if a file is empty.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the file to check.</p> required <code>file_path</code> <code>Path</code> <p>The path to the file to check.</p> required <p>Returns:</p> Type Description <p>None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; check_content_size('file', Path('path/to/file'))\nNone\n</code></pre> Source code in <code>src/baktfold/bakta/config.py</code> <pre><code>def check_content_size(file_name: str, file_path: Path):\n    \"\"\"\n    Checks if a file is empty.\n\n    Args:\n      file_name (str): The name of the file to check.\n      file_path (Path): The path to the file to check.\n\n    Returns:\n      None.\n\n    Examples:\n      &gt;&gt;&gt; check_content_size('file', Path('path/to/file'))\n      None\n    \"\"\"\n    if(file_path.stat().st_size == 0):\n        log.error('empty %s file! path=%s', file_name, file_path)\n        sys.exit(f'ERROR: {file_name} file ({file_path}) is empty!')\n</code></pre>"},{"location":"reference/#src.baktfold.bakta.config.check_db_path","title":"<code>check_db_path(args)</code>","text":"<p>Checks the path to the database.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments passed to the program.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>The path to the database.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; check_db_path(args)\nPath('path/to/db')\n</code></pre> Source code in <code>src/baktfold/bakta/config.py</code> <pre><code>def check_db_path(args: Namespace) -&gt; Path:\n    \"\"\"\n    Checks the path to the database.\n\n    Args:\n      args (Namespace): The arguments passed to the program.\n\n    Returns:\n      Path: The path to the database.\n\n    Examples:\n      &gt;&gt;&gt; check_db_path(args)\n      Path('path/to/db')\n    \"\"\"\n    global db_path\n    env = os.environ.copy()\n    if(args.db):\n        db_dir = args.db\n        log.debug('test parameter db: db_tmp=%s', db_dir)\n        try:\n            db_tmp_path = Path(db_dir).resolve()\n            if(db_tmp_path.is_dir()):\n                db_path = db_tmp_path\n                log.info('database: type=parameter, path=%s', db_path)\n            else:\n                log.error('unvalid database path: type=parameter, path=%s', db_tmp_path)\n                raise IOError()\n        except:\n            sys.exit(f'ERROR: wrong database path! --db={db_dir}')\n    elif('BAKTA_DB' in env):\n        db_dir = env['BAKTA_DB']\n        log.debug('test env db: db_tmp=%s', db_dir)\n        try:\n            db_tmp_path = Path(db_dir).resolve()\n            if(db_tmp_path.is_dir()):\n                db_path = db_tmp_path\n                log.info('database: type=environment, path=%s', db_path)\n            else:\n                log.error('unvalid database path: type=environment, path=%s', db_tmp_path)\n                raise IOError()\n        except:\n            sys.exit(f'ERROR: wrong database path! BAKTA_DB={db_dir}')\n    else:\n        base_dir = Path(__file__).parent\n        db_tmp_path = base_dir.joinpath('db')\n        log.debug('test base_dir db: db_tmp=%s', db_tmp_path)\n        if(db_tmp_path.is_dir()):\n            db_path = db_tmp_path\n            log.info('database: type=base-dir, path=%s', db_path)\n        else:\n            log.error('unvalid database path: type=base-dir, path=%s', db_tmp_path)\n            sys.exit('ERROR: database neither provided nor auto-detected!\\nPlease, download the mandatory db and provide it via either the --db parameter, a BAKTA_DB environment variable or copy it into the Bakta base directory.\\nFor further information please read the readme.md')\n    return db_path\n</code></pre>"},{"location":"reference/#src.baktfold.bakta.config.check_output_path","title":"<code>check_output_path(output, force_override)</code>","text":"<p>Check provided output path</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>string</code> <p>The output directory destination path</p> required <code>force_override</code> <code>Bool</code> <p>Whether to override existing output directories</p> required Source code in <code>src/baktfold/bakta/config.py</code> <pre><code>def check_output_path(output: str, force_override: bool) -&gt; Path:\n    \"\"\"Check provided output path\n    Args:\n        output (string): The output directory destination path\n        force_override (Bool): Whether to override existing output directories\n    \"\"\"\n    global output_path\n    output_path = Path(output)\n    if(not output_path.exists()):\n        try:\n            output_path.mkdir(parents=True, exist_ok=True)\n        except:\n            sys.exit(f'ERROR: could not resolve or create output directory ({output})!')\n    else:\n        if(output_path == Path(os.getcwd())):\n            pass\n        elif(force_override is False):\n            sys.exit(f'ERROR: output path ({output_path}) already exists! Either provide a non-existent new path or force overwriting it via \\'--force\\'')\n        elif(not os.access(str(output_path), os.X_OK)):\n            sys.exit(f'ERROR: output path ({output_path}) not accessible!')\n        elif(not os.access(str(output_path), os.W_OK)):\n            sys.exit(f'ERROR: output path ({output_path}) not writable!')\n    output_path = output_path.resolve()\n    return output_path\n</code></pre>"},{"location":"reference/#src.baktfold.bakta.config.check_readability","title":"<code>check_readability(file_name, file_Path)</code>","text":"<p>Checks if a file is readable.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the file to check.</p> required <code>file_Path</code> <code>Path</code> <p>The path to the file to check.</p> required <p>Returns:</p> Type Description <p>None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; check_readability('file', Path('path/to/file'))\nNone\n</code></pre> Source code in <code>src/baktfold/bakta/config.py</code> <pre><code>def check_readability(file_name: str, file_Path: Path):\n    \"\"\"\n    Checks if a file is readable.\n\n    Args:\n      file_name (str): The name of the file to check.\n      file_Path (Path): The path to the file to check.\n\n    Returns:\n      None.\n\n    Examples:\n      &gt;&gt;&gt; check_readability('file', Path('path/to/file'))\n      None\n    \"\"\"\n    if(not os.access(str(file_Path), os.R_OK)):\n        log.error('%s file not readable! path=%s', file_name, file_Path)\n        sys.exit(f'ERROR: {file_name} file ({file_Path}) not readable!')\n</code></pre>"},{"location":"reference/#src.baktfold.bakta.config.check_threads","title":"<code>check_threads(args)</code>","text":"<p>Checks the number of threads to use.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments passed to the program.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of threads to use.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; check_threads(args)\n4\n</code></pre> Source code in <code>src/baktfold/bakta/config.py</code> <pre><code>def check_threads(args: Namespace) -&gt; int:\n    \"\"\"\n    Checks the number of threads to use.\n\n    Args:\n      args (Namespace): The arguments passed to the program.\n\n    Returns:\n      int: The number of threads to use.\n\n    Examples:\n      &gt;&gt;&gt; check_threads(args)\n      4\n    \"\"\"\n    global threads\n    threads = args.threads\n\n    try:\n        max_threads = len(os.sched_getaffinity(0))\n        log.debug(f\"max-threads={max_threads}\")\n    except AttributeError:\n        max_threads = mp.cpu_count()\n        log.debug(f\"scheduler affinity not availabe! max-threads={max_threads}\")\n\n    if(threads == 0):\n        threads = max_threads\n        log.debug(\"request max threads.\")\n    elif(threads &lt; 0):\n        log.error(\"wrong argument for 'threads' parameter! threads=%i\", threads)\n        sys.exit(f\"ERROR: wrong argument ({threads}) for 'threads' parameter! Value must be larger than/equal to 0.\")\n    elif(threads &gt; max_threads):\n        log.error(\"wrong argument for 'threads' parameter! More threads requested than available: requested=%i, available=%i\", threads, max_threads)\n        sys.exit(f\"ERROR: wrong argument ({threads}) for 'threads' parameter! More threads requested ({threads}) than available ({max_threads}).\")\n    log.info('threads=%i', threads)\n    return threads\n</code></pre>"},{"location":"reference/#src.baktfold.bakta.config.check_tmp_path","title":"<code>check_tmp_path(args)</code>","text":"<p>Checks the path to the temporary directory.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments passed to the program.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>The path to the temporary directory.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; check_tmp_path(args)\nPath('path/to/tmp_dir')\n</code></pre> Source code in <code>src/baktfold/bakta/config.py</code> <pre><code>def check_tmp_path(args: Namespace) -&gt; Path:\n    \"\"\"\n    Checks the path to the temporary directory.\n\n    Args:\n      args (Namespace): The arguments passed to the program.\n\n    Returns:\n      Path: The path to the temporary directory.\n\n    Examples:\n      &gt;&gt;&gt; check_tmp_path(args)\n      Path('path/to/tmp_dir')\n    \"\"\"\n    global tmp_path\n    if(args.tmp_dir is not None):\n        tmp_path = Path(args.tmp_dir)\n        if(not tmp_path.exists()):\n            log.debug('dedicated temp dir does not exist! tmp-dir=%s', tmp_path)\n            sys.exit(f'ERROR: dedicated temporary directory ({tmp_path}) does not exist!')\n        else:\n            log.info('use dedicated temp dir: path=%s', tmp_path)\n            tmp_path = Path(tempfile.mkdtemp(dir=str(tmp_path))).resolve()\n    else:\n        tmp_path = Path(tempfile.mkdtemp()).resolve()\n    log.info('tmp-path=%s', tmp_path)\n    return tmp_path\n</code></pre>"},{"location":"reference/#src.baktfold.bakta.config.check_user_proteins","title":"<code>check_user_proteins(args)</code>","text":"<p>Checks the path to the user proteins file.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments passed to the program.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <p>The path to the user proteins file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; check_user_proteins(args)\nPath('path/to/user_proteins')\n</code></pre> Source code in <code>src/baktfold/bakta/config.py</code> <pre><code>def check_user_proteins(args: Namespace):\n    \"\"\"\n    Checks the path to the user proteins file.\n\n    Args:\n      args (Namespace): The arguments passed to the program.\n\n    Returns:\n      Path: The path to the user proteins file.\n\n    Examples:\n      &gt;&gt;&gt; check_user_proteins(args)\n      Path('path/to/user_proteins')\n    \"\"\"\n    global user_proteins\n    user_proteins = args.proteins\n    if(user_proteins is not None):\n        try:\n            if(user_proteins == ''):\n                raise ValueError('File path argument must be non-empty')\n            user_proteins_path = Path(args.proteins).resolve()\n            check_readability('user proteins', user_proteins_path)\n            check_content_size('user proteins', user_proteins_path)\n            user_proteins = user_proteins_path\n            log.info('user-proteins=%s', user_proteins)\n            return user_proteins\n        except:\n            log.error('provided user proteins file not valid! path=%s', user_proteins)\n            sys.exit(f'ERROR: user proteins file ({user_proteins}) not valid!')\n    else:\n        return None\n</code></pre>"},{"location":"reference/#src.baktfold.bakta.config.setup","title":"<code>setup(args)</code>","text":"<p>Test environment and build a runtime configuration.</p> Source code in <code>src/baktfold/bakta/config.py</code> <pre><code>def setup(args):\n    \"\"\"Test environment and build a runtime configuration.\"\"\"\n    # runtime configurations\n    global env, threads, verbose, debug\n    env['BLAST_USAGE_REPORT'] = 'false'  # prevent BLAST from contacting NCBI\n\n    threads = check_threads(args)\n    verbose = args.verbose\n    log.info('verbose=%s', verbose)\n    debug = args.debug\n    log.info('debug=%s', debug)\n    if(debug):\n        verbose = True\n\n    # input / output path configurations\n    global db_path, db_info, tmp_path, genome_path, min_sequence_length, prefix, output_path, force\n    db_path = check_db_path(args)\n    tmp_path = check_tmp_path(args)\n\n    try:\n        if(args.genome == ''):\n            raise ValueError('File path argument must be non-empty')\n        genome_path = Path(args.genome).resolve()\n        check_readability('genome', genome_path)\n        check_content_size('genome', genome_path)\n    except:\n        log.error('provided genome file not valid! path=%s', args.genome)\n        sys.exit(f'ERROR: genome file ({args.genome}) not valid!')\n    log.info('genome-path=%s', genome_path)\n\n    # input / output configurations\n    min_sequence_length = args.min_contig_length\n    if(min_sequence_length &lt;= 0):\n        log.error(\"wrong argument for 'min-contig-length' parameter! min_contig_length=%s\", min_sequence_length)\n        sys.exit(f\"ERROR: wrong argument ({min_sequence_length}) for 'min- contig-length' parameter! Value must be larger than 0\")\n    log.info('min_contig_length=%s', min_sequence_length)\n    log.info('prefix=%s', prefix)  # set in main.py before global logger config\n    log.info('output-path=%s', output_path)\n    force = args.force\n    log.info('force=%s', force)\n\n    # organism configurations\n    global genus, species, strain, plasmid, taxon\n    genus = args.genus\n    if(genus is not None):\n        genus = genus.strip()\n        if(genus == ''):\n            log.error(\"Empty 'genus' parameter! genus=%s\", genus)\n            sys.exit(f\"ERROR: empty 'genus' parameter!\")\n        else:\n            genus = genus.capitalize()\n    log.info('genus=%s', genus)\n    species = args.species\n    if(species is not None):\n        species = species.strip()\n        if(species == ''):\n            log.error(\"Empty 'species' parameter! species=%s\", species)\n            sys.exit(f\"ERROR: empty 'species' parameter!\")\n        else:\n            species = species.lower()\n    log.info('species=%s', species)\n    strain = args.strain\n    if(strain is not None):\n        strain = strain.strip()\n        if(strain == ''):\n            log.error(\"Empty 'strain' parameter! strain=%s\", species)\n            sys.exit(f\"ERROR: empty 'strain' parameter!\")\n    log.info('strain=%s', strain)\n    plasmid = args.plasmid\n    if(plasmid is not None):\n        plasmid = plasmid.strip()\n        if(plasmid == ''):\n            log.error(\"Empty 'plasmid' parameter! plasmid=%s\", plasmid)\n            sys.exit(f\"ERROR: empty 'plasmid' parameter!\")\n        elif('plasmid' in plasmid.lower()):\n            log.error(\"Wrong 'plasmid' parameter! plasmid=%s\", plasmid)\n            sys.exit(f\"ERROR: wrong 'plasmid' parameter! The plasmid name mustn't contain the word 'plasmid'.\")\n        elif(PLASMID_NAME_PATTERN.fullmatch(plasmid) is None and PLASMID_UNNAMED_PATTERN.fullmatch(plasmid) is None):\n            log.error(\"Wrong 'plasmid' name! plasmid=%s\", plasmid)\n            sys.exit(f\"ERROR: wrong 'plasmid' name! Plasmid names must either be named as 'unnamed', 'unnamed1', ... or start with a lower 'p', contain only digits, dots, underscores and letters, and are limited to 20 characters in total.\")\n    log.info('plasmid=%s', plasmid)\n    taxon = ' '.join([t for t in [genus, species, strain] if t is not None])\n    if(taxon == ''):\n        taxon = None\n\n    # annotation configurations\n    global complete, prodigal_tf, translation_table, keep_sequence_headers, locus, locus_tag, locus_tag_increment, gram, replicons, compliant, user_proteins, user_hmms, meta, regions\n    complete = args.complete\n    log.info('complete=%s', complete)\n    prodigal_tf = args.prodigal_tf\n    if(prodigal_tf is not None):\n        try:\n            if(prodigal_tf == ''):\n                raise ValueError('File path argument must be non-empty')\n            prodigal_tf_path = Path(args.prodigal_tf).resolve()\n            check_readability('prodigal training', prodigal_tf_path)\n            check_content_size('prodigal training', prodigal_tf_path)\n            prodigal_tf = prodigal_tf_path\n        except:\n            log.error('provided prodigal training file not valid! path=%s', prodigal_tf)\n            sys.exit(f'ERROR: Prodigal training file ({prodigal_tf}) not valid!')\n    log.info('prodigal_tf=%s', prodigal_tf)\n    translation_table = args.translation_table\n    log.info('translation_table=%s', translation_table)\n    gram = args.gram\n    log.info('gram=%s', gram)\n    compliant = args.compliant\n    log.info('compliant=%s', compliant)\n    if(compliant):\n        min_sequence_length = 200\n        log.info('compliant mode! min_contig_length=%s', min_sequence_length)\n    meta = args.meta\n    log.info('meta=%s', meta)\n    locus = args.locus\n    if(locus is not None):\n        if(locus == ''):\n            log.error(\"Empty 'locus' parameter! locus=%s\", locus)\n            sys.exit(f\"ERROR: empty 'locus' parameter!\")\n        if(' ' in locus):\n            log.error(\"Whitespace character in 'locus' parameter! locus=%s\", locus)\n            sys.exit(f\"ERROR: whitespace character ({locus}) in 'locus' parameter!\")\n        if(bc.RE_INSDC_ID_PREFIX.fullmatch(locus) is None):\n            log.error(\"Invalid 'locus' parameter! locus=%s\", locus)\n            sys.exit(f\"ERROR: invalid 'locus' parameter ({locus})!\\nLocus prefixes must contain between 1 and 20 alphanumeric or '-_' characters.\")\n    log.info('locus=%s', locus)\n    locus_tag = args.locus_tag\n    if(locus_tag is not None):\n        if(locus_tag == ''):\n            log.error(\"Empty 'locus-tag' parameter! locus=%s\", locus_tag)\n            sys.exit(f\"ERROR: empty 'locus-tag' parameter!\")\n        if(' ' in locus_tag):\n            log.error(\"Whitespace character in 'locus-tag' parameter! locus-tag=%s\", locus_tag)\n            sys.exit(f\"ERROR: whitespace character ({locus_tag}) in 'locus-tag' parameter!\")\n        if(compliant):\n            if(bc.RE_INSDC_LOCUSTAG_PREFIX.fullmatch(locus_tag) is None):\n                log.error(\"INSDC-incompliant 'locus-tag' parameter! locus-tag=%s\", locus_tag)\n                sys.exit(f\"ERROR: INSDC-incompliant 'locus-tag' parameter ({locus_tag})!\\nINSDC Locus tag prefixes must contain between 3 and 12 alphanumeric uppercase characters and start with a letter.\")\n        else:\n            if(bc.RE_LOCUSTAG_PREFIX.fullmatch(locus_tag) is None):\n                log.error(\"Invalid 'locus-tag' parameter! locus-tag=%s\", locus_tag)\n                sys.exit(f\"ERROR: invalid 'locus-tag' parameter ({locus_tag})!\\nLocus tag prefixes must contain between 1 and 24 alphanumeric characters or '_.-' signs.\")\n    log.info('locus-tag=%s', locus_tag)\n    locus_tag_increment = args.locus_tag_increment\n    log.info('locus-tag-increment=%s', locus_tag_increment)\n    keep_sequence_headers = args.keep_contig_headers\n    log.info('keep_contig_headers=%s', keep_sequence_headers)\n    replicons = args.replicons\n    if(replicons is not None):\n        try:\n            if(replicons == ''):\n                raise ValueError('File path argument must be non-empty')\n            replicon_table_path = Path(args.replicons).resolve()\n            check_readability('replicon table', replicon_table_path)\n            check_content_size('replicon table', replicon_table_path)\n            replicons = replicon_table_path\n        except:\n            log.error('provided replicon file not valid! path=%s', replicons)\n            sys.exit(f'ERROR: replicon table file ({replicons}) not valid!')\n    log.info('replicon-table=%s', replicons)\n    user_proteins = check_user_proteins(args)\n    user_hmms = args.hmms\n    if(user_hmms is not None):\n        try:\n            if(user_hmms == ''):\n                raise ValueError('File path argument must be non-empty')\n            user_hmms_path = Path(user_hmms).resolve()\n            check_readability('HMM', user_hmms_path)\n            check_content_size('HMM', user_hmms_path)\n            user_hmms = user_hmms_path\n        except:\n            log.error('provided HMM file not valid! path=%s', user_hmms)\n            sys.exit(f'ERROR: HMM file ({user_hmms}) not valid!')\n\n    regions = args.regions\n    if(regions is not None):\n        try:\n            if(regions == ''):\n                raise ValueError('File path argument must be non-empty')\n            regions_path = Path(args.regions).resolve()\n            check_readability('regions', regions_path)\n            check_content_size('regions', regions_path)\n            regions = regions_path\n        except:\n            log.error('provided regions file not valid! path=%s', regions)\n            sys.exit(f'ERROR: regions file ({regions}) not valid!')\n    log.info('regions=%s', regions)\n\n\n    # workflow configurations\n    global skip_trna, skip_tmrna, skip_rrna, skip_ncrna, skip_ncrna_region, skip_crispr, skip_cds, skip_pseudo, skip_sorf, skip_gap, skip_ori, skip_filter, skip_plot\n    skip_trna = args.skip_trna\n    log.info('skip-tRNA=%s', skip_trna)\n    skip_tmrna = args.skip_tmrna\n    log.info('skip-tmRNA=%s', skip_tmrna)\n    skip_rrna = args.skip_rrna\n    log.info('skip-rRNA=%s', skip_rrna)\n    skip_ncrna = args.skip_ncrna\n    log.info('skip-ncRNA=%s', skip_ncrna)\n    skip_ncrna_region = args.skip_ncrna_region\n    log.info('skip-ncRNA-region=%s', skip_ncrna_region)\n    skip_crispr = args.skip_crispr\n    log.info('skip-CRISPR=%s', skip_crispr)\n    skip_cds = args.skip_cds\n    log.info('skip-CDS=%s', skip_cds)\n    skip_pseudo = args.skip_pseudo\n    log.info('skip-pseudo=%s', skip_pseudo)\n    skip_sorf = args.skip_sorf\n    log.info('skip-sORF=%s', skip_sorf)\n    skip_gap = args.skip_gap\n    log.info('skip-gap=%s', skip_gap)\n    skip_ori = args.skip_ori\n    log.info('skip-ori=%s', skip_ori)\n    skip_filter = args.skip_filter\n    log.info('skip-filter=%s', skip_filter)\n    skip_plot = args.skip_plot\n    log.info('skip-plot=%s', skip_plot)\n</code></pre>"},{"location":"reference/#src.baktfold.bakta.pstc.fetch_db_pscc_result","title":"<code>fetch_db_pscc_result(conn, uniref50_id)</code>","text":"<p>Fetches the PSCC result for a given uniref50_id from a sqlite3 database.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <code>sqlite3.Connection</code> <p>The connection to the sqlite3 database.</p> required <code>uniref50_id</code> <code>str</code> <p>The uniref50_id to fetch the PSCC result for.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>The PSCC result for the given uniref50_id.</p> Source code in <code>src/baktfold/bakta/pstc.py</code> <pre><code>def fetch_db_pscc_result(conn: sqlite3.Connection, uniref50_id: str):\n    \"\"\"\n    Fetches the PSCC result for a given uniref50_id from a sqlite3 database.\n\n    Args:\n      conn (sqlite3.Connection): The connection to the sqlite3 database.\n      uniref50_id (str): The uniref50_id to fetch the PSCC result for.\n\n    Returns:\n      tuple: The PSCC result for the given uniref50_id.\n    \"\"\"\n    c = conn.cursor()\n    c.execute('select * from pscc where uniref50_id=?', (uniref50_id,))\n    rec = c.fetchone()\n    c.close()\n    return rec\n</code></pre>"},{"location":"reference/#src.baktfold.bakta.pstc.fetch_sql_description","title":"<code>fetch_sql_description(conn, source, accession)</code>","text":"<p>Fetches the product description for a given source and accession from a sqlite3 database.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <code>sqlite3.Connection</code> <p>The connection to the sqlite3 database.</p> required <code>source</code> <code>str</code> <p>The source of the accession.</p> required <code>accession</code> <code>str</code> <p>The accession to fetch the description for.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The product description for the given source and accession.</p> Source code in <code>src/baktfold/bakta/pstc.py</code> <pre><code>def fetch_sql_description(conn, source, accession):\n    \"\"\"\n    Fetches the product description for a given source and accession from a sqlite3 database.\n\n    Args:\n      conn (sqlite3.Connection): The connection to the sqlite3 database.\n      source (str): The source of the accession.\n      accession (str): The accession to fetch the description for.\n\n    Returns:\n      str: The product description for the given source and accession.\n    \"\"\"\n    table_map = {\n        'swissprot': 'swissprot',\n        'afdb': 'afdbclusters',\n        'pdb': 'pdb',\n        'cath': 'cath',\n    }\n\n    table = table_map.get(source)\n    if table is None:\n        return None\n\n    # special case for cath, which can have multiple top hits (greedy) - multidomain proteins\n    if table == 'cath':\n        cursor = conn.execute(\"SELECT product FROM cath WHERE id = ?\", (accession,))\n    else:\n        cursor = conn.execute(f\"SELECT product FROM {table} WHERE id = ?\", (accession,))\n\n    row = cursor.fetchone()\n    return row[0] if row else None\n</code></pre>"},{"location":"reference/#src.baktfold.bakta.pstc.fetch_sql_description_threadsafe","title":"<code>fetch_sql_description_threadsafe(db_path, source, accession)</code>","text":"<p>makes new connection every time so don't have 2 CATH accessions colliding (for multi domain proteins)</p> Source code in <code>src/baktfold/bakta/pstc.py</code> <pre><code>def fetch_sql_description_threadsafe(db_path, source, accession):\n    \"\"\"\n    makes new connection every time so don't have 2 CATH accessions colliding (for multi domain proteins)\n    \"\"\"\n    import sqlite3\n    conn = sqlite3.connect(db_path, uri=True, check_same_thread=False)\n    try:\n        result = fetch_sql_description(conn, source, accession)\n    finally:\n        conn.close()\n    return result\n</code></pre>"},{"location":"reference/#src.baktfold.bakta.pstc.lookup_custom","title":"<code>lookup_custom(features, baktfold_db, custom_annotations)</code>","text":"<p>Lookup PSTC information from custom db</p> Source code in <code>src/baktfold/bakta/pstc.py</code> <pre><code>def lookup_custom(features: Sequence[dict], baktfold_db: Path, custom_annotations: Path):\n    \"\"\"Lookup PSTC information from custom db \"\"\"\n    no_pstc_lookups = 0\n\n    # custom\n    if custom_annotations:\n        custom_dict = {}\n        with open(f\"{custom_annotations}\", \"r\") as f:\n            reader = csv.reader(f, delimiter=\"\\t\")\n            for row in reader:\n                if len(row) &gt;= 2:\n                    custom_dict[row[0]] = row[1]\n\n    for feat in features:\n        pstc = feat.get('pstc')\n        if not pstc:\n            continue\n\n        # Normalize to list for consistent handling\n        pstc_entries = pstc if isinstance(pstc, list) else [pstc]\n\n        for entry in pstc_entries:\n            accession = entry.get('id')\n            source = entry.get('source')\n            if source == 'custom_db':\n                if accession in custom_dict:\n                    entry['description'] = custom_dict[accession]\n                else:\n                    entry['description'] = accession # mark as accession if no annotation given for custom for now\n\n        # Write back normalized list or single entry\n        feat['pstc'] = pstc_entries if isinstance(pstc, list) else pstc_entries[0]\n\n    return features\n</code></pre>"},{"location":"reference/#src.baktfold.bakta.pstc.lookup_sql","title":"<code>lookup_sql(features, baktfold_db, threads)</code>","text":"<p>Lookup PSTC information</p> Source code in <code>src/baktfold/bakta/pstc.py</code> <pre><code>def lookup_sql(features: Sequence[dict], baktfold_db: Path, threads: int):\n    \"\"\"Lookup PSTC information\"\"\"\n\n    no_pstc_lookups = 0\n    # try:\n    rec_futures = []\n    logger.info(\"Looking up PSTC descriptions\")\n    # with sqlite3.connect(f\"file:{baktfold_db.joinpath('baktfold.db')}?mode=ro&amp;nolock=1&amp;cache=shared\", uri=True, check_same_thread=False) as conn:\n    #     conn.execute('PRAGMA omit_readlock;')\n    #     conn.row_factory = sqlite3.Row\n    with ThreadPoolExecutor(max_workers=max(10, threads)) as tpe:  # use min 10 threads for IO bound non-CPU lookups\n        for feat in features:\n            pstc = feat.get('pstc')\n            if not pstc:\n                continue\n\n            # Normalize to list for consistent handling\n            pstc_entries = pstc if isinstance(pstc, list) else [pstc]\n\n            rec_futures = []\n            for entry in pstc_entries:\n                accession = entry.get('id')\n\n                source = entry.get('source')\n\n                # submit database query as a future\n                future = tpe.submit(fetch_sql_description_threadsafe, baktfold_db.joinpath('baktfold.db'), source, accession)\n                rec_futures.append((entry, future))\n\n\n            # Collect results\n            for entry, future in rec_futures:\n                desc = future.result()\n                if desc:\n                    entry['description'] = desc\n                else:\n                    if entry.get('source') == 'custom_db':\n                        entry['description'] = accession  # keep accession if custom_db but missing\n                    else:\n                        entry['description'] = \"hypothetical protein\"\n\n        # Write back normalized list or single entry\n        feat['pstc'] = pstc_entries if isinstance(pstc, list) else pstc_entries[0]\n\n    # except Exception as ex:\n    #     logger.error('Could not read PSTCs from db!')\n    #     raise Exception('SQL error!', ex)\n    # log.info('looked-up=%i', no_pstc_lookups)\n\n    return features\n</code></pre>"},{"location":"reference/#src.baktfold.bakta.pstc.parse","title":"<code>parse(features, foldseek_df, db_name='swissprot', has_duplicate_locus=False)</code>","text":"<p>Update CDS in place with PSTC hits from foldseek_df if they pass filters.</p> <p>has_duplicate_locus - some euks have multiple CDS per locus tag</p> Source code in <code>src/baktfold/bakta/pstc.py</code> <pre><code>def parse(features: Sequence[dict], foldseek_df: pd.DataFrame, db_name: str = 'swissprot', has_duplicate_locus: bool = False) -&gt; None:\n    \"\"\"Update CDS in place with PSTC hits from foldseek_df if they pass filters.\n\n    has_duplicate_locus - some euks have multiple CDS per locus tag\n\n    \"\"\"\n\n    # Convert foldseek_df to a lookup table keyed by query ID\n    foldseek_hits = {row['query']: row for _, row in foldseek_df.iterrows()}\n\n    # each query maps to a list of rows now (to handle multiple CATH greedy tophits for multidomain proteins)\n    foldseek_hits = defaultdict(list)\n    for _, row in foldseek_df.iterrows():\n        foldseek_hits[row['query']].append(row)\n\n    updated_count = 0\n\n\n    for cds in features:\n        if has_duplicate_locus:\n            aa_identifier = cds.get('id')\n        else:\n            aa_identifier = cds.get('locus')\n\n        if aa_identifier not in foldseek_hits:\n            continue  # no hits, skip\n\n        cds_updated = False  \n\n        # Iterate over *all* hits for this query\n        for row in foldseek_hits[aa_identifier]:\n            query_cov = float(row['qCov'])\n            subject_cov = float(row['tCov'])\n            identity = float(row['fident'])\n            evalue = float(row['evalue'])\n            bitscore = float(row['bitscore'])\n            target_id = row['target']\n\n            # Extract accession depending on database\n            if db_name in {\"swissprot\", \"afdb\"}:\n                accession = target_id.split('-')[1]\n            elif db_name == \"pdb\":\n                accession = target_id.split('-')[0]\n            else:  # cath and custom\n                accession = target_id\n\n            # Apply your filters\n            if (\n                query_cov &gt;= bc.MIN_PSTC_QCOVERAGE\n                and subject_cov &gt;= bc.MIN_PSTC_TCOVERAGE\n                and identity &gt;= bc.MIN_PSTC_IDENTITY\n            ):\n                new_pstc = {\n                    'source': db_name,\n                    'id': accession,\n                    'query_cov': query_cov,\n                    'subject_cov': subject_cov,\n                    'identity': identity,\n                    'score': bitscore,\n                    'evalue': evalue,\n                }\n\n                # Append or initialize 'pstc'\n                if 'pstc' in cds:\n                    if isinstance(cds['pstc'], dict):\n                        cds['pstc'] = [cds['pstc'], new_pstc]\n                    elif isinstance(cds['pstc'], list):\n                        cds['pstc'].append(new_pstc)\n                    else:\n                        cds['pstc'] = [new_pstc]\n                else:\n                    cds['pstc'] = [new_pstc]  # \u2190 ensure list, since we may have many hits\n\n\n                cds_updated = True  \n\n        # Increment only once per CDS that had at least one valid hit (CATH might have multiple)\n        if cds_updated:\n            updated_count += 1\n\n    logger.info(f\"PSTC for {db_name} updated in place for {updated_count} CDSs\")\n    return features\n</code></pre>"},{"location":"reference/#src.baktfold.utils.external_tools.ExternalTool","title":"<code>ExternalTool</code>","text":"<p>Class for running external tools.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>str</code> <p>The path to the tool to run.</p> required <code>input</code> <code>str</code> <p>The input file.</p> required <code>output</code> <code>str</code> <p>The output file.</p> required <code>params</code> <code>str</code> <p>The parameters to pass to the tool.</p> required <code>logdir</code> <code>Path</code> <p>The directory to store log files.</p> required <p>Attributes:</p> Name Type Description <code>command</code> <code>List[str]</code> <p>The command to run.</p> <code>out_log</code> <code>str</code> <p>The path to the stdout log file.</p> <code>err_log</code> <code>str</code> <p>The path to the stderr log file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n&gt;&gt;&gt; tool.command\n[\"tool\", \"params\", \"output\", \"input\"]\n&gt;&gt;&gt; tool.out_log\n\"logdir/tool_1234567890abcdef1234567890abcdef.out\"\n&gt;&gt;&gt; tool.err_log\n\"logdir/tool_1234567890abcdef1234567890abcdef.err\"\n</code></pre> Source code in <code>src/baktfold/utils/external_tools.py</code> <pre><code>class ExternalTool:\n    \"\"\"\n    Class for running external tools.\n\n    Args:\n      tool (str): The path to the tool to run.\n      input (str): The input file.\n      output (str): The output file.\n      params (str): The parameters to pass to the tool.\n      logdir (Path): The directory to store log files.\n\n    Attributes:\n      command (List[str]): The command to run.\n      out_log (str): The path to the stdout log file.\n      err_log (str): The path to the stderr log file.\n\n    Examples:\n      &gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n      &gt;&gt;&gt; tool.command\n      [\"tool\", \"params\", \"output\", \"input\"]\n      &gt;&gt;&gt; tool.out_log\n      \"logdir/tool_1234567890abcdef1234567890abcdef.out\"\n      &gt;&gt;&gt; tool.err_log\n      \"logdir/tool_1234567890abcdef1234567890abcdef.err\"\n    \"\"\"\n    def __init__(self, tool: str, input: str, output: str, params: str, logdir: Path):\n        \"\"\"\n        Initializes an ExternalTool object.\n\n        Args:\n          tool (str): The path to the tool to run.\n          input (str): The input file.\n          output (str): The output file.\n          params (str): The parameters to pass to the tool.\n          logdir (Path): The directory to store log files.\n\n        Attributes:\n          command (List[str]): The command to run.\n          out_log (str): The path to the stdout log file.\n          err_log (str): The path to the stderr log file.\n\n        Examples:\n          &gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n          &gt;&gt;&gt; tool.command\n          [\"tool\", \"params\", \"output\", \"input\"]\n          &gt;&gt;&gt; tool.out_log\n          \"logdir/tool_1234567890abcdef1234567890abcdef.out\"\n          &gt;&gt;&gt; tool.err_log\n          \"logdir/tool_1234567890abcdef1234567890abcdef.err\"\n        \"\"\"\n        logdir = Path(logdir)   \n        self.command: List[str] = self._build_command(tool, input, output, params)\n        Path(logdir).mkdir(parents=True, exist_ok=True)\n        command_hash = hashlib.sha256(self.command_as_str.encode(\"utf-8\")).hexdigest()\n        tool_name = Path(tool).name\n        logfile_prefix: Path = logdir / f\"{tool_name}_{command_hash}\"\n        self.out_log = f\"{logfile_prefix}.out\"\n        self.err_log = f\"{logfile_prefix}.err\"\n\n    @property\n    def command_as_str(self) -&gt; str:\n        \"\"\"\n        Returns the command as a string.\n\n        Returns:\n          str: The command as a string.\n\n        Examples:\n          &gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n          &gt;&gt;&gt; tool.command_as_str\n          \"tool params output input\"\n        \"\"\"\n        return shlex.join(self.command)\n\n    @staticmethod\n    def _build_command(tool: str, input: str, output: str, params: str) -&gt; List[str]:\n        \"\"\"\n        Builds the command to run.\n\n        Args:\n          tool (str): The path to the tool to run.\n          input (str): The input file.\n          output (str): The output file.\n          params (str): The parameters to pass to the tool.\n\n        Returns:\n          List[str]: The command to run.\n\n        Examples:\n          &gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n          &gt;&gt;&gt; tool._build_command(\"tool\", \"input\", \"output\", \"params\")\n          [\"tool\", \"params\", \"output\", \"input\"]\n        \"\"\"\n        # note: shlex.join does not allow us to shlex.split() later\n        # this is explicitly a \" \".join()\n        command = \" \".join([tool, params, output, input])\n        escaped_command = shlex.split(command)\n        return escaped_command\n\n    def run(self) -&gt; None:\n        \"\"\"\n        Runs the tool.\n\n        Examples:\n          &gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n          &gt;&gt;&gt; tool.run()\n        \"\"\"\n        with open(self.out_log, \"w\") as stdout_fh, open(self.err_log, \"w\") as stderr_fh:\n            print(f\"Command line: {self.command_as_str}\", file=stderr_fh)\n            logger.info(f\"Started running {self.command_as_str} ...\")\n            self._run_core(self.command, stdout_fh=stdout_fh, stderr_fh=stderr_fh)\n            logger.info(f\"Done running {self.command_as_str}\")\n\n    \"\"\"\n    stream to terminal (aria2c) so the user knows how long it is taking\n    \"\"\"\n\n    def run_stream(self) -&gt; None:\n        \"\"\"\n        Runs the tool and streams the output to the terminal.\n\n        Examples:\n          &gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n          &gt;&gt;&gt; tool.run_stream()\n        \"\"\"\n        with open(self.out_log, \"w\") as stdout_fh, open(self.err_log, \"w\") as stderr_fh:\n            print(f\"Command line: {self.command_as_str}\", file=stderr_fh)\n            logger.info(f\"Started running {self.command_as_str} ...\")\n\n            process = subprocess.Popen(\n                self.command,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT,\n                bufsize=1,\n                universal_newlines=True,\n            )\n\n            for line in process.stdout:\n                print(line, end=\"\")         # Live output to terminal\n                stdout_fh.write(line)       # Also write to stdout log\n\n            process.stdout.close()\n            return_code = process.wait()\n\n            logger.info(f\"Done running {self.command_as_str}\")\n\n            if return_code != 0:\n                raise subprocess.CalledProcessError(return_code, self.command)\n\n\n    @staticmethod\n    def _run_core(command: List[str], stdout_fh, stderr_fh) -&gt; None:\n        \"\"\"\n        Runs the tool.\n\n        Args:\n          command (List[str]): The command to run.\n          stdout_fh: The file handle to write stdout to.\n          stderr_fh: The file handle to write stderr to.\n\n        Examples:\n          &gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n          &gt;&gt;&gt; tool._run_core([\"tool\", \"params\", \"output\", \"input\"], stdout_fh, stderr_fh)\n        \"\"\"\n        subprocess.check_call(command, stdout=stdout_fh, stderr=stderr_fh)\n\n    @staticmethod\n    def run_tools(\n        tools_to_run: Tuple[\"ExternalTool\", ...], ctx: Optional[click.Context] = None\n    ) -&gt; None:\n        \"\"\"\n        Runs a list of tools.\n\n        Args:\n          tools_to_run (Tuple[ExternalTool]): The list of tools to run.\n          ctx (Optional[click.Context]): The click context.\n\n        Examples:\n          &gt;&gt;&gt; tool1 = ExternalTool(\"tool1\", \"input1\", \"output1\", \"params1\", \"logdir\")\n          &gt;&gt;&gt; tool2 = ExternalTool(\"tool2\", \"input2\", \"output2\", \"params2\", \"logdir\")\n          &gt;&gt;&gt; ExternalTool.run_tools((tool1, tool2))\n          &gt;&gt;&gt; ExternalTool.run_tools((tool1, tool2), ctx)\n        \"\"\"\n        for tool in tools_to_run:\n            try:\n                tool.run()\n            except subprocess.CalledProcessError as error:\n                logger.error(\n                    f\"Error calling {tool.command_as_str} (return code {error.returncode})\"\n                )\n                logger.error(f\"Please check stdout log file: {tool.out_log}\")\n                logger.error(f\"Please check stderr log file: {tool.err_log}\")\n                logger.error(\"Temporary files are preserved for debugging\")\n                logger.error(\"Exiting...\")\n\n                if ctx:\n                    ctx.exit(1)\n                else:\n                    sys.exit(1)\n\n    \"\"\"\n    Only one toolf\n    \"\"\"\n\n    @staticmethod\n    def run_tool(tool: \"ExternalTool\", ctx: Optional[click.Context] = None) -&gt; None:\n        \"\"\"\n        Runs the given external tool.\n\n        Args:\n          tool (ExternalTool): The external tool to run.\n          ctx (Optional[click.Context]): The click context to use. Defaults to None.\n\n        Returns:\n          None.\n\n        Raises:\n          subprocess.CalledProcessError: If there is an error calling the external tool.\n\n        Examples:\n          &gt;&gt;&gt; tool = ExternalTool()\n          &gt;&gt;&gt; ExternalTool.run_tool(tool)\n          None\n        \"\"\"\n        try:\n            tool.run()\n        except subprocess.CalledProcessError as error:\n            logger.error(\n                f\"Error calling {tool.command_as_str} (return code {error.returncode})\"\n            )\n            logger.error(f\"Please check stdout log file: {tool.out_log}\")\n            logger.error(f\"Please check stderr log file: {tool.err_log}\")\n            logger.error(\"Temporary files are preserved for debugging\")\n            logger.error(\"Exiting...\")\n\n            if ctx:\n                ctx.exit(1)\n            else:\n                sys.exit(1)\n\n\n    \"\"\"\n    Only download - so can print the aria2c output to screen\n    \"\"\"\n\n    @staticmethod\n    def run_download(tool: \"ExternalTool\", ctx: Optional[click.Context] = None) -&gt; None:\n        \"\"\"\n        Runs the given external tool and prints the aria2c output to the screen.\n\n        Args:\n          tool (ExternalTool): The external tool to run.\n          ctx (Optional[click.Context]): The click context to use. Defaults to None.\n\n        Returns:\n          None.\n\n        Raises:\n          subprocess.CalledProcessError: If there is an error calling the external tool.\n\n        Examples:\n          &gt;&gt;&gt; tool = ExternalTool()\n          &gt;&gt;&gt; ExternalTool.run_download(tool)\n          None\n        \"\"\"\n        try:\n            tool.run_stream()\n        except subprocess.CalledProcessError as error:\n            logger.error(\n                f\"Error calling {tool.command_as_str} (return code {error.returncode})\"\n            )\n            logger.error(f\"Please check stdout log file: {tool.out_log}\")\n            logger.error(f\"Please check stderr log file: {tool.err_log}\")\n            logger.error(\"Temporary files are preserved for debugging\")\n            logger.error(\"Exiting...\")\n\n            if ctx:\n                ctx.exit(1)\n            else:\n                sys.exit(1)\n</code></pre>"},{"location":"reference/#src.baktfold.utils.external_tools.ExternalTool.command_as_str","title":"<code>command_as_str: str</code>  <code>property</code>","text":"<p>Returns the command as a string.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The command as a string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n&gt;&gt;&gt; tool.command_as_str\n\"tool params output input\"\n</code></pre>"},{"location":"reference/#src.baktfold.utils.external_tools.ExternalTool.__init__","title":"<code>__init__(tool, input, output, params, logdir)</code>","text":"<p>Initializes an ExternalTool object.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>str</code> <p>The path to the tool to run.</p> required <code>input</code> <code>str</code> <p>The input file.</p> required <code>output</code> <code>str</code> <p>The output file.</p> required <code>params</code> <code>str</code> <p>The parameters to pass to the tool.</p> required <code>logdir</code> <code>Path</code> <p>The directory to store log files.</p> required <p>Attributes:</p> Name Type Description <code>command</code> <code>List[str]</code> <p>The command to run.</p> <code>out_log</code> <code>str</code> <p>The path to the stdout log file.</p> <code>err_log</code> <code>str</code> <p>The path to the stderr log file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n&gt;&gt;&gt; tool.command\n[\"tool\", \"params\", \"output\", \"input\"]\n&gt;&gt;&gt; tool.out_log\n\"logdir/tool_1234567890abcdef1234567890abcdef.out\"\n&gt;&gt;&gt; tool.err_log\n\"logdir/tool_1234567890abcdef1234567890abcdef.err\"\n</code></pre> Source code in <code>src/baktfold/utils/external_tools.py</code> <pre><code>def __init__(self, tool: str, input: str, output: str, params: str, logdir: Path):\n    \"\"\"\n    Initializes an ExternalTool object.\n\n    Args:\n      tool (str): The path to the tool to run.\n      input (str): The input file.\n      output (str): The output file.\n      params (str): The parameters to pass to the tool.\n      logdir (Path): The directory to store log files.\n\n    Attributes:\n      command (List[str]): The command to run.\n      out_log (str): The path to the stdout log file.\n      err_log (str): The path to the stderr log file.\n\n    Examples:\n      &gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n      &gt;&gt;&gt; tool.command\n      [\"tool\", \"params\", \"output\", \"input\"]\n      &gt;&gt;&gt; tool.out_log\n      \"logdir/tool_1234567890abcdef1234567890abcdef.out\"\n      &gt;&gt;&gt; tool.err_log\n      \"logdir/tool_1234567890abcdef1234567890abcdef.err\"\n    \"\"\"\n    logdir = Path(logdir)   \n    self.command: List[str] = self._build_command(tool, input, output, params)\n    Path(logdir).mkdir(parents=True, exist_ok=True)\n    command_hash = hashlib.sha256(self.command_as_str.encode(\"utf-8\")).hexdigest()\n    tool_name = Path(tool).name\n    logfile_prefix: Path = logdir / f\"{tool_name}_{command_hash}\"\n    self.out_log = f\"{logfile_prefix}.out\"\n    self.err_log = f\"{logfile_prefix}.err\"\n</code></pre>"},{"location":"reference/#src.baktfold.utils.external_tools.ExternalTool.run","title":"<code>run()</code>","text":"<p>Runs the tool.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n&gt;&gt;&gt; tool.run()\n</code></pre> Source code in <code>src/baktfold/utils/external_tools.py</code> <pre><code>def run(self) -&gt; None:\n    \"\"\"\n    Runs the tool.\n\n    Examples:\n      &gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n      &gt;&gt;&gt; tool.run()\n    \"\"\"\n    with open(self.out_log, \"w\") as stdout_fh, open(self.err_log, \"w\") as stderr_fh:\n        print(f\"Command line: {self.command_as_str}\", file=stderr_fh)\n        logger.info(f\"Started running {self.command_as_str} ...\")\n        self._run_core(self.command, stdout_fh=stdout_fh, stderr_fh=stderr_fh)\n        logger.info(f\"Done running {self.command_as_str}\")\n</code></pre>"},{"location":"reference/#src.baktfold.utils.external_tools.ExternalTool.run_download","title":"<code>run_download(tool, ctx=None)</code>  <code>staticmethod</code>","text":"<p>Runs the given external tool and prints the aria2c output to the screen.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>ExternalTool</code> <p>The external tool to run.</p> required <code>ctx</code> <code>Optional[click.Context]</code> <p>The click context to use. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> <p>Raises:</p> Type Description <code>subprocess.CalledProcessError</code> <p>If there is an error calling the external tool.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool = ExternalTool()\n&gt;&gt;&gt; ExternalTool.run_download(tool)\nNone\n</code></pre> Source code in <code>src/baktfold/utils/external_tools.py</code> <pre><code>@staticmethod\ndef run_download(tool: \"ExternalTool\", ctx: Optional[click.Context] = None) -&gt; None:\n    \"\"\"\n    Runs the given external tool and prints the aria2c output to the screen.\n\n    Args:\n      tool (ExternalTool): The external tool to run.\n      ctx (Optional[click.Context]): The click context to use. Defaults to None.\n\n    Returns:\n      None.\n\n    Raises:\n      subprocess.CalledProcessError: If there is an error calling the external tool.\n\n    Examples:\n      &gt;&gt;&gt; tool = ExternalTool()\n      &gt;&gt;&gt; ExternalTool.run_download(tool)\n      None\n    \"\"\"\n    try:\n        tool.run_stream()\n    except subprocess.CalledProcessError as error:\n        logger.error(\n            f\"Error calling {tool.command_as_str} (return code {error.returncode})\"\n        )\n        logger.error(f\"Please check stdout log file: {tool.out_log}\")\n        logger.error(f\"Please check stderr log file: {tool.err_log}\")\n        logger.error(\"Temporary files are preserved for debugging\")\n        logger.error(\"Exiting...\")\n\n        if ctx:\n            ctx.exit(1)\n        else:\n            sys.exit(1)\n</code></pre>"},{"location":"reference/#src.baktfold.utils.external_tools.ExternalTool.run_stream","title":"<code>run_stream()</code>","text":"<p>Runs the tool and streams the output to the terminal.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n&gt;&gt;&gt; tool.run_stream()\n</code></pre> Source code in <code>src/baktfold/utils/external_tools.py</code> <pre><code>def run_stream(self) -&gt; None:\n    \"\"\"\n    Runs the tool and streams the output to the terminal.\n\n    Examples:\n      &gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n      &gt;&gt;&gt; tool.run_stream()\n    \"\"\"\n    with open(self.out_log, \"w\") as stdout_fh, open(self.err_log, \"w\") as stderr_fh:\n        print(f\"Command line: {self.command_as_str}\", file=stderr_fh)\n        logger.info(f\"Started running {self.command_as_str} ...\")\n\n        process = subprocess.Popen(\n            self.command,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            bufsize=1,\n            universal_newlines=True,\n        )\n\n        for line in process.stdout:\n            print(line, end=\"\")         # Live output to terminal\n            stdout_fh.write(line)       # Also write to stdout log\n\n        process.stdout.close()\n        return_code = process.wait()\n\n        logger.info(f\"Done running {self.command_as_str}\")\n\n        if return_code != 0:\n            raise subprocess.CalledProcessError(return_code, self.command)\n</code></pre>"},{"location":"reference/#src.baktfold.utils.external_tools.ExternalTool.run_tool","title":"<code>run_tool(tool, ctx=None)</code>  <code>staticmethod</code>","text":"<p>Runs the given external tool.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>ExternalTool</code> <p>The external tool to run.</p> required <code>ctx</code> <code>Optional[click.Context]</code> <p>The click context to use. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> <p>Raises:</p> Type Description <code>subprocess.CalledProcessError</code> <p>If there is an error calling the external tool.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool = ExternalTool()\n&gt;&gt;&gt; ExternalTool.run_tool(tool)\nNone\n</code></pre> Source code in <code>src/baktfold/utils/external_tools.py</code> <pre><code>@staticmethod\ndef run_tool(tool: \"ExternalTool\", ctx: Optional[click.Context] = None) -&gt; None:\n    \"\"\"\n    Runs the given external tool.\n\n    Args:\n      tool (ExternalTool): The external tool to run.\n      ctx (Optional[click.Context]): The click context to use. Defaults to None.\n\n    Returns:\n      None.\n\n    Raises:\n      subprocess.CalledProcessError: If there is an error calling the external tool.\n\n    Examples:\n      &gt;&gt;&gt; tool = ExternalTool()\n      &gt;&gt;&gt; ExternalTool.run_tool(tool)\n      None\n    \"\"\"\n    try:\n        tool.run()\n    except subprocess.CalledProcessError as error:\n        logger.error(\n            f\"Error calling {tool.command_as_str} (return code {error.returncode})\"\n        )\n        logger.error(f\"Please check stdout log file: {tool.out_log}\")\n        logger.error(f\"Please check stderr log file: {tool.err_log}\")\n        logger.error(\"Temporary files are preserved for debugging\")\n        logger.error(\"Exiting...\")\n\n        if ctx:\n            ctx.exit(1)\n        else:\n            sys.exit(1)\n</code></pre>"},{"location":"reference/#src.baktfold.utils.external_tools.ExternalTool.run_tools","title":"<code>run_tools(tools_to_run, ctx=None)</code>  <code>staticmethod</code>","text":"<p>Runs a list of tools.</p> <p>Parameters:</p> Name Type Description Default <code>tools_to_run</code> <code>Tuple[ExternalTool]</code> <p>The list of tools to run.</p> required <code>ctx</code> <code>Optional[click.Context]</code> <p>The click context.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool1 = ExternalTool(\"tool1\", \"input1\", \"output1\", \"params1\", \"logdir\")\n&gt;&gt;&gt; tool2 = ExternalTool(\"tool2\", \"input2\", \"output2\", \"params2\", \"logdir\")\n&gt;&gt;&gt; ExternalTool.run_tools((tool1, tool2))\n&gt;&gt;&gt; ExternalTool.run_tools((tool1, tool2), ctx)\n</code></pre> Source code in <code>src/baktfold/utils/external_tools.py</code> <pre><code>@staticmethod\ndef run_tools(\n    tools_to_run: Tuple[\"ExternalTool\", ...], ctx: Optional[click.Context] = None\n) -&gt; None:\n    \"\"\"\n    Runs a list of tools.\n\n    Args:\n      tools_to_run (Tuple[ExternalTool]): The list of tools to run.\n      ctx (Optional[click.Context]): The click context.\n\n    Examples:\n      &gt;&gt;&gt; tool1 = ExternalTool(\"tool1\", \"input1\", \"output1\", \"params1\", \"logdir\")\n      &gt;&gt;&gt; tool2 = ExternalTool(\"tool2\", \"input2\", \"output2\", \"params2\", \"logdir\")\n      &gt;&gt;&gt; ExternalTool.run_tools((tool1, tool2))\n      &gt;&gt;&gt; ExternalTool.run_tools((tool1, tool2), ctx)\n    \"\"\"\n    for tool in tools_to_run:\n        try:\n            tool.run()\n        except subprocess.CalledProcessError as error:\n            logger.error(\n                f\"Error calling {tool.command_as_str} (return code {error.returncode})\"\n            )\n            logger.error(f\"Please check stdout log file: {tool.out_log}\")\n            logger.error(f\"Please check stderr log file: {tool.err_log}\")\n            logger.error(\"Temporary files are preserved for debugging\")\n            logger.error(\"Exiting...\")\n\n            if ctx:\n                ctx.exit(1)\n            else:\n                sys.exit(1)\n</code></pre>"},{"location":"reference/#src.baktfold.utils.util.log_fmt","title":"<code>log_fmt = '[&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss}&lt;/green&gt;] &lt;level&gt;{level: &lt;8}&lt;/level&gt; | &lt;level&gt;{message}&lt;/level&gt;'</code>  <code>module-attribute</code>","text":"<p>begin and end functions</p>"},{"location":"reference/#src.baktfold.utils.util.OrderedCommands","title":"<code>OrderedCommands</code>","text":"<p>             Bases: <code>click.Group</code></p> <p>This class will preserve the order of subcommands, which is useful when printing --help</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>class OrderedCommands(click.Group):\n    \"\"\"This class will preserve the order of subcommands, which is useful when printing --help\"\"\"\n\n    def list_commands(self, ctx: click.Context):\n        \"\"\"\n        Returns a list of subcommands in the order they were added.\n\n        Args:\n          ctx (click.Context): The click context.\n\n        Returns:\n          list: A list of subcommands in the order they were added.\n        \"\"\"\n        return list(self.commands)\n</code></pre>"},{"location":"reference/#src.baktfold.utils.util.OrderedCommands.list_commands","title":"<code>list_commands(ctx)</code>","text":"<p>Returns a list of subcommands in the order they were added.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>click.Context</code> <p>The click context.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list of subcommands in the order they were added.</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def list_commands(self, ctx: click.Context):\n    \"\"\"\n    Returns a list of subcommands in the order they were added.\n\n    Args:\n      ctx (click.Context): The click context.\n\n    Returns:\n      list: A list of subcommands in the order they were added.\n    \"\"\"\n    return list(self.commands)\n</code></pre>"},{"location":"reference/#src.baktfold.utils.util.baktfold_base","title":"<code>baktfold_base(rel_path)</code>","text":"<p>Returns the absolute path to the given relative path.</p> <p>Parameters:</p> Name Type Description Default <code>rel_path</code> <code>str</code> <p>The relative path to the file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The absolute path to the file.</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def baktfold_base(rel_path):\n    \"\"\"\n    Returns the absolute path to the given relative path.\n\n    Args:\n      rel_path (str): The relative path to the file.\n\n    Returns:\n      str: The absolute path to the file.\n    \"\"\"\n    return os.path.join(os.path.dirname(os.path.realpath(__file__)), rel_path)\n</code></pre>"},{"location":"reference/#src.baktfold.utils.util.begin_baktfold","title":"<code>begin_baktfold(params, subcommand, no_log=False)</code>","text":"<p>Begin baktfold process.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Dict[str, Any]</code> <p>A dictionary of parameters for baktfold.</p> required <code>subcommand</code> <code>str</code> <p>Subcommand indicating the baktfold operation.</p> required <code>no_log</code> <code>bool</code> <p>No log file</p> <code>False</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Start time of the baktfold process.</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def begin_baktfold(params: Dict[str, Any], subcommand: str, no_log: bool = False) -&gt; int:\n    \"\"\"\n    Begin baktfold process.\n\n    Parameters:\n        params (Dict[str, Any]): A dictionary of parameters for baktfold.\n        subcommand (str): Subcommand indicating the baktfold operation.\n        no_log (bool): No log file\n\n    Returns:\n        int: Start time of the baktfold process.\n    \"\"\"\n    # get start time\n    start_time = time.time()\n\n    cfg.run_start = datetime.now()\n\n    # initial logging stuff\n    if not no_log:\n        log_file = os.path.join(params[\"--output\"], f\"baktfold_{subcommand}_{start_time}.log\")\n        # adds log file\n        logger.add(log_file)\n    logger.add(lambda _: sys.exit(1), level=\"ERROR\")\n\n    print_splash()\n    logger.info(\"baktfold: rapid &amp; standardized annotation of bacterial genomes, MAGs &amp; plasmids using protein structural information\")\n\n    logger.info(f\"You are using baktfold version {get_version()}\")\n    logger.info(\"Repository homepage is https://github.com/gbouras13/baktfold\")\n    logger.info(f\"You are running baktfold {subcommand}\")\n    logger.info(f\"Listing parameters\")\n    for key, value in params.items():\n        logger.info(f\"Parameter: {key} {value}\")\n\n    return start_time\n</code></pre>"},{"location":"reference/#src.baktfold.utils.util.clean_up_temporary_files","title":"<code>clean_up_temporary_files(output, prefix)</code>","text":"<p>Clean up temporary files generated during the baktfold process.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Path</code> <p>Path to the output directory.</p> required <code>prefix</code> <code>str</code> <p>prefix str</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def clean_up_temporary_files(output: Path, prefix: str) -&gt; None:\n    \"\"\"\n    Clean up temporary files generated during the baktfold process.\n\n    Parameters:\n        output (Path): Path to the output directory.\n        prefix (str): prefix str\n\n\n    Returns:\n        None\n    \"\"\"\n\n    baktfold_aa: Path = Path(output) / f\"{prefix}_aa.fasta\"\n    result_tsv_swissprot: Path = Path(output) / \"foldseek_results_swissprot.tsv\"\n    result_tsv_afdb: Path = Path(output) / \"foldseek_results_afdb_clusters.tsv\"\n    result_tsv_pdb: Path = Path(output) / \"foldseek_results_pdb.tsv\"\n    result_tsv_cath: Path = Path(output) / \"foldseek_results_cath.tsv\"\n    result_tsv_custom: Path = Path(output) / \"foldseek_results_custom.tsv\"\n    foldseek_db: Path = Path(output) / \"foldseek_db\"\n    result_db_base: Path = Path(output) / \"result_db\"\n    temp_db: Path = Path(output) / \"temp_db\"\n\n    remove_directory(result_db_base)\n    remove_directory(temp_db)\n    remove_directory(foldseek_db)\n\n    remove_file(baktfold_aa)\n    remove_file(result_tsv_swissprot)\n    remove_file(result_tsv_afdb)\n    remove_file(result_tsv_pdb)\n    remove_file(result_tsv_custom)\n    remove_file(result_tsv_cath)\n</code></pre>"},{"location":"reference/#src.baktfold.utils.util.echo_click","title":"<code>echo_click(msg, log=None)</code>","text":"<p>Prints a message to stdout and optionally to a log file.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>The message to print.</p> required <code>log</code> <code>str</code> <p>The path to the log file.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def echo_click(msg, log=None):\n    \"\"\"\n    Prints a message to stdout and optionally to a log file.\n\n    Args:\n      msg (str): The message to print.\n      log (str): The path to the log file.\n\n    Returns:\n      None\n    \"\"\"\n    click.echo(msg, nl=False, err=True)\n    if log:\n        with open(log, \"a\") as lo:\n            lo.write(msg)\n</code></pre>"},{"location":"reference/#src.baktfold.utils.util.end_baktfold","title":"<code>end_baktfold(start_time, subcommand)</code>","text":"<p>Finish baktfold process and log elapsed time.</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>float</code> <p>Start time of the process.</p> required <code>subcommand</code> <code>str</code> <p>Subcommand name indicating the baktfold operation.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def end_baktfold(start_time: float, subcommand: str) -&gt; None:\n    \"\"\"\n    Finish baktfold process and log elapsed time.\n\n    Parameters:\n        start_time (float): Start time of the process.\n        subcommand (str): Subcommand name indicating the baktfold operation.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Determine elapsed time\n    elapsed_time = time.time() - start_time\n    elapsed_time = round(elapsed_time, 2)\n\n    cfg.run_end = datetime.now()\n    run_duration = (cfg.run_end - cfg.run_start).total_seconds()\n    # logger.info(f'If you use these results please cite Baktfold: https://doi.org/{bc.BAKTA_DOI}')\n    logger.info(f'If you use these results please cite Baktfold: https://github.com/gbouras13/baktfold')\n    logger.info(f'baktfold {subcommand} successfully finished in {int(run_duration / 60):02}:{int(run_duration % 60):02} [mm:ss].')\n\n\n    # Show elapsed time for the process\n    logger.info(f\"baktfold {subcommand} has finished\")\n    logger.info(\"Elapsed time: \" + str(elapsed_time) + \" seconds\")\n</code></pre>"},{"location":"reference/#src.baktfold.utils.util.get_type_rank","title":"<code>get_type_rank(f)</code>","text":"<p>ranks eukaryotic features 1) in order of gene -&gt; mRNA -&gt; CDS and gene -&gt; tRNA dynamically adjusts if 5'UTR and 3'UTR is present</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def get_type_rank(f):\n    \"\"\"\n    ranks eukaryotic features 1) in order of gene -&gt; mRNA -&gt; CDS and gene -&gt; tRNA\n    dynamically adjusts if 5'UTR and 3'UTR is present\n    \"\"\"\n    t = f['type']\n    strand = f.get('strand', '+')  # default to + if missing\n\n    # fixed ranks\n    base_order = {\n        'gene': 0,\n        'mRNA': 1,\n        'cds': 3,\n        'tRNA': 6\n    }\n\n    # dynamic UTR ordering\n    if t == bc.FEATURE_5UTR:\n        return 2 if strand == '+' else 4\n    if t == bc.FEATURE_3UTR:\n        return 4 if strand == '+' else 2\n\n    return base_order.get(t, 99)   # non-protein features become 99\n</code></pre>"},{"location":"reference/#src.baktfold.utils.util.get_version","title":"<code>get_version()</code>","text":"<p>Returns the version number from the VERSION file.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>The version number.</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def get_version():\n    \"\"\"\n    Returns the version number from the VERSION file.\n\n    Returns:\n      str: The version number.\n    \"\"\"\n    with open(baktfold_base(\"VERSION\"), \"r\") as f:\n        version = f.readline()\n    return version\n</code></pre>"},{"location":"reference/#src.baktfold.utils.util.print_citation","title":"<code>print_citation()</code>","text":"<p>Prints the contents of the CITATION file to stdout.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def print_citation():\n    \"\"\"\n    Prints the contents of the CITATION file to stdout.\n\n    Returns:\n      None\n    \"\"\"\n    with open(baktfold_base(\"CITATION\"), \"r\") as f:\n        for line in f:\n            echo_click(line)\n</code></pre>"},{"location":"reference/#src.baktfold.utils.util.print_splash","title":"<code>print_splash()</code>","text":"<p>Prints the splash screen to stdout.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def print_splash():\n    \"\"\"\n    Prints the splash screen to stdout.\n\n    Returns:\n      None\n    \"\"\"\n    click.echo(\n        \"\"\"\\b\n\n  _           _    _    __      _     _ \n | |         | |  | |  / _|    | |   | |\n | |__   __ _| | _| |_| |_ ___ | | __| |\n | '_ \\ / _` | |/ / __|  _/ _ \\| |/ _` |\n | |_) | (_| |   &lt;| |_| || (_) | | (_| |\n |_.__/ \\__,_|_|\\_\\\\__|_| \\___/|_|\\__,_|\n\n\n\"\"\"\n    )\n</code></pre>"},{"location":"reference/#src.baktfold.utils.util.remove_directory","title":"<code>remove_directory(dir_path)</code>","text":"<p>Remove a directory and all its contents if it exists.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Path</code> <p>Path to the directory to remove.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def remove_directory(dir_path: Path) -&gt; None:\n    \"\"\"\n    Remove a directory and all its contents if it exists.\n\n    Parameters:\n        dir_path (Path): Path to the directory to remove.\n\n    Returns:\n        None\n    \"\"\"\n    if dir_path.exists():\n        shutil.rmtree(dir_path)\n</code></pre>"},{"location":"reference/#src.baktfold.utils.util.remove_file","title":"<code>remove_file(file_path)</code>","text":"<p>Remove a file if it exists.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the file to remove.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def remove_file(file_path: Path) -&gt; None:\n    \"\"\"\n    Remove a file if it exists.\n\n    Parameters:\n        file_path (Path): Path to the file to remove.\n\n    Returns:\n        None\n    \"\"\"\n    if file_path.exists():\n        file_path.unlink()  # Use unlink to remove the file\n</code></pre>"},{"location":"reference/#src.baktfold.utils.util.sort_euk_feature_key","title":"<code>sort_euk_feature_key(f)</code>","text":"<p>Sorts a feature dictionary by start, locus, type rank, and stop.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>dict</code> <p>The feature dictionary.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple of the sorted values.</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def sort_euk_feature_key(f):\n    \"\"\"\n    Sorts a feature dictionary by start, locus, type rank, and stop.\n\n    Args:\n      f (dict): The feature dictionary.\n\n    Returns:\n      tuple: A tuple of the sorted values.\n    \"\"\"\n    start = f.get('start', float('inf'))\n    stop = f.get('stop', float('inf'))\n    locus = f.get('locus')\n    type_rank = get_type_rank(f)\n\n    if locus and type_rank != 99:\n        # Within a locus \u2192 sort by type rank second and stop last (if multiple CDS e.g.)\n        return (start, 0, locus, type_rank, stop)\n    else:\n        # Non-locus or non-gene features \u2192 sort only by start\n        return (start, 1, '', 99, stop)\n</code></pre>"},{"location":"reference/#src.baktfold.utils.util.touch_file","title":"<code>touch_file(path)</code>","text":"<p>Update the access and modification times of a file to the current time, creating the file if it does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def touch_file(path: Path) -&gt; None:\n    \"\"\"\n    Update the access and modification times of a file to the current time, creating the file if it does not exist.\n\n    Parameters:\n        path (Path): Path to the file.\n\n    Returns:\n        None\n    \"\"\"\n    with open(path, \"a\"):\n        os.utime(path, None)\n</code></pre>"},{"location":"reference/#src.baktfold.utils.validation.check_dependencies","title":"<code>check_dependencies()</code>","text":"<p>Checks the dependencies and versions of non Python programs (i.e. Foldseek)</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/utils/validation.py</code> <pre><code>def check_dependencies() -&gt; None:\n    \"\"\"\n    Checks the dependencies and versions of non Python programs (i.e. Foldseek)\n\n    Parameters:\n        None\n\n    Returns:\n        None\n\n    \"\"\"\n\n    #############\n    # foldseek\n    #############\n    try:\n        process = sp.Popen([\"foldseek\", \"version\"], stdout=sp.PIPE, stderr=sp.STDOUT)\n    except:\n        logger.error(\"Foldseek not found. Please reinstall baktfold.\")\n\n    foldseek_out, _ = process.communicate()\n    foldseek_out = foldseek_out.decode()\n\n    foldseek_version = foldseek_out.strip()\n\n    if \"941cd33\" in foldseek_version:\n        foldseek_major_version=10\n        foldseek_minor_version=\"941cd33\"\n        logger.info(\n        f\"Foldseek version found is v{foldseek_major_version}.{foldseek_minor_version}\"\n    )\n    else:\n        logger.warning(f\"Foldseek version found is v{foldseek_version}\")\n        logger.warning(f\"baktfold is recommended to be run with Foldseek v10.941cd33\")\n        logger.warning(f\"Using a different Foldseek version is likely to work without issue, but this cannot be guaranteed.\")\n\n\n    logger.info(\"Foldseek version is ok\")\n</code></pre>"},{"location":"reference/#src.baktfold.utils.validation.check_genbank_and_prokka","title":"<code>check_genbank_and_prokka(filepath, euk)</code>","text":"<p>Validate that an input file is a readable GenBank file and check whether it was annotated using Prokka. The function transparently supports compressed files (e.g., .gz, .bz2, .xz, .zst) via <code>xopen</code>.</p> Validation steps <p>\u2022 Attempts to parse the file as GenBank using Biopython. \u2022 Logs an error and returns None if no GenBank records are found. \u2022 Checks the COMMENT field of each record for a Prokka signature   (\"Annotated using prokka\", case-insensitive). \u2022 If no Prokka annotation is detected, a warning is logged but parsing continues as it is a valid genbank.</p>"},{"location":"reference/#src.baktfold.utils.validation.check_genbank_and_prokka--parameters","title":"Parameters","text":"str <p>Path to the GenBank or compressed GenBank file.</p> flag <p>whether or not the input is eukaryotic (skips prokka)</p>"},{"location":"reference/#src.baktfold.utils.validation.check_genbank_and_prokka--returns","title":"Returns","text":"<p>list[SeqRecord] or None     A list of Biopython SeqRecord objects if parsing succeeds.     Returns None if the file is not valid GenBank or cannot be parsed.</p> Source code in <code>src/baktfold/utils/validation.py</code> <pre><code>def check_genbank_and_prokka(filepath, euk):\n    \"\"\"\n    Validate that an input file is a readable GenBank file and check whether it was\n    annotated using Prokka. The function transparently supports compressed files\n    (e.g., .gz, .bz2, .xz, .zst) via `xopen`.\n\n    Validation steps:\n      \u2022 Attempts to parse the file as GenBank using Biopython.\n      \u2022 Logs an error and returns None if no GenBank records are found.\n      \u2022 Checks the COMMENT field of each record for a Prokka signature\n        (\"Annotated using prokka\", case-insensitive).\n      \u2022 If no Prokka annotation is detected, a warning is logged but parsing continues as it is a valid genbank.\n\n    Parameters\n    ----------\n    filepath : str\n        Path to the GenBank or compressed GenBank file.\n    euk: flag\n        whether or not the input is eukaryotic (skips prokka)\n\n    Returns\n    -------\n    list[SeqRecord] or None\n        A list of Biopython SeqRecord objects if parsing succeeds.\n        Returns None if the file is not valid GenBank or cannot be parsed.\n    \"\"\"\n\n    logger.add(lambda _: sys.exit(1), level=\"ERROR\")\n\n    is_valid_genbank = False\n    is_prokka = False\n\n    try:\n        # Use xopen so gzip/bz2/xz/zst work automatically\n        with xopen(filepath, \"rb\") as handle:\n            # SeqIO.parse expects text handle -&gt; decode\n            # Use .read() is too big; instead wrap in TextIOWrapper\n            import io\n            text_handle = io.TextIOWrapper(handle, encoding=\"utf-8\", errors=\"replace\")\n\n            records = list(SeqIO.parse(text_handle, \"genbank\"))\n\n        if not records:\n            logger.error(f\"Input file {filepath} is not GenBank format. Please check your input\")\n            return None\n        else:\n            is_valid_genbank = True\n\n\n        # Scan comments for Prokka signature\n        if not euk:\n            for rec in records:\n                comment = rec.annotations.get(\"comment\", \"\") or \"\"\n                if \"annotated using prokka\" in comment.lower():\n                    is_prokka = True\n                    break\n\n\n            if is_prokka is False:\n                logger.warning(f\"Input file {filepath} does not appear to come from Prokka.\")\n                logger.warning(f\"Conversion will proceed but no guarantee of success.\")\n\n    except Exception:\n        logger.error(f\"There was an error parsing {filepath}. Please check your input\")\n        return None\n\n    return records\n</code></pre>"},{"location":"reference/#src.baktfold.utils.validation.instantiate_dirs","title":"<code>instantiate_dirs(output_dir, force)</code>","text":"<p>Checks and instantiates the output directory.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>Union[str, Path]</code> <p>Path to the output directory.</p> required <code>force</code> <code>bool</code> <p>Force flag indicating whether to overwrite existing directory.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Final output directory path.</p> Source code in <code>src/baktfold/utils/validation.py</code> <pre><code>def instantiate_dirs(output_dir: Union[str, Path], force: bool) -&gt; Path:\n    \"\"\"\n    Checks and instantiates the output directory.\n\n    Parameters:\n        output_dir (Union[str, Path]): Path to the output directory.\n        force (bool): Force flag indicating whether to overwrite existing directory.\n\n    Returns:\n        Path: Final output directory path.\n    \"\"\"\n\n    # Checks the output directory\n    # remove outdir on force\n    logger.add(lambda _: sys.exit(1), level=\"ERROR\")\n    logger.info(f\"Checking the output directory {output_dir}\")\n    if force is True:\n        if Path(output_dir).exists():\n            logger.info(f\"Removing {output_dir} because --force was specified\")\n            shutil.rmtree(output_dir)\n        else:\n            logger.info(\n                \"--force was specified even though the output directory does not already exist. Continuing\"\n            )\n    else:\n        if Path(output_dir).exists():\n            logger.error(\n                \"Output directory already exists and force was not specified. Please specify -f or --force to overwrite the output directory\"\n            )\n\n    # instantiate outdir\n    if Path(output_dir).exists() is False:\n        Path(output_dir).mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"reference/#src.baktfold.utils.validation.validate_input","title":"<code>validate_input(input, threads)</code>","text":"<p>Validate the input file format and retrieve genomic data.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Path</code> <p>Path to the input file.</p> required <code>threads</code> <code>int</code> <p>Number of threads to use for prediction.</p> required <p>Returns:</p> Type Description <code>Dict[str, Union[bool, Dict]]</code> <p>Dict[str, Union[bool, Dict]]: A dictionary containing validation flags and genomic data.</p> Source code in <code>src/baktfold/utils/validation.py</code> <pre><code>def validate_input(input: Path, threads: int) -&gt; Dict[str, Union[bool, Dict]]:\n    \"\"\"\n    Validate the input file format and retrieve genomic data.\n\n    Parameters:\n        input (Path): Path to the input file.\n        threads (int): Number of threads to use for prediction.\n\n    Returns:\n        Dict[str, Union[bool, Dict]]: A dictionary containing validation flags and genomic data.\n    \"\"\"\n\n    # validates input\n    fasta_flag = False\n    gb_dict, method = get_genbank(input)\n\n    if not gb_dict:\n        logger.warning(f\"{input} was not a Genbank format file\")\n        logger.warning(\n            f\"Now checking if the input {input} is a genome in nucleotide FASTA format\"\n        )\n        logger.warning(f\"pyrodigal-gv will be used to predict CDS\")\n        logger.warning(f\"baktfold will not predict tRNAs, tmRNAs or CRISPR repeats\")\n        logger.warning(\n            f\"Please use pharokka https://github.com/gbouras13/pharokka if you would like to predict these\"\n        )\n        logger.warning(\n            f\"And then use the genbank output pharokka.gbk as --input for baktfold\"\n        )\n\n        # check the contig ids are &lt; 54 chars\n        for record in SeqIO.parse(input, \"fasta\"):\n            # Check if the length of the record ID is 54 characters or more\n            if len(record.id) &gt;= 54:\n                logger.warning(\n                    f\"The contig header {record.id} is longer than 54 characters. It is recommended that you use shorter contig headers as this can create issues downstream.\"\n                )\n\n        gb_dict = get_fasta_run_pyrodigal_gv(input, threads)\n        if not gb_dict:\n            logger.warning(\"Error: no records found in FASTA file\")\n            logger.error(\"Please check your input\")\n        else:\n            logger.info(\n                f\"Successfully parsed input {input} as a FASTA and predicted CDS\"\n            )\n            fasta_flag = True\n    else:\n        logger.info(f\"Successfully parsed input {input} as a {method} style Genbank file.\")\n\n    return fasta_flag, gb_dict, method\n</code></pre>"},{"location":"reference/#src.baktfold.utils.validation.validate_outfile","title":"<code>validate_outfile(outfile, force)</code>","text":"<p>Checks and instantiates the output file for baktfold convert-prokka</p> <p>Parameters:</p> Name Type Description Default <code>outfile</code> <code>Union[str, Path]</code> <p>Path to the output file.</p> required <code>force</code> <code>bool</code> <p>Force flag indicating whether to overwrite existing outfile.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Final output file path.</p> Source code in <code>src/baktfold/utils/validation.py</code> <pre><code>def validate_outfile(outfile: Union[str, Path], force: bool) -&gt; Path:\n    \"\"\"\n    Checks and instantiates the output file for baktfold convert-prokka\n\n    Parameters:\n        outfile (Union[str, Path]): Path to the output file.\n        force (bool): Force flag indicating whether to overwrite existing outfile.\n\n    Returns:\n        Path: Final output file path.\n    \"\"\"\n\n    # Checks the output directory\n    # remove outdir on force\n    logger.add(lambda _: sys.exit(1), level=\"ERROR\")\n    logger.info(f\"Checking the output file {outfile}\")\n    if force is True:\n        if Path(outfile).exists():\n            logger.info(f\"Removing {outfile} because --force was specified\")\n            Path(outfile).unlink()\n        else:\n            logger.info(\n                f\"--force was specified even though the output file {outfile} does not already exist. Continuing\"\n            )\n    else:\n        if Path(outfile).exists():\n            logger.error(\n                f\"Output file {outfile} already exists and force was not specified. Please specify -f or --force to overwrite the output file\"\n            )\n</code></pre>"},{"location":"reference/#src.baktfold.results.tophit.get_tophit","title":"<code>get_tophit(result_tsv, structures, cath=False)</code>","text":"<p>Process Foldseek output to extract top hit and weighted bitscores.</p> <p>Parameters:</p> Name Type Description Default <code>result_tsv</code> <code>Path</code> <p>Path to the Foldseek result TSV file.</p> required <code>structures</code> <code>bool</code> <p>Flag indicating whether structures have been added.</p> required <code>cath</code> <code>bool</code> <p>Flag indicating whether this is for CATH database (all greedy besthits kept not just top)</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[pd.DataFrame, pd.DataFrame]</code> <p>Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing two DataFrames: 1. DataFrame containing the top functions extracted from the Foldseek output. 2. DataFrame containing weighted bitscores for different functions.</p> Source code in <code>src/baktfold/results/tophit.py</code> <pre><code>def get_tophit(\n    result_tsv: Path,\n    structures: bool,\n    cath: bool = False\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Process Foldseek output to extract top hit and weighted bitscores.\n\n    Args:\n        result_tsv (Path): Path to the Foldseek result TSV file.\n        structures (bool): Flag indicating whether structures have been added.\n        cath (bool): Flag indicating whether this is for CATH database (all greedy besthits kept not just top)\n\n    Returns:\n        Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing two DataFrames:\n            1. DataFrame containing the top functions extracted from the Foldseek output.\n            2. DataFrame containing weighted bitscores for different functions.\n    \"\"\"\n\n    logger.info(\"Processing Foldseek output\")\n\n    if structures:\n\n        col_list = [\n            \"query\",\n            \"target\",\n            \"bitscore\",\n            \"fident\",\n            \"evalue\",\n            \"qStart\",\n            \"qEnd\",\n            \"qLen\",\n            \"tStart\",\n            \"tEnd\",\n            \"tLen\",\n            \"alntmscore\",\n            \"lddt\"\n        ]\n    else:\n\n        col_list = [\n            \"query\",\n            \"target\",\n            \"bitscore\",\n            \"fident\",\n            \"evalue\",\n            \"qStart\",\n            \"qEnd\",\n            \"qLen\",\n            \"tStart\",\n            \"tEnd\",\n            \"tLen\",\n        ]\n\n    foldseek_df = pd.read_csv(\n        result_tsv, delimiter=\"\\t\", index_col=False, names=col_list\n    )\n\n\n    # in case the foldseek output is empty\n    if foldseek_df.empty:\n        logger.error(\n            \"Foldseek found no hits whatsoever - please check whether your input\"\n        )\n\n\n    # add qcov and tcov \n    foldseek_df[\"qCov\"] = ((foldseek_df[\"qEnd\"] - foldseek_df[\"qStart\"] ) / foldseek_df[\"qLen\"]).round(2)\n    foldseek_df[\"tCov\"] = ((foldseek_df[\"tEnd\"] - foldseek_df[\"tStart\"] ) / foldseek_df[\"tLen\"]).round(2)\n\n    # reorder\n    qLen_index = foldseek_df.columns.get_loc(\"qLen\")\n    tLen_index = foldseek_df.columns.get_loc(\"tLen\")\n\n    new_column_order = (\n        list(\n            [\n                col\n                for col in foldseek_df.columns[: qLen_index + 1]\n                if col not in [\"qCov\", \"tStart\",\"tEnd\",\t\"tLen\", \"tCov\"]\n            ]\n        )\n        + [\"qCov\", \"tStart\",\"tEnd\",\t\"tLen\", \"tCov\"]\n        + list(\n            [\n                col\n                for col in foldseek_df.columns[tLen_index + 1 :]\n                if col not in [\"qCov\", \"tStart\",\"tEnd\",\t\"tLen\", \"tCov\"]\n            ]\n        )\n    )\n    foldseek_df = foldseek_df.reindex(columns=new_column_order)\n\n\n    if not cath:\n        # get only the tophit - will always be the first hit for each query (top bitscore)\n        foldseek_df = foldseek_df.drop_duplicates(subset=\"query\", keep=\"first\")\n    # otherwise, the df will contain all greedy tophits from CATH\n\n\n    return foldseek_df\n</code></pre>"},{"location":"reference/bakta/","title":"Bakta","text":""},{"location":"reference/bakta/#src.baktfold.bakta.annotation.annotate_aa","title":"<code>annotate_aa(aas)</code>","text":"<p>Combines IPS and PSC annotations and marks hypotheticals.</p> <p>Parameters:</p> Name Type Description Default <code>aas</code> <code>Sequence[dict]</code> <p>A sequence of amino acid dictionaries to annotate.</p> required <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; aas = [{'sequence': 'ATG', 'start': 1, 'stop': 3, 'strand': '+'}]\n&gt;&gt;&gt; annotate_aa(aas)\n&gt;&gt;&gt; aas\n[{'sequence': 'ATG', 'start': 1, 'stop': 3, 'strand': '+'}]\n</code></pre> Source code in <code>src/baktfold/bakta/annotation.py</code> <pre><code>def annotate_aa(aas: Sequence[dict]):\n    \"\"\"\n    Combines IPS and PSC annotations and marks hypotheticals.\n\n    Args:\n      aas (Sequence[dict]): A sequence of amino acid dictionaries to annotate.\n\n    Returns:\n      None\n\n    Examples:\n      &gt;&gt;&gt; aas = [{'sequence': 'ATG', 'start': 1, 'stop': 3, 'strand': '+'}]\n      &gt;&gt;&gt; annotate_aa(aas)\n      &gt;&gt;&gt; aas\n      [{'sequence': 'ATG', 'start': 1, 'stop': 3, 'strand': '+'}]\n    \"\"\"\n\n    print('\\tcombine annotations and mark hypotheticals...')\n\n    for aa in aas:\n        print(aa)\n        combine_annotation(aa)  # combine IPS &amp; PSC annotations and mark hypothetical\n    logger.debug('analyze hypotheticals')\n    hypotheticals = [aa for aa in aas if 'hypothetical' in aa]\n    if(len(hypotheticals) &gt; 0):\n        print(f'\\tanalyze hypothetical proteins: {len(hypotheticals)}')\n        print('\\tcalculated proteins statistics')\n</code></pre>"},{"location":"reference/bakta/#src.baktfold.bakta.annotation.calc_annotation_score","title":"<code>calc_annotation_score(orf)</code>","text":"<p>Calculates the annotation score for a given ORF.</p> <p>Parameters:</p> Name Type Description Default <code>orf</code> <code>dict</code> <p>The ORF to calculate the annotation score for.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The annotation score for the given ORF.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; calc_annotation_score(orf)\n</code></pre> Source code in <code>src/baktfold/bakta/annotation.py</code> <pre><code>def calc_annotation_score(orf:dict) -&gt; int:\n    \"\"\"\n    Calculates the annotation score for a given ORF.\n\n    Args:\n      orf (dict): The ORF to calculate the annotation score for.\n\n    Returns:\n      int: The annotation score for the given ORF.\n\n    Examples:\n      &gt;&gt;&gt; calc_annotation_score(orf)\n    \"\"\"\n    score = 0\n    if(orf.get('gene', None)):\n        score += 1\n    if(orf.get('product', None)):\n        score += 1\n    return score\n</code></pre>"},{"location":"reference/bakta/#src.baktfold.bakta.annotation.combine_annotation","title":"<code>combine_annotation(feature, fast)</code>","text":"<p>Combines annotation information from different sources into a single feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>dict</code> <p>The feature to combine annotation for.</p> required <code>fast</code> <code>bool</code> <p>If True, skips AFDB</p> required <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; combine_annotation(feature)\n</code></pre> Source code in <code>src/baktfold/bakta/annotation.py</code> <pre><code>def combine_annotation(feature: dict, fast: bool):\n    \"\"\"\n    Combines annotation information from different sources into a single feature.\n\n    Args:\n      feature (dict): The feature to combine annotation for.\n      fast (bool): If True, skips AFDB\n    Returns:\n      None\n\n    Examples:\n      &gt;&gt;&gt; combine_annotation(feature)\n    \"\"\"\n\n\n    # ups = feature.get('ups', None)\n    # ips = feature.get('ips', None)\n    # psc = feature.get('psc', None)\n    # pscc = feature.get('pscc', None)\n    pstc = feature.get('pstc', None)\n    # expert_hits = feature.get('expert', [])\n\n    # gene = None\n    # genes = set()\n    # product = None\n\n    product = feature.get('product', None)\n    db_xrefs = feature.get('db_xrefs', [])\n\n    if(pstc):\n\n        # Always normalize pstc to a list\n        if isinstance(pstc, dict):\n            pstc = [pstc]\n        elif isinstance(pstc, str):\n            pstc = [pstc]\n\n        # afdb\n        afdb_entry = None if fast else next(\n            (p for p in pstc if isinstance(p, dict) and p.get('source') == 'afdb'),\n            None\n        )\n        # swissprot\n        swissprot_entry = next((p for p in pstc if isinstance(p, dict) and p.get('source') == 'swissprot'), None)\n        # pdb\n        pdb_entry = next((p for p in pstc if isinstance(p, dict) and p.get('source') == 'pdb'), None)\n        # cath\n        cath_entry = next((p for p in pstc if isinstance(p, dict) and p.get('source') == 'cath'), None)\n        # custom\n        custom_entry = next((p for p in pstc if isinstance(p, dict) and p.get('source') == 'custom_db'), None)\n\n        ####\n        # hierarchy\n        # if it exists, custom is at the top\n        # custom\n        # if not\n        # 1. SwissProt\n        # 2. AFDB\n        # 3. PDB\n        # 4. CATH\n        ####\n\n        if custom_entry:\n            pstc_product = custom_entry['description'] \n        elif swissprot_entry:\n            pstc_product = swissprot_entry['description']\n        elif afdb_entry:\n            pstc_product = afdb_entry['description'] \n        elif pdb_entry:\n            pstc_product = pdb_entry['description'] \n        elif cath_entry:\n            pstc_product = cath_entry['description'] \n        else:\n            pstc_product = None\n\n        if(pstc_product):\n            product = pstc_product\n\n        # Collect all db_xref IDs\n        for entry in pstc:\n            if isinstance(entry, dict):\n                src = entry.get('source', '').lower()\n                eid = entry.get('id')\n                if eid:\n                    if src == 'afdb':\n                        if not fast:\n                            db_xrefs.append(f\"afdb_v6:afdbclusters_{eid}\")\n                    elif src == 'swissprot':\n                        db_xrefs.append(f\"afdb_v6:swissprot_{eid}\")\n                    elif src == 'pdb':\n                        db_xrefs.append(f\"pdb:pdb_{eid}\")\n                    elif src == 'cath':\n                        db_xrefs.append(f\"cath:cath_{eid}\")\n                    elif src == 'custom_db':\n                        db_xrefs.append(f\"custom:custom_{eid}\")\n                    else:\n                        db_xrefs.append(eid)\n            elif isinstance(entry, str):\n                # Preserve any existing string cross-references\n                db_xrefs.append(entry)\n\n        # mark as baktfold\n        mark_as_baktfold(feature)\n\n\n\n\n    # if(len(expert_hits) &gt; 0):\n    #     top_expert_hit = sorted(expert_hits,key=lambda k: (k['rank'], k.get('score', 0), calc_annotation_score(k)), reverse=True)[0]\n    #     expert_genes = top_expert_hit.get('gene', None)\n    #     if(expert_genes):\n    #         expert_genes = expert_genes.replace('/', ',').split(',')\n    #         genes.update(expert_genes)\n    #         gene = expert_genes[0]\n    #     product = top_expert_hit.get('product', None)\n    #     for hit in expert_hits:\n    #         db_xrefs.update(hit.get('db_xrefs', []))\n\n    if product and \"hypothetical protein\" not in product.lower():\n        product = revise_cds_product(product)\n        if(product):\n            if(cfg.compliant):\n                product = insdc.revise_product_insdc(product)\n            feature['product'] = product\n\n            unmark_as_hypothetical(feature)\n\n            # protein_gene_symbol = extract_protein_gene_symbol(product)\n            # if(protein_gene_symbol):\n            #     genes.add(protein_gene_symbol)\n            # revised_genes = revise_cds_gene_symbols(genes)\n            # revised_gene = None\n            # if gene is not None:\n            #     revised_gene = revise_cds_gene_symbols([gene])  # special treatment for selected gene symbol\n            #     revised_gene = revised_gene[0] if len(revised_gene) &gt; 0 else None\n            # if(revised_gene is None  and  len(revised_genes) &gt;= 1):  # select first from gene symbol list if no symbol was selected before\n            #     revised_gene = revised_genes[0]\n\n            # feature['gene'] = revised_gene\n            # feature['genes'] = sorted(revised_genes)\n        else:\n            mark_as_hypothetical(feature)\n    else:\n        mark_as_hypothetical(feature)\n\n    feature['db_xrefs'] = sorted(list(db_xrefs))\n</code></pre>"},{"location":"reference/bakta/#src.baktfold.bakta.annotation.extract_protein_gene_symbol","title":"<code>extract_protein_gene_symbol(product)</code>","text":"<p>Extracts a valid gene symbol from a protein name.</p> <p>Parameters:</p> Name Type Description Default <code>product</code> <code>str</code> <p>The protein name to extract a gene symbol from.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The extracted gene symbol.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; extract_protein_gene_symbol(product)\n</code></pre> Source code in <code>src/baktfold/bakta/annotation.py</code> <pre><code>def extract_protein_gene_symbol(product: str) -&gt; str:\n    \"\"\"\n    Extracts a valid gene symbol from a protein name.\n\n    Args:\n      product (str): The protein name to extract a gene symbol from.\n\n    Returns:\n      str: The extracted gene symbol.\n\n    Examples:\n      &gt;&gt;&gt; extract_protein_gene_symbol(product)\n    \"\"\"\n    gene_symbols = []\n    for part in product.split(' '):  # try to extract valid gene symbols\n        m = RE_GENE_SYMBOL.fullmatch(part)\n        if(m):\n            symbol = m[0]\n            logger.info('fix gene: extract symbol from protein name. symbol=%s', symbol)\n            gene_symbols.append(symbol)\n        else:\n            m = RE_PROTEIN_SYMBOL.fullmatch(part)  # extract protein names\n            if(m):\n                symbol = m[0]\n                symbol = symbol[0].lower() + symbol[1:]\n                logger.info('fix gene: extract symbol from protein name. symbol=%s', symbol)\n                gene_symbols.append(symbol)\n    if(len(gene_symbols) == 0):  # None found\n        return None\n    elif(len(gene_symbols) == 1):  # found 1\n        return gene_symbols[0]\n    else:  # found more than one, take the 2nd as the 1st often describes a broader gene family like \"xyz family trancsriptional regulator ...\"\n        return gene_symbols[1]\n</code></pre>"},{"location":"reference/bakta/#src.baktfold.bakta.annotation.mark_as_baktfold","title":"<code>mark_as_baktfold(feature)</code>","text":"<p>Adds the baktfold key to the given feature dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>dict</code> <p>The feature dictionary to add the baktfold key to.</p> required <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; feature = {'sequence': 'ATG', 'start': 1, 'stop': 3, 'strand': '+'}\n&gt;&gt;&gt; mark_as_baktfold(feature)\n&gt;&gt;&gt; feature\n{'sequence': 'ATG', 'start': 1, 'stop': 3, 'strand': '+', 'baktfold': True}\n</code></pre> Source code in <code>src/baktfold/bakta/annotation.py</code> <pre><code>def mark_as_baktfold(feature: dict):\n    \"\"\"\n    Adds the baktfold key to the given feature dictionary.\n\n    Args:\n      feature (dict): The feature dictionary to add the baktfold key to.\n\n    Returns:\n      None\n\n    Examples:\n      &gt;&gt;&gt; feature = {'sequence': 'ATG', 'start': 1, 'stop': 3, 'strand': '+'}\n      &gt;&gt;&gt; mark_as_baktfold(feature)\n      &gt;&gt;&gt; feature\n      {'sequence': 'ATG', 'start': 1, 'stop': 3, 'strand': '+', 'baktfold': True}\n    \"\"\"\n    # logger.info(\n    #     f'baktfold found hit(s) for: seq={feature['sequence']}, start={feature['start']}, stop={feature['stop']}, strand={feature['strand']}'\n    # )\n    feature['baktfold'] = True\n</code></pre>"},{"location":"reference/bakta/#src.baktfold.bakta.annotation.mark_as_hypothetical","title":"<code>mark_as_hypothetical(feature)</code>","text":"<p>Marks a feature as hypothetical.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>dict</code> <p>The feature to mark as hypothetical.</p> required <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mark_as_hypothetical(feature)\n</code></pre> Source code in <code>src/baktfold/bakta/annotation.py</code> <pre><code>def mark_as_hypothetical(feature: dict):\n    \"\"\"\n    Marks a feature as hypothetical.\n\n    Args:\n      feature (dict): The feature to mark as hypothetical.\n\n    Returns:\n      None\n\n    Examples:\n      &gt;&gt;&gt; mark_as_hypothetical(feature)\n    \"\"\"\n    # no need to actually print this I think\n    # logger.info(\n    #     f'marked as hypothetical: seq={feature['sequence']}, start={feature['start']}, stop={feature['stop']}, strand={feature['strand']}'\n    # )\n    feature['hypothetical'] = True\n    feature['gene'] = None\n    feature['genes'] = []\n    feature['product'] = bc.HYPOTHETICAL_PROTEIN\n</code></pre>"},{"location":"reference/bakta/#src.baktfold.bakta.annotation.revise_cds_gene_symbols","title":"<code>revise_cds_gene_symbols(raw_genes)</code>","text":"<p>Revises a list of gene symbols to ensure they are valid.</p> <p>Parameters:</p> Name Type Description Default <code>raw_genes</code> <code>Sequence[str]</code> <p>The list of gene symbols to revise.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>The revised list of gene symbols.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; revise_cds_gene_symbols(raw_genes)\n</code></pre> Source code in <code>src/baktfold/bakta/annotation.py</code> <pre><code>def revise_cds_gene_symbols(raw_genes: Sequence[str]):\n    \"\"\"\n    Revises a list of gene symbols to ensure they are valid.\n\n    Args:\n      raw_genes (Sequence[str]): The list of gene symbols to revise.\n\n    Returns:\n      list: The revised list of gene symbols.\n\n    Examples:\n      &gt;&gt;&gt; revise_cds_gene_symbols(raw_genes)\n    \"\"\"\n    revised_genes = set()\n    for gene in raw_genes:\n        old_gene = gene\n        if(RE_GENE_SUSPECT_CHARS.search(gene)):  # check for suspect characters -&gt; remove gene symbol\n            logger.info('fix gene: remove gene symbol containing suspect chars. old=%s', old_gene)\n            continue\n\n        old_gene = gene\n        gene = gene.replace('gene', '')\n        if(gene != old_gene):  # remove gene literal\n            logger.info('fix gene: remove gene literal. new=%s, old=%s', gene, old_gene)\n\n        old_gene = gene\n        if(gene[-1] == '-'):  # remove orphan hyphen\n            gene = gene[:-1]\n            logger.info('fix gene: remove orphan hypen. new=%s, old=%s', gene, old_gene)\n\n        old_gene = gene\n        gene = RE_MULTIWHITESPACE.sub(' ', gene).strip()  # revise whitespaces\n        if(gene != old_gene):\n            logger.info('fix gene: revise whitespaces. new=%s, old=%s', gene, old_gene)\n\n        old_gene = gene\n        if(RE_GENE_CAPITALIZED.fullmatch(gene)):\n            gene = gene[0].lower() + gene[1:]\n            logger.info('fix gene: lowercase first char. new=%s, old=%s', gene, old_gene)\n\n        if(len(gene) &gt;= 3):\n            if(len(gene) &lt;= 12):\n                revised_genes.add(gene)\n            else:\n                old_gene = gene\n                gene = extract_protein_gene_symbol(gene)\n                if(gene):\n                    revised_genes.add(gene)\n    return list(revised_genes)\n</code></pre>"},{"location":"reference/bakta/#src.baktfold.bakta.annotation.revise_cds_product","title":"<code>revise_cds_product(product)</code>","text":"<p>Revise product name for INSDC compliant submissions</p> Source code in <code>src/baktfold/bakta/annotation.py</code> <pre><code>def revise_cds_product(product: str):\n    \"\"\"Revise product name for INSDC compliant submissions\"\"\"\n\n    # from gb \n    # grep \"Uncharacterized protein\" AFDBClusters.tsv | wc -l\n    #     805448\n\n    if \"Uncharacterized protein\" in product:\n        old_product = product\n        product = \"hypothetical protein\"\n        if product != old_product:\n            logger.info(f'fix product: renamed uncharacterized protein as hypothetical. new={product}, old={old_product}')\n\n    # from bakta\n\n    old_product = product\n    product = RE_PROTEIN_WEIGHT.sub(' ', product)  # remove protein weight in (k)Da\n    if(product != old_product):\n        logger.info('fix product: remove protein weight in (k)Da. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    product = re.sub(RE_PROTEIN_PERIOD_SEPARATOR, r'\\1-\\2', product)  # replace separator periods\n    if(product != old_product):\n        logger.info('fix product: replace separator periods. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    if(product[0] in RE_PROTEIN_SUSPECT_CHARS_BEGINNING):  # remove suspect first character\n        product = product[1:]\n        logger.info('fix product: replace invalid first character. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    product = RE_PROTEIN_SUSPECT_CHARS_DISCARD.sub('', product)  # remove suspect characters\n    if(product != old_product):\n        logger.info('fix product: replace invalid characters. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    product = RE_PROTEIN_SUSPECT_CHARS_REPLACE.sub(' ', product)  # replace suspect characters by single whitespace\n    if(product != old_product):\n        logger.info('fix product: replace invalid characters. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    product = RE_PROTEIN_WRONG_PRIMES.sub('\\u0027', product)  # replace wrong prime characters with single quote (U+0027) (') according to https://www.ncbi.nlm.nih.gov/genome/doc/internatprot_nomenguide/\n    if(product != old_product):\n        logger.info('fix product: replace wrong prime characters. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    product = product.replace('FOG:', '')  # remove FOG ids\n    if(product != old_product):\n        logger.info('fix product: replace FOG ids. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    product = RE_PROTEIN_REMNANT.sub('', product)  # remove 'Remnant of's\n    if(product != old_product):\n        logger.info('fix product: replace remnant ofs. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    dufs = []  # replace DUF-containing products\n    for m in RE_DOMAIN_OF_UNKNOWN_FUNCTION.finditer(product):\n        dufs.append(m.group(1).upper())\n    if(len(dufs) &gt;= 1):\n        product = f\"{' '.join(dufs)} domain{'s' if len(dufs) &gt; 1 else ''}-containing protein\"\n        if(product != old_product):\n            logger.info('fix product: revise DUF. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    if('conserved' in product.lower()):  # replace conserved UPF proteins\n        upfs = []\n        for m in RE_UNCHARACTERIZED_PROTEIN_FAMILY.finditer(product):\n            upfs.append(m.group(1).upper())\n        if(len(upfs) &gt;= 1):\n            product = f\"{' '.join(upfs)} protein\"\n            if(product != old_product):\n                logger.info('fix product: revise UPF. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    product = RE_PROTEIN_HOMOLOG.sub('-like protein', product)  # replace Homologs\n    if(product != old_product):\n        if(product.count('protein') == 2):\n            product = product.replace('protein', '', 1)  # remove former protein term if existing\n        logger.info('fix product: replace Homolog. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    product = RE_MULTIWHITESPACE.sub(' ', product).strip()  # revise whitespaces\n    if(product != old_product):\n        logger.info('fix product: revise whitespaces. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    product = RE_PROTEIN_PUTATIVE.sub('putative', product)  # replace putative synonyms)\n    if(product != old_product):\n        logger.info('fix product: replace putative synonyms. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    if(RE_PROTEIN_DOMAIN_CONTAINING.search(product)):  # replace domain name underscores in domain names\n        product = product.replace('_', '-')\n        if(product != old_product):\n            logger.info('fix product: replace domain name underscores. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    if(RE_PROTEIN_TMRNA.fullmatch(product)):\n        product = ''\n        logger.info('fix product: discard pure tmRNA product descriptions. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    if(\n        RE_PROTEIN_CONTIG.search(product) or  # protein containing 'sequence'\n        RE_PROTEIN_NODE.search(product) or  # potential contig name (SPAdes)\n        RE_PROTEIN_POTENTIAL_CONTIG_NAME.search(product) or  # potential contig name (SPAdes)\n        RE_PROTEIN_NO_LETTERS.fullmatch(product)  # no letters -&gt; set to Hypothetical\n        ):  # remove suspect products and mark as hypothetical\n        product = None\n        logger.info('remove product: mark proteins with suspect products as hypothetical. old=%s', old_product)\n\n    return product\n</code></pre>"},{"location":"reference/bakta/#src.baktfold.bakta.annotation.unmark_as_hypothetical","title":"<code>unmark_as_hypothetical(feature)</code>","text":"<p>Removes the hypothetical key from the given feature dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>dict</code> <p>The feature dictionary to remove the hypothetical key from.</p> required <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; feature = {'sequence': 'ATG', 'start': 1, 'stop': 3, 'strand': '+'}\n&gt;&gt;&gt; unmark_as_hypothetical(feature)\n&gt;&gt;&gt; feature\n{'sequence': 'ATG', 'start': 1, 'stop': 3, 'strand': '+'}\n</code></pre> Source code in <code>src/baktfold/bakta/annotation.py</code> <pre><code>def unmark_as_hypothetical(feature: dict):\n    \"\"\"\n    Removes the hypothetical key from the given feature dictionary.\n\n    Args:\n      feature (dict): The feature dictionary to remove the hypothetical key from.\n\n    Returns:\n      None\n\n    Examples:\n      &gt;&gt;&gt; feature = {'sequence': 'ATG', 'start': 1, 'stop': 3, 'strand': '+'}\n      &gt;&gt;&gt; unmark_as_hypothetical(feature)\n      &gt;&gt;&gt; feature\n      {'sequence': 'ATG', 'start': 1, 'stop': 3, 'strand': '+'}\n    \"\"\"\n    # logger.info(\n    #     f'unmarked as hypothetical: seq={feature['sequence']}, start={feature['start']}, stop={feature['stop']}, strand={feature['strand']}'\n    # )\n    feature.pop('hypothetical', None)  # remove completely\n</code></pre>"},{"location":"reference/bakta/#src.baktfold.bakta.config.check_content_size","title":"<code>check_content_size(file_name, file_path)</code>","text":"<p>Checks if a file is empty.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the file to check.</p> required <code>file_path</code> <code>Path</code> <p>The path to the file to check.</p> required <p>Returns:</p> Type Description <p>None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; check_content_size('file', Path('path/to/file'))\nNone\n</code></pre> Source code in <code>src/baktfold/bakta/config.py</code> <pre><code>def check_content_size(file_name: str, file_path: Path):\n    \"\"\"\n    Checks if a file is empty.\n\n    Args:\n      file_name (str): The name of the file to check.\n      file_path (Path): The path to the file to check.\n\n    Returns:\n      None.\n\n    Examples:\n      &gt;&gt;&gt; check_content_size('file', Path('path/to/file'))\n      None\n    \"\"\"\n    if(file_path.stat().st_size == 0):\n        log.error('empty %s file! path=%s', file_name, file_path)\n        sys.exit(f'ERROR: {file_name} file ({file_path}) is empty!')\n</code></pre>"},{"location":"reference/bakta/#src.baktfold.bakta.config.check_db_path","title":"<code>check_db_path(args)</code>","text":"<p>Checks the path to the database.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments passed to the program.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>The path to the database.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; check_db_path(args)\nPath('path/to/db')\n</code></pre> Source code in <code>src/baktfold/bakta/config.py</code> <pre><code>def check_db_path(args: Namespace) -&gt; Path:\n    \"\"\"\n    Checks the path to the database.\n\n    Args:\n      args (Namespace): The arguments passed to the program.\n\n    Returns:\n      Path: The path to the database.\n\n    Examples:\n      &gt;&gt;&gt; check_db_path(args)\n      Path('path/to/db')\n    \"\"\"\n    global db_path\n    env = os.environ.copy()\n    if(args.db):\n        db_dir = args.db\n        log.debug('test parameter db: db_tmp=%s', db_dir)\n        try:\n            db_tmp_path = Path(db_dir).resolve()\n            if(db_tmp_path.is_dir()):\n                db_path = db_tmp_path\n                log.info('database: type=parameter, path=%s', db_path)\n            else:\n                log.error('unvalid database path: type=parameter, path=%s', db_tmp_path)\n                raise IOError()\n        except:\n            sys.exit(f'ERROR: wrong database path! --db={db_dir}')\n    elif('BAKTA_DB' in env):\n        db_dir = env['BAKTA_DB']\n        log.debug('test env db: db_tmp=%s', db_dir)\n        try:\n            db_tmp_path = Path(db_dir).resolve()\n            if(db_tmp_path.is_dir()):\n                db_path = db_tmp_path\n                log.info('database: type=environment, path=%s', db_path)\n            else:\n                log.error('unvalid database path: type=environment, path=%s', db_tmp_path)\n                raise IOError()\n        except:\n            sys.exit(f'ERROR: wrong database path! BAKTA_DB={db_dir}')\n    else:\n        base_dir = Path(__file__).parent\n        db_tmp_path = base_dir.joinpath('db')\n        log.debug('test base_dir db: db_tmp=%s', db_tmp_path)\n        if(db_tmp_path.is_dir()):\n            db_path = db_tmp_path\n            log.info('database: type=base-dir, path=%s', db_path)\n        else:\n            log.error('unvalid database path: type=base-dir, path=%s', db_tmp_path)\n            sys.exit('ERROR: database neither provided nor auto-detected!\\nPlease, download the mandatory db and provide it via either the --db parameter, a BAKTA_DB environment variable or copy it into the Bakta base directory.\\nFor further information please read the readme.md')\n    return db_path\n</code></pre>"},{"location":"reference/bakta/#src.baktfold.bakta.config.check_output_path","title":"<code>check_output_path(output, force_override)</code>","text":"<p>Check provided output path</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>string</code> <p>The output directory destination path</p> required <code>force_override</code> <code>Bool</code> <p>Whether to override existing output directories</p> required Source code in <code>src/baktfold/bakta/config.py</code> <pre><code>def check_output_path(output: str, force_override: bool) -&gt; Path:\n    \"\"\"Check provided output path\n    Args:\n        output (string): The output directory destination path\n        force_override (Bool): Whether to override existing output directories\n    \"\"\"\n    global output_path\n    output_path = Path(output)\n    if(not output_path.exists()):\n        try:\n            output_path.mkdir(parents=True, exist_ok=True)\n        except:\n            sys.exit(f'ERROR: could not resolve or create output directory ({output})!')\n    else:\n        if(output_path == Path(os.getcwd())):\n            pass\n        elif(force_override is False):\n            sys.exit(f'ERROR: output path ({output_path}) already exists! Either provide a non-existent new path or force overwriting it via \\'--force\\'')\n        elif(not os.access(str(output_path), os.X_OK)):\n            sys.exit(f'ERROR: output path ({output_path}) not accessible!')\n        elif(not os.access(str(output_path), os.W_OK)):\n            sys.exit(f'ERROR: output path ({output_path}) not writable!')\n    output_path = output_path.resolve()\n    return output_path\n</code></pre>"},{"location":"reference/bakta/#src.baktfold.bakta.config.check_readability","title":"<code>check_readability(file_name, file_Path)</code>","text":"<p>Checks if a file is readable.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the file to check.</p> required <code>file_Path</code> <code>Path</code> <p>The path to the file to check.</p> required <p>Returns:</p> Type Description <p>None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; check_readability('file', Path('path/to/file'))\nNone\n</code></pre> Source code in <code>src/baktfold/bakta/config.py</code> <pre><code>def check_readability(file_name: str, file_Path: Path):\n    \"\"\"\n    Checks if a file is readable.\n\n    Args:\n      file_name (str): The name of the file to check.\n      file_Path (Path): The path to the file to check.\n\n    Returns:\n      None.\n\n    Examples:\n      &gt;&gt;&gt; check_readability('file', Path('path/to/file'))\n      None\n    \"\"\"\n    if(not os.access(str(file_Path), os.R_OK)):\n        log.error('%s file not readable! path=%s', file_name, file_Path)\n        sys.exit(f'ERROR: {file_name} file ({file_Path}) not readable!')\n</code></pre>"},{"location":"reference/bakta/#src.baktfold.bakta.config.check_threads","title":"<code>check_threads(args)</code>","text":"<p>Checks the number of threads to use.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments passed to the program.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of threads to use.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; check_threads(args)\n4\n</code></pre> Source code in <code>src/baktfold/bakta/config.py</code> <pre><code>def check_threads(args: Namespace) -&gt; int:\n    \"\"\"\n    Checks the number of threads to use.\n\n    Args:\n      args (Namespace): The arguments passed to the program.\n\n    Returns:\n      int: The number of threads to use.\n\n    Examples:\n      &gt;&gt;&gt; check_threads(args)\n      4\n    \"\"\"\n    global threads\n    threads = args.threads\n\n    try:\n        max_threads = len(os.sched_getaffinity(0))\n        log.debug(f\"max-threads={max_threads}\")\n    except AttributeError:\n        max_threads = mp.cpu_count()\n        log.debug(f\"scheduler affinity not availabe! max-threads={max_threads}\")\n\n    if(threads == 0):\n        threads = max_threads\n        log.debug(\"request max threads.\")\n    elif(threads &lt; 0):\n        log.error(\"wrong argument for 'threads' parameter! threads=%i\", threads)\n        sys.exit(f\"ERROR: wrong argument ({threads}) for 'threads' parameter! Value must be larger than/equal to 0.\")\n    elif(threads &gt; max_threads):\n        log.error(\"wrong argument for 'threads' parameter! More threads requested than available: requested=%i, available=%i\", threads, max_threads)\n        sys.exit(f\"ERROR: wrong argument ({threads}) for 'threads' parameter! More threads requested ({threads}) than available ({max_threads}).\")\n    log.info('threads=%i', threads)\n    return threads\n</code></pre>"},{"location":"reference/bakta/#src.baktfold.bakta.config.check_tmp_path","title":"<code>check_tmp_path(args)</code>","text":"<p>Checks the path to the temporary directory.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments passed to the program.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>The path to the temporary directory.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; check_tmp_path(args)\nPath('path/to/tmp_dir')\n</code></pre> Source code in <code>src/baktfold/bakta/config.py</code> <pre><code>def check_tmp_path(args: Namespace) -&gt; Path:\n    \"\"\"\n    Checks the path to the temporary directory.\n\n    Args:\n      args (Namespace): The arguments passed to the program.\n\n    Returns:\n      Path: The path to the temporary directory.\n\n    Examples:\n      &gt;&gt;&gt; check_tmp_path(args)\n      Path('path/to/tmp_dir')\n    \"\"\"\n    global tmp_path\n    if(args.tmp_dir is not None):\n        tmp_path = Path(args.tmp_dir)\n        if(not tmp_path.exists()):\n            log.debug('dedicated temp dir does not exist! tmp-dir=%s', tmp_path)\n            sys.exit(f'ERROR: dedicated temporary directory ({tmp_path}) does not exist!')\n        else:\n            log.info('use dedicated temp dir: path=%s', tmp_path)\n            tmp_path = Path(tempfile.mkdtemp(dir=str(tmp_path))).resolve()\n    else:\n        tmp_path = Path(tempfile.mkdtemp()).resolve()\n    log.info('tmp-path=%s', tmp_path)\n    return tmp_path\n</code></pre>"},{"location":"reference/bakta/#src.baktfold.bakta.config.check_user_proteins","title":"<code>check_user_proteins(args)</code>","text":"<p>Checks the path to the user proteins file.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments passed to the program.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <p>The path to the user proteins file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; check_user_proteins(args)\nPath('path/to/user_proteins')\n</code></pre> Source code in <code>src/baktfold/bakta/config.py</code> <pre><code>def check_user_proteins(args: Namespace):\n    \"\"\"\n    Checks the path to the user proteins file.\n\n    Args:\n      args (Namespace): The arguments passed to the program.\n\n    Returns:\n      Path: The path to the user proteins file.\n\n    Examples:\n      &gt;&gt;&gt; check_user_proteins(args)\n      Path('path/to/user_proteins')\n    \"\"\"\n    global user_proteins\n    user_proteins = args.proteins\n    if(user_proteins is not None):\n        try:\n            if(user_proteins == ''):\n                raise ValueError('File path argument must be non-empty')\n            user_proteins_path = Path(args.proteins).resolve()\n            check_readability('user proteins', user_proteins_path)\n            check_content_size('user proteins', user_proteins_path)\n            user_proteins = user_proteins_path\n            log.info('user-proteins=%s', user_proteins)\n            return user_proteins\n        except:\n            log.error('provided user proteins file not valid! path=%s', user_proteins)\n            sys.exit(f'ERROR: user proteins file ({user_proteins}) not valid!')\n    else:\n        return None\n</code></pre>"},{"location":"reference/bakta/#src.baktfold.bakta.config.setup","title":"<code>setup(args)</code>","text":"<p>Test environment and build a runtime configuration.</p> Source code in <code>src/baktfold/bakta/config.py</code> <pre><code>def setup(args):\n    \"\"\"Test environment and build a runtime configuration.\"\"\"\n    # runtime configurations\n    global env, threads, verbose, debug\n    env['BLAST_USAGE_REPORT'] = 'false'  # prevent BLAST from contacting NCBI\n\n    threads = check_threads(args)\n    verbose = args.verbose\n    log.info('verbose=%s', verbose)\n    debug = args.debug\n    log.info('debug=%s', debug)\n    if(debug):\n        verbose = True\n\n    # input / output path configurations\n    global db_path, db_info, tmp_path, genome_path, min_sequence_length, prefix, output_path, force\n    db_path = check_db_path(args)\n    tmp_path = check_tmp_path(args)\n\n    try:\n        if(args.genome == ''):\n            raise ValueError('File path argument must be non-empty')\n        genome_path = Path(args.genome).resolve()\n        check_readability('genome', genome_path)\n        check_content_size('genome', genome_path)\n    except:\n        log.error('provided genome file not valid! path=%s', args.genome)\n        sys.exit(f'ERROR: genome file ({args.genome}) not valid!')\n    log.info('genome-path=%s', genome_path)\n\n    # input / output configurations\n    min_sequence_length = args.min_contig_length\n    if(min_sequence_length &lt;= 0):\n        log.error(\"wrong argument for 'min-contig-length' parameter! min_contig_length=%s\", min_sequence_length)\n        sys.exit(f\"ERROR: wrong argument ({min_sequence_length}) for 'min- contig-length' parameter! Value must be larger than 0\")\n    log.info('min_contig_length=%s', min_sequence_length)\n    log.info('prefix=%s', prefix)  # set in main.py before global logger config\n    log.info('output-path=%s', output_path)\n    force = args.force\n    log.info('force=%s', force)\n\n    # organism configurations\n    global genus, species, strain, plasmid, taxon\n    genus = args.genus\n    if(genus is not None):\n        genus = genus.strip()\n        if(genus == ''):\n            log.error(\"Empty 'genus' parameter! genus=%s\", genus)\n            sys.exit(f\"ERROR: empty 'genus' parameter!\")\n        else:\n            genus = genus.capitalize()\n    log.info('genus=%s', genus)\n    species = args.species\n    if(species is not None):\n        species = species.strip()\n        if(species == ''):\n            log.error(\"Empty 'species' parameter! species=%s\", species)\n            sys.exit(f\"ERROR: empty 'species' parameter!\")\n        else:\n            species = species.lower()\n    log.info('species=%s', species)\n    strain = args.strain\n    if(strain is not None):\n        strain = strain.strip()\n        if(strain == ''):\n            log.error(\"Empty 'strain' parameter! strain=%s\", species)\n            sys.exit(f\"ERROR: empty 'strain' parameter!\")\n    log.info('strain=%s', strain)\n    plasmid = args.plasmid\n    if(plasmid is not None):\n        plasmid = plasmid.strip()\n        if(plasmid == ''):\n            log.error(\"Empty 'plasmid' parameter! plasmid=%s\", plasmid)\n            sys.exit(f\"ERROR: empty 'plasmid' parameter!\")\n        elif('plasmid' in plasmid.lower()):\n            log.error(\"Wrong 'plasmid' parameter! plasmid=%s\", plasmid)\n            sys.exit(f\"ERROR: wrong 'plasmid' parameter! The plasmid name mustn't contain the word 'plasmid'.\")\n        elif(PLASMID_NAME_PATTERN.fullmatch(plasmid) is None and PLASMID_UNNAMED_PATTERN.fullmatch(plasmid) is None):\n            log.error(\"Wrong 'plasmid' name! plasmid=%s\", plasmid)\n            sys.exit(f\"ERROR: wrong 'plasmid' name! Plasmid names must either be named as 'unnamed', 'unnamed1', ... or start with a lower 'p', contain only digits, dots, underscores and letters, and are limited to 20 characters in total.\")\n    log.info('plasmid=%s', plasmid)\n    taxon = ' '.join([t for t in [genus, species, strain] if t is not None])\n    if(taxon == ''):\n        taxon = None\n\n    # annotation configurations\n    global complete, prodigal_tf, translation_table, keep_sequence_headers, locus, locus_tag, locus_tag_increment, gram, replicons, compliant, user_proteins, user_hmms, meta, regions\n    complete = args.complete\n    log.info('complete=%s', complete)\n    prodigal_tf = args.prodigal_tf\n    if(prodigal_tf is not None):\n        try:\n            if(prodigal_tf == ''):\n                raise ValueError('File path argument must be non-empty')\n            prodigal_tf_path = Path(args.prodigal_tf).resolve()\n            check_readability('prodigal training', prodigal_tf_path)\n            check_content_size('prodigal training', prodigal_tf_path)\n            prodigal_tf = prodigal_tf_path\n        except:\n            log.error('provided prodigal training file not valid! path=%s', prodigal_tf)\n            sys.exit(f'ERROR: Prodigal training file ({prodigal_tf}) not valid!')\n    log.info('prodigal_tf=%s', prodigal_tf)\n    translation_table = args.translation_table\n    log.info('translation_table=%s', translation_table)\n    gram = args.gram\n    log.info('gram=%s', gram)\n    compliant = args.compliant\n    log.info('compliant=%s', compliant)\n    if(compliant):\n        min_sequence_length = 200\n        log.info('compliant mode! min_contig_length=%s', min_sequence_length)\n    meta = args.meta\n    log.info('meta=%s', meta)\n    locus = args.locus\n    if(locus is not None):\n        if(locus == ''):\n            log.error(\"Empty 'locus' parameter! locus=%s\", locus)\n            sys.exit(f\"ERROR: empty 'locus' parameter!\")\n        if(' ' in locus):\n            log.error(\"Whitespace character in 'locus' parameter! locus=%s\", locus)\n            sys.exit(f\"ERROR: whitespace character ({locus}) in 'locus' parameter!\")\n        if(bc.RE_INSDC_ID_PREFIX.fullmatch(locus) is None):\n            log.error(\"Invalid 'locus' parameter! locus=%s\", locus)\n            sys.exit(f\"ERROR: invalid 'locus' parameter ({locus})!\\nLocus prefixes must contain between 1 and 20 alphanumeric or '-_' characters.\")\n    log.info('locus=%s', locus)\n    locus_tag = args.locus_tag\n    if(locus_tag is not None):\n        if(locus_tag == ''):\n            log.error(\"Empty 'locus-tag' parameter! locus=%s\", locus_tag)\n            sys.exit(f\"ERROR: empty 'locus-tag' parameter!\")\n        if(' ' in locus_tag):\n            log.error(\"Whitespace character in 'locus-tag' parameter! locus-tag=%s\", locus_tag)\n            sys.exit(f\"ERROR: whitespace character ({locus_tag}) in 'locus-tag' parameter!\")\n        if(compliant):\n            if(bc.RE_INSDC_LOCUSTAG_PREFIX.fullmatch(locus_tag) is None):\n                log.error(\"INSDC-incompliant 'locus-tag' parameter! locus-tag=%s\", locus_tag)\n                sys.exit(f\"ERROR: INSDC-incompliant 'locus-tag' parameter ({locus_tag})!\\nINSDC Locus tag prefixes must contain between 3 and 12 alphanumeric uppercase characters and start with a letter.\")\n        else:\n            if(bc.RE_LOCUSTAG_PREFIX.fullmatch(locus_tag) is None):\n                log.error(\"Invalid 'locus-tag' parameter! locus-tag=%s\", locus_tag)\n                sys.exit(f\"ERROR: invalid 'locus-tag' parameter ({locus_tag})!\\nLocus tag prefixes must contain between 1 and 24 alphanumeric characters or '_.-' signs.\")\n    log.info('locus-tag=%s', locus_tag)\n    locus_tag_increment = args.locus_tag_increment\n    log.info('locus-tag-increment=%s', locus_tag_increment)\n    keep_sequence_headers = args.keep_contig_headers\n    log.info('keep_contig_headers=%s', keep_sequence_headers)\n    replicons = args.replicons\n    if(replicons is not None):\n        try:\n            if(replicons == ''):\n                raise ValueError('File path argument must be non-empty')\n            replicon_table_path = Path(args.replicons).resolve()\n            check_readability('replicon table', replicon_table_path)\n            check_content_size('replicon table', replicon_table_path)\n            replicons = replicon_table_path\n        except:\n            log.error('provided replicon file not valid! path=%s', replicons)\n            sys.exit(f'ERROR: replicon table file ({replicons}) not valid!')\n    log.info('replicon-table=%s', replicons)\n    user_proteins = check_user_proteins(args)\n    user_hmms = args.hmms\n    if(user_hmms is not None):\n        try:\n            if(user_hmms == ''):\n                raise ValueError('File path argument must be non-empty')\n            user_hmms_path = Path(user_hmms).resolve()\n            check_readability('HMM', user_hmms_path)\n            check_content_size('HMM', user_hmms_path)\n            user_hmms = user_hmms_path\n        except:\n            log.error('provided HMM file not valid! path=%s', user_hmms)\n            sys.exit(f'ERROR: HMM file ({user_hmms}) not valid!')\n\n    regions = args.regions\n    if(regions is not None):\n        try:\n            if(regions == ''):\n                raise ValueError('File path argument must be non-empty')\n            regions_path = Path(args.regions).resolve()\n            check_readability('regions', regions_path)\n            check_content_size('regions', regions_path)\n            regions = regions_path\n        except:\n            log.error('provided regions file not valid! path=%s', regions)\n            sys.exit(f'ERROR: regions file ({regions}) not valid!')\n    log.info('regions=%s', regions)\n\n\n    # workflow configurations\n    global skip_trna, skip_tmrna, skip_rrna, skip_ncrna, skip_ncrna_region, skip_crispr, skip_cds, skip_pseudo, skip_sorf, skip_gap, skip_ori, skip_filter, skip_plot\n    skip_trna = args.skip_trna\n    log.info('skip-tRNA=%s', skip_trna)\n    skip_tmrna = args.skip_tmrna\n    log.info('skip-tmRNA=%s', skip_tmrna)\n    skip_rrna = args.skip_rrna\n    log.info('skip-rRNA=%s', skip_rrna)\n    skip_ncrna = args.skip_ncrna\n    log.info('skip-ncRNA=%s', skip_ncrna)\n    skip_ncrna_region = args.skip_ncrna_region\n    log.info('skip-ncRNA-region=%s', skip_ncrna_region)\n    skip_crispr = args.skip_crispr\n    log.info('skip-CRISPR=%s', skip_crispr)\n    skip_cds = args.skip_cds\n    log.info('skip-CDS=%s', skip_cds)\n    skip_pseudo = args.skip_pseudo\n    log.info('skip-pseudo=%s', skip_pseudo)\n    skip_sorf = args.skip_sorf\n    log.info('skip-sORF=%s', skip_sorf)\n    skip_gap = args.skip_gap\n    log.info('skip-gap=%s', skip_gap)\n    skip_ori = args.skip_ori\n    log.info('skip-ori=%s', skip_ori)\n    skip_filter = args.skip_filter\n    log.info('skip-filter=%s', skip_filter)\n    skip_plot = args.skip_plot\n    log.info('skip-plot=%s', skip_plot)\n</code></pre>"},{"location":"reference/bakta/#src.baktfold.bakta.pstc.fetch_db_pscc_result","title":"<code>fetch_db_pscc_result(conn, uniref50_id)</code>","text":"<p>Fetches the PSCC result for a given uniref50_id from a sqlite3 database.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <code>sqlite3.Connection</code> <p>The connection to the sqlite3 database.</p> required <code>uniref50_id</code> <code>str</code> <p>The uniref50_id to fetch the PSCC result for.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>The PSCC result for the given uniref50_id.</p> Source code in <code>src/baktfold/bakta/pstc.py</code> <pre><code>def fetch_db_pscc_result(conn: sqlite3.Connection, uniref50_id: str):\n    \"\"\"\n    Fetches the PSCC result for a given uniref50_id from a sqlite3 database.\n\n    Args:\n      conn (sqlite3.Connection): The connection to the sqlite3 database.\n      uniref50_id (str): The uniref50_id to fetch the PSCC result for.\n\n    Returns:\n      tuple: The PSCC result for the given uniref50_id.\n    \"\"\"\n    c = conn.cursor()\n    c.execute('select * from pscc where uniref50_id=?', (uniref50_id,))\n    rec = c.fetchone()\n    c.close()\n    return rec\n</code></pre>"},{"location":"reference/bakta/#src.baktfold.bakta.pstc.fetch_sql_description","title":"<code>fetch_sql_description(conn, source, accession)</code>","text":"<p>Fetches the product description for a given source and accession from a sqlite3 database.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <code>sqlite3.Connection</code> <p>The connection to the sqlite3 database.</p> required <code>source</code> <code>str</code> <p>The source of the accession.</p> required <code>accession</code> <code>str</code> <p>The accession to fetch the description for.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The product description for the given source and accession.</p> Source code in <code>src/baktfold/bakta/pstc.py</code> <pre><code>def fetch_sql_description(conn, source, accession):\n    \"\"\"\n    Fetches the product description for a given source and accession from a sqlite3 database.\n\n    Args:\n      conn (sqlite3.Connection): The connection to the sqlite3 database.\n      source (str): The source of the accession.\n      accession (str): The accession to fetch the description for.\n\n    Returns:\n      str: The product description for the given source and accession.\n    \"\"\"\n    table_map = {\n        'swissprot': 'swissprot',\n        'afdb': 'afdbclusters',\n        'pdb': 'pdb',\n        'cath': 'cath',\n    }\n\n    table = table_map.get(source)\n    if table is None:\n        return None\n\n    # special case for cath, which can have multiple top hits (greedy) - multidomain proteins\n    if table == 'cath':\n        cursor = conn.execute(\"SELECT product FROM cath WHERE id = ?\", (accession,))\n    else:\n        cursor = conn.execute(f\"SELECT product FROM {table} WHERE id = ?\", (accession,))\n\n    row = cursor.fetchone()\n    return row[0] if row else None\n</code></pre>"},{"location":"reference/bakta/#src.baktfold.bakta.pstc.fetch_sql_description_threadsafe","title":"<code>fetch_sql_description_threadsafe(db_path, source, accession)</code>","text":"<p>makes new connection every time so don't have 2 CATH accessions colliding (for multi domain proteins)</p> Source code in <code>src/baktfold/bakta/pstc.py</code> <pre><code>def fetch_sql_description_threadsafe(db_path, source, accession):\n    \"\"\"\n    makes new connection every time so don't have 2 CATH accessions colliding (for multi domain proteins)\n    \"\"\"\n    import sqlite3\n    conn = sqlite3.connect(db_path, uri=True, check_same_thread=False)\n    try:\n        result = fetch_sql_description(conn, source, accession)\n    finally:\n        conn.close()\n    return result\n</code></pre>"},{"location":"reference/bakta/#src.baktfold.bakta.pstc.lookup_custom","title":"<code>lookup_custom(features, baktfold_db, custom_annotations)</code>","text":"<p>Lookup PSTC information from custom db</p> Source code in <code>src/baktfold/bakta/pstc.py</code> <pre><code>def lookup_custom(features: Sequence[dict], baktfold_db: Path, custom_annotations: Path):\n    \"\"\"Lookup PSTC information from custom db \"\"\"\n    no_pstc_lookups = 0\n\n    # custom\n    if custom_annotations:\n        custom_dict = {}\n        with open(f\"{custom_annotations}\", \"r\") as f:\n            reader = csv.reader(f, delimiter=\"\\t\")\n            for row in reader:\n                if len(row) &gt;= 2:\n                    custom_dict[row[0]] = row[1]\n\n    for feat in features:\n        pstc = feat.get('pstc')\n        if not pstc:\n            continue\n\n        # Normalize to list for consistent handling\n        pstc_entries = pstc if isinstance(pstc, list) else [pstc]\n\n        for entry in pstc_entries:\n            accession = entry.get('id')\n            source = entry.get('source')\n            if source == 'custom_db':\n                if accession in custom_dict:\n                    entry['description'] = custom_dict[accession]\n                else:\n                    entry['description'] = accession # mark as accession if no annotation given for custom for now\n\n        # Write back normalized list or single entry\n        feat['pstc'] = pstc_entries if isinstance(pstc, list) else pstc_entries[0]\n\n    return features\n</code></pre>"},{"location":"reference/bakta/#src.baktfold.bakta.pstc.lookup_sql","title":"<code>lookup_sql(features, baktfold_db, threads)</code>","text":"<p>Lookup PSTC information</p> Source code in <code>src/baktfold/bakta/pstc.py</code> <pre><code>def lookup_sql(features: Sequence[dict], baktfold_db: Path, threads: int):\n    \"\"\"Lookup PSTC information\"\"\"\n\n    no_pstc_lookups = 0\n    # try:\n    rec_futures = []\n    logger.info(\"Looking up PSTC descriptions\")\n    # with sqlite3.connect(f\"file:{baktfold_db.joinpath('baktfold.db')}?mode=ro&amp;nolock=1&amp;cache=shared\", uri=True, check_same_thread=False) as conn:\n    #     conn.execute('PRAGMA omit_readlock;')\n    #     conn.row_factory = sqlite3.Row\n    with ThreadPoolExecutor(max_workers=max(10, threads)) as tpe:  # use min 10 threads for IO bound non-CPU lookups\n        for feat in features:\n            pstc = feat.get('pstc')\n            if not pstc:\n                continue\n\n            # Normalize to list for consistent handling\n            pstc_entries = pstc if isinstance(pstc, list) else [pstc]\n\n            rec_futures = []\n            for entry in pstc_entries:\n                accession = entry.get('id')\n\n                source = entry.get('source')\n\n                # submit database query as a future\n                future = tpe.submit(fetch_sql_description_threadsafe, baktfold_db.joinpath('baktfold.db'), source, accession)\n                rec_futures.append((entry, future))\n\n\n            # Collect results\n            for entry, future in rec_futures:\n                desc = future.result()\n                if desc:\n                    entry['description'] = desc\n                else:\n                    if entry.get('source') == 'custom_db':\n                        entry['description'] = accession  # keep accession if custom_db but missing\n                    else:\n                        entry['description'] = \"hypothetical protein\"\n\n        # Write back normalized list or single entry\n        feat['pstc'] = pstc_entries if isinstance(pstc, list) else pstc_entries[0]\n\n    # except Exception as ex:\n    #     logger.error('Could not read PSTCs from db!')\n    #     raise Exception('SQL error!', ex)\n    # log.info('looked-up=%i', no_pstc_lookups)\n\n    return features\n</code></pre>"},{"location":"reference/bakta/#src.baktfold.bakta.pstc.parse","title":"<code>parse(features, foldseek_df, db_name='swissprot', has_duplicate_locus=False)</code>","text":"<p>Update CDS in place with PSTC hits from foldseek_df if they pass filters.</p> <p>has_duplicate_locus - some euks have multiple CDS per locus tag</p> Source code in <code>src/baktfold/bakta/pstc.py</code> <pre><code>def parse(features: Sequence[dict], foldseek_df: pd.DataFrame, db_name: str = 'swissprot', has_duplicate_locus: bool = False) -&gt; None:\n    \"\"\"Update CDS in place with PSTC hits from foldseek_df if they pass filters.\n\n    has_duplicate_locus - some euks have multiple CDS per locus tag\n\n    \"\"\"\n\n    # Convert foldseek_df to a lookup table keyed by query ID\n    foldseek_hits = {row['query']: row for _, row in foldseek_df.iterrows()}\n\n    # each query maps to a list of rows now (to handle multiple CATH greedy tophits for multidomain proteins)\n    foldseek_hits = defaultdict(list)\n    for _, row in foldseek_df.iterrows():\n        foldseek_hits[row['query']].append(row)\n\n    updated_count = 0\n\n\n    for cds in features:\n        if has_duplicate_locus:\n            aa_identifier = cds.get('id')\n        else:\n            aa_identifier = cds.get('locus')\n\n        if aa_identifier not in foldseek_hits:\n            continue  # no hits, skip\n\n        cds_updated = False  \n\n        # Iterate over *all* hits for this query\n        for row in foldseek_hits[aa_identifier]:\n            query_cov = float(row['qCov'])\n            subject_cov = float(row['tCov'])\n            identity = float(row['fident'])\n            evalue = float(row['evalue'])\n            bitscore = float(row['bitscore'])\n            target_id = row['target']\n\n            # Extract accession depending on database\n            if db_name in {\"swissprot\", \"afdb\"}:\n                accession = target_id.split('-')[1]\n            elif db_name == \"pdb\":\n                accession = target_id.split('-')[0]\n            else:  # cath and custom\n                accession = target_id\n\n            # Apply your filters\n            if (\n                query_cov &gt;= bc.MIN_PSTC_QCOVERAGE\n                and subject_cov &gt;= bc.MIN_PSTC_TCOVERAGE\n                and identity &gt;= bc.MIN_PSTC_IDENTITY\n            ):\n                new_pstc = {\n                    'source': db_name,\n                    'id': accession,\n                    'query_cov': query_cov,\n                    'subject_cov': subject_cov,\n                    'identity': identity,\n                    'score': bitscore,\n                    'evalue': evalue,\n                }\n\n                # Append or initialize 'pstc'\n                if 'pstc' in cds:\n                    if isinstance(cds['pstc'], dict):\n                        cds['pstc'] = [cds['pstc'], new_pstc]\n                    elif isinstance(cds['pstc'], list):\n                        cds['pstc'].append(new_pstc)\n                    else:\n                        cds['pstc'] = [new_pstc]\n                else:\n                    cds['pstc'] = [new_pstc]  # \u2190 ensure list, since we may have many hits\n\n\n                cds_updated = True  \n\n        # Increment only once per CDS that had at least one valid hit (CATH might have multiple)\n        if cds_updated:\n            updated_count += 1\n\n    logger.info(f\"PSTC for {db_name} updated in place for {updated_count} CDSs\")\n    return features\n</code></pre>"},{"location":"reference/databases/","title":"Databases","text":""},{"location":"reference/databases/#src.baktfold.databases.db.calc_md5_sum","title":"<code>calc_md5_sum(tarball_path, buffer_size=1024 * 1024)</code>","text":"<p>Calculate the MD5 checksum of the given file.</p> <p>Parameters:</p> Name Type Description Default <code>tarball_path</code> <code>Path</code> <p>The path to the file for which the MD5 checksum needs to be calculated.</p> required <code>buffer_size</code> <code>int</code> <p>The buffer size for reading the file.</p> <code>1024 * 1024</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The MD5 checksum of the file.</p> Source code in <code>src/baktfold/databases/db.py</code> <pre><code>def calc_md5_sum(tarball_path: Path, buffer_size: int = 1024 * 1024) -&gt; str:\n    \"\"\"\n    Calculate the MD5 checksum of the given file.\n\n    Args:\n        tarball_path (Path): The path to the file for which the MD5 checksum needs to be calculated.\n        buffer_size (int): The buffer size for reading the file.\n\n    Returns:\n        str: The MD5 checksum of the file.\n    \"\"\"\n\n    md5 = hashlib.md5()\n    with tarball_path.open(\"rb\") as fh:\n        data = fh.read(buffer_size)\n        while data:\n            md5.update(data)\n            data = fh.read(buffer_size)\n    return md5.hexdigest()\n</code></pre>"},{"location":"reference/databases/#src.baktfold.databases.db.check_db_installation","title":"<code>check_db_installation(db_dir, foldseek_gpu)</code>","text":"<p>Check if the baktfold database is installed.</p> <p>Parameters:</p> Name Type Description Default <code>db_dir</code> <code>Path</code> <p>The directory where the database is installed.</p> required <code>foldseek_gpu</code> <code>bool</code> <p>Whether to install foldseek-gpu compatible baktfold db</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if all required files are present, False otherwise.</p> Source code in <code>src/baktfold/databases/db.py</code> <pre><code>def check_db_installation(db_dir: Path, foldseek_gpu: bool) -&gt; bool:\n    \"\"\"\n    Check if the baktfold database is installed.\n\n    Args:\n        db_dir Path: The directory where the database is installed.\n        foldseek_gpu bool: Whether to install foldseek-gpu compatible baktfold db\n\n    Returns:\n        bool: True if all required files are present, False otherwise.\n    \"\"\"\n    downloaded_flag = True\n    for file_name in BAKTFOLD_DB_NAMES:\n        path = Path(db_dir) / file_name\n        if not path.is_file():\n            logger.warning(f\"baktfold Database file {path} is missing\")\n            downloaded_flag = False\n            break\n\n    gpu_flag = True\n    if foldseek_gpu:\n        for file_name in baktfold_DB_FOLDSEEK_GPU_NAMES:\n            path = Path(db_dir) / file_name\n            if not path.is_file():\n                logger.warning(f\"baktfold Foldseek-GPU Database file {path} is missing\")\n                gpu_flag = False\n                break \n\n    return downloaded_flag, gpu_flag\n</code></pre>"},{"location":"reference/databases/#src.baktfold.databases.db.check_prostT5_download","title":"<code>check_prostT5_download(model_dir, model_name)</code>","text":"<p>Args:     model_dir (Path): Directory where the model and tokenizer is be stored.     model_name (str): Name of the pre-trained T5 model.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>bool to tell baktfold whether to download ProstT5</p> Source code in <code>src/baktfold/databases/db.py</code> <pre><code>def check_prostT5_download(model_dir: Path, model_name: str) -&gt; bool:\n    \"\"\"\n     Args:\n        model_dir (Path): Directory where the model and tokenizer is be stored.\n        model_name (str): Name of the pre-trained T5 model.\n    Returns:\n        bool: bool to tell baktfold whether to download ProstT5\n    \"\"\"\n\n    # assumes already has been downloaded\n    download = False\n\n    if model_name == \"Rostlab/ProstT5_fp16\":\n\n        model_sub_dir = \"models--Rostlab--ProstT5_fp16\"\n        DICT = PROSTT5_MD5_DICTIONARY\n\n\n    for key in DICT:\n        for nested_key in DICT[key]:\n            file_path = Path(\n                f\"{model_dir}/{model_sub_dir}/{key}/{nested_key}\"\n            )\n\n            # check file exists\n            if file_path.exists():\n                md5_sum = calc_md5_sum(file_path)\n                if md5_sum != DICT[key][nested_key]:\n                    logger.warning(\n                        f\"Corrupt model file {file_path}! MD5 should be '{DICT[key][nested_key]}' but is '{md5_sum}'\"\n                    )\n                    download = True\n            else:\n                logger.warning(f\"Model file {file_path} does not exist.\")\n                download = True\n\n    return download\n</code></pre>"},{"location":"reference/databases/#src.baktfold.databases.db.download","title":"<code>download(tarball_path, cache_dir)</code>","text":"<p>Download the database from the given URL using HF.</p> <p>Parameters:</p> Name Type Description Default <code>tarball_path</code> <code>Path</code> <p>The path where the downloaded tarball should be saved.</p> required Source code in <code>src/baktfold/databases/db.py</code> <pre><code>def download(tarball_path: Path, cache_dir: Path) -&gt; None:\n    \"\"\"\n    Download the database from the given URL using HF.\n\n    Args:\n        tarball_path (Path): The path where the downloaded tarball should be saved.\n    \"\"\"\n\n    hf_tarball_path = hf_hub_download(\n        repo_id=\"gbouras13/baktfold-db\",\n        repo_type=\"dataset\",\n        filename=\"baktfold_db.tar.gz\"  ,\n        cache_dir=f\"{cache_dir}\"\n    )\n    # move from cache_dir to the base\n    # need to get the actual path not symlink\n\n    real_tarball = Path(hf_tarball_path).resolve()\n    tarball_path.parent.mkdir(parents=True, exist_ok=True)\n\n    shutil.move(real_tarball, tarball_path)\n\n    logger.info(f\"Tarball saved to {tarball_path}\")\n</code></pre>"},{"location":"reference/databases/#src.baktfold.databases.db.download_requests","title":"<code>download_requests(db_url, tarball_path)</code>","text":"<p>Downloads a file from a given URL using the requests library.</p> <p>Parameters:</p> Name Type Description Default <code>db_url</code> <code>str</code> <p>The URL of the file to download.</p> required <code>tarball_path</code> <code>Path</code> <p>The path to save the downloaded file.</p> required <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; download_requests(\"https://zenodo.org/records/17347516/files/baktfold_db.tar.gz\", Path(\"baktfold_db.tar.gz\"))\n</code></pre> Source code in <code>src/baktfold/databases/db.py</code> <pre><code>def download_requests(db_url: str, tarball_path: Path):\n    \"\"\"\n    Downloads a file from a given URL using the requests library.\n\n    Args:\n      db_url (str): The URL of the file to download.\n      tarball_path (Path): The path to save the downloaded file.\n\n    Returns:\n      None\n\n    Examples:\n      &gt;&gt;&gt; download_requests(\"https://zenodo.org/records/17347516/files/baktfold_db.tar.gz\", Path(\"baktfold_db.tar.gz\"))\n    \"\"\"\n\n    headers = {\n        \"User-Agent\": f\"baktfold/{CURRENT_DB_VERSION} (contact: george.bouras@adelaide.edu.au)\"\n    }\n\n    try:\n        with tarball_path.open(\"wb\") as fh_out, requests.get(\n            db_url, stream=True, headers=headers\n        ) as resp:\n            total_length = resp.headers.get(\"content-length\")\n            if total_length is not None:  # content length header is set\n                total_length = int(total_length)\n            with alive_bar(total=total_length, scale=\"SI\") as bar:\n                for data in resp.iter_content(chunk_size=1024 * 1024):\n                    fh_out.write(data)\n                    bar(count=len(data))\n    except:\n        logger.error(\n            f\"ERROR: Could not download file from Zenodo! url={db_url}, path={tarball_path}\"\n        )\n</code></pre>"},{"location":"reference/databases/#src.baktfold.databases.db.download_zenodo_prostT5","title":"<code>download_zenodo_prostT5(model_dir, logdir, threads)</code>","text":"<p>Download the ProstT5 model from Zenodo</p> <p>Parameters:</p> Name Type Description Default <code>db_url</code> <code>str</code> <p>The URL of the database.</p> required <code>tarball_path</code> <code>Path</code> <p>The path where the downloaded tarball should be saved.</p> required Source code in <code>src/baktfold/databases/db.py</code> <pre><code>def download_zenodo_prostT5(model_dir, logdir, threads):\n    \"\"\"\n    Download the ProstT5 model from Zenodo\n\n    Args:\n        db_url (str): The URL of the database.\n        tarball_path (Path): The path where the downloaded tarball should be saved.\n    \"\"\"\n\n    db_url = VERSION_DICTIONARY[CURRENT_DB_VERSION][\"prostt5_backup_url\"]\n    requiredmd5 = VERSION_DICTIONARY[CURRENT_DB_VERSION][\"prostt5_backup_md5\"]\n\n    logger.info(f\"Downloading ProstT5 model backup from {db_url}\")\n\n    tarball = VERSION_DICTIONARY[CURRENT_DB_VERSION][\"prostt5_backup_tarball\"]\n    tarball_path = Path(f\"{model_dir}/{tarball}\")\n    download_requests(db_url, tarball_path)\n\n    md5_sum = calc_md5_sum(tarball_path)\n\n    if md5_sum == requiredmd5:\n        logger.info(f\"ProstT5 model backup file download OK: {md5_sum}\")\n    else:\n        logger.error(\n            f\"Error: corrupt file! MD5 should be '{requiredmd5}' but is '{md5_sum}'\"\n        )\n\n    logger.info(\n        f\"Extracting ProstT5 model backup tarball: file={tarball_path}, output={model_dir}\"\n    )\n\n    try:\n        with tarball_path.open(\"rb\") as fh_in, tarfile.open(\n            fileobj=fh_in, mode=\"r:gz\"\n        ) as tar_file:\n            tar_file.extractall(path=str(model_dir))\n\n    except OSError:\n        logger.warning(\"Encountered OSError: {}\".format(OSError))\n        logger.error(f\"Could not extract {tarball_path} to {model_dir}\")\n\n    tarball_path.unlink()\n</code></pre>"},{"location":"reference/databases/#src.baktfold.databases.db.foldseek_makepaddedseqdb","title":"<code>foldseek_makepaddedseqdb(db_dir)</code>","text":"<p>Runs the Foldseek makepaddedseqdb command on a given database directory.</p> <p>Parameters:</p> Name Type Description Default <code>db_dir</code> <code>Path</code> <p>The path to the database directory.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; foldseek_makepaddedseqdb(Path(\"baktfold_db\"))\n</code></pre> Source code in <code>src/baktfold/databases/db.py</code> <pre><code>def foldseek_makepaddedseqdb(db_dir: Path) -&gt; None:\n    \"\"\"\n    Runs the Foldseek makepaddedseqdb command on a given database directory.\n\n    Args:\n      db_dir (Path): The path to the database directory.\n\n    Returns:\n      None\n\n    Examples:\n      &gt;&gt;&gt; foldseek_makepaddedseqdb(Path(\"baktfold_db\"))\n    \"\"\"\n\n    dbs = [\"AFDBClusters\", \"pdb\", \"cath\", \"swissprot\"]\n    logdir = Path(db_dir) / \"logdir\"\n\n    for db_name in dbs:\n        db_path = Path(db_dir) / db_name\n        db_path_gpu = Path(db_dir) / f\"{db_name}_gpu\"\n\n        foldseek_makepaddedseqdb = ExternalTool(\n            tool=\"foldseek\",\n            input=\"\",\n            output=\"\",\n            params=f\"makepaddedseqdb {db_path} {db_path_gpu}\",\n            logdir=logdir,\n        )\n\n        ExternalTool.run_tool(foldseek_makepaddedseqdb)\n</code></pre>"},{"location":"reference/databases/#src.baktfold.databases.db.install_database","title":"<code>install_database(db_dir, foldseek_gpu, threads)</code>","text":"<p>Install the baktfold database.</p> <p>Parameters:</p> Name Type Description Default <code>db_dir</code> <code>Path</code> <p>The directory where the database should be installed.</p> required <code>foldseek_gpu</code> <code>bool</code> <p>Whether to install foldseek-gpu compatible baktfold db</p> required <code>threads</code> <code>int</code> <p>Number of threads available (makes downloading faster)</p> required Source code in <code>src/baktfold/databases/db.py</code> <pre><code>def install_database(db_dir: Path, foldseek_gpu: bool, threads: int) -&gt; None:\n    \"\"\"\n    Install the baktfold database.\n\n    Args:\n        db_dir Path: The directory where the database should be installed.\n        foldseek_gpu bool: Whether to install foldseek-gpu compatible baktfold db\n        threads int: Number of threads available (makes downloading faster)\n    \"\"\"\n\n    # check the database is installed\n    logger.info(f\"Checking baktfold database installation in {db_dir}.\")\n    downloaded_flag, gpu_flag = check_db_installation(db_dir, foldseek_gpu)\n    if downloaded_flag:\n        logger.info(\"All baktfold databases files are present\")\n    else:\n        logger.info(\"Some baktfold databases files are missing\")\n\n        DICT = VERSION_DICTIONARY\n        db_url = DICT[CURRENT_DB_VERSION][\"db_url\"]\n        logger.info(f\"Downloading baktfold DB from {db_url}\")\n\n        requiredmd5s = DICT[CURRENT_DB_VERSION][\"md5\"]\n        tarball = DICT[CURRENT_DB_VERSION][\"tarball\"]\n\n        tarball_path = Path(f\"{db_dir}/{tarball}\")\n        logdir = Path(db_dir) / \"logdir\"\n\n        try: \n            logger.info(f\"Downloading from HuggingFace\")\n            download(tarball_path, db_dir)\n        except:\n            logger.warning(\n                f\"Could not download file from HuggingFace: path={tarball_path}\"\n            )\n            logger.warning(f\"Trying now with requests\")\n            download_requests(db_url, tarball_path)\n\n\n        md5_sum = calc_md5_sum(tarball_path)\n\n\n        if md5_sum in requiredmd5s:\n            logger.info(f\"baktfold database file download OK: {md5_sum}\")\n        else:\n            logger.error(\n                f\"Error: corrupt database file! MD5 should be '{requiredmd5s}' but is '{md5_sum}'\"\n            )\n\n        logger.info(\n            f\"Extracting baktfold database tarball: file={tarball_path}, output={db_dir}\"\n        )\n        untar(tarball_path, db_dir, DICT)\n        tarball_path.unlink()\n\n    if foldseek_gpu:\n        if gpu_flag:\n            logger.info(\"All baktfold database files compatible with Foldseek-GPU are present\")\n        else:\n            logger.info(\"Some baktfold database files compatible with Foldseek-GPU are missing\")\n            logger.info(\"Creating them\")\n            foldseek_makepaddedseqdb(db_dir)\n\n    logger.info(\"Database download and processing complete\")\n</code></pre>"},{"location":"reference/databases/#src.baktfold.databases.db.untar","title":"<code>untar(tarball_path, output_path, DICT)</code>","text":"<p>Extract the tarball to the output path.</p> <p>Parameters:</p> Name Type Description Default <code>tarball_path</code> <code>Path</code> <p>The path to the tarball file.</p> required <code>output_path</code> <code>Path</code> <p>The path where the contents of the tarball should be extracted.</p> required <code>DICT</code> <code>dict</code> <p>version dictionary</p> required Source code in <code>src/baktfold/databases/db.py</code> <pre><code>def untar(tarball_path: Path, output_path: Path, DICT: dict) -&gt; None:\n    \"\"\"\n    Extract the tarball to the output path.\n\n    Args:\n        tarball_path (Path): The path to the tarball file.\n        output_path (Path): The path where the contents of the tarball should be extracted.\n        DICT (dict): version dictionary\n    \"\"\"\n    try:\n        with tarball_path.open(\"rb\") as fh_in, tarfile.open(\n            fileobj=fh_in, mode=\"r:gz\"\n        ) as tar_file:\n            tar_file.extractall(path=str(output_path))\n\n        tarpath = Path(output_path) / DICT[CURRENT_DB_VERSION][\"dir_name\"]\n\n        # Get a list of all files in the directory\n        files_to_move = [f for f in tarpath.iterdir() if f.is_file()]\n\n        # Move each file to the destination directory\n        for file_name in files_to_move:\n            destination_path = output_path / file_name.name\n            shutil.move(file_name, destination_path)\n        # remove the directory\n        remove_directory(tarpath)\n\n    except OSError:\n        logger.warning(\"Encountered OSError: {}\".format(OSError))\n        logger.error(f\"Could not extract {tarball_path} to {output_path}\")\n</code></pre>"},{"location":"reference/databases/#src.baktfold.databases.db.validate_db","title":"<code>validate_db(database, default_dir, foldseek_gpu)</code>","text":"<p>Validates the baktfold database is installed.</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>str</code> <p>The directory where the database is installed.</p> required <code>default_dir</code> <code>str</code> <p>Default DB location</p> required <code>foldseek_gpu</code> <code>bool</code> <p>Whether to install foldseek-gpu compatible baktfold db</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>Path</code> <p>True if all required files are present, False otherwise.</p> Source code in <code>src/baktfold/databases/db.py</code> <pre><code>def validate_db(database: str, default_dir: str, foldseek_gpu: bool) -&gt; Path:\n    \"\"\"\n    Validates the baktfold database is installed.\n\n    Args:\n        database str: The directory where the database is installed.\n        default_dir str: Default DB location\n        foldseek_gpu bool: Whether to install foldseek-gpu compatible baktfold db\n\n    Returns:\n        bool: True if all required files are present, False otherwise.\n    \"\"\"\n    # set default DB if not specified\n    if database is not None:\n        database: Path = Path(database)\n    else:\n        database = Path(default_dir)\n\n    # check the database is installed\n    logger.info(f\"Checking baktfold database installation in {database}\")\n    downloaded_flag, gpu_flag = check_db_installation(database, foldseek_gpu)\n    if downloaded_flag == True:\n        logger.info(\"All baktfold databases files are present\")\n    else:\n        if database == Path(default_dir):  # default\n            logger.error(\n                f\"baktfold database not found. Please run baktfold install to download and install the baktfold database\"\n            )\n        else:  # specific\n            logger.error(\n                f\"baktfold database not found. Please run baktfold install -d {database} to download and install the baktfold database\"\n            )\n    if foldseek_gpu:\n        if gpu_flag:\n            logger.info(\"All baktfold database files compatible with Foldseek-GPU are present\")\n        else:\n            logger.error(\n                f\"baktfold database files compatible with Foldseek-GPU not found. Please run baktfold install -d {database} --foldseek_gpu\"\n            )\n\n\n    return database\n</code></pre>"},{"location":"reference/features/","title":"Features","text":"<p>Some code adapted from @mheinzinger </p> <p>https://github.com/mheinzinger/ProstT5/blob/main/scripts/generate_foldseek_db.py</p> <p>Code adapted from @mheinzinger </p> <p>https://github.com/mheinzinger/ProstT5/blob/main/scripts/predict_3Di_encoderOnly.py</p>"},{"location":"reference/features/#src.baktfold.features.create_foldseek_db.create_foldseek_prostt5_gpu_db","title":"<code>create_foldseek_prostt5_gpu_db(fasta_aa, foldseek_db_path, db_dir, logdir)</code>","text":"<p>Convert a Foldseek DB with ProstT5 3Di predictions using Foldseek-GPU</p> <p>Parameters:</p> Name Type Description Default <code>fasta_aa</code> <code>Path</code> <p>Path to the amino-acid FASTA file.</p> required <code>foldseek_db_path</code> <code>Path</code> <p>Path to the directory where Foldseek database will be stored.</p> required <code>db_dir</code> <code>Path</code> <p>Path to the baktfold DB</p> required <code>logdir</code> <code>Path</code> <p>Path to the directory where logs will be stored.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/features/create_foldseek_db.py</code> <pre><code>def create_foldseek_prostt5_gpu_db(\n    fasta_aa: Path, foldseek_db_path: Path, db_dir: Path, logdir: Path\n) -&gt; None:\n    \"\"\"\n    Convert a Foldseek DB with ProstT5 3Di predictions using Foldseek-GPU\n\n    Args:\n        fasta_aa (Path): Path to the amino-acid FASTA file.\n        foldseek_db_path (Path): Path to the directory where Foldseek database will be stored.\n        db_dir (Path): Path to the baktfold DB\n        logdir (Path): Path to the directory where logs will be stored.\n    Returns:\n        None\n    \"\"\"\n\n    prostt5_db_path = Path(db_dir) / \"prostt5_weights\"\n\n    foldseek_createdb_prostt5 = ExternalTool(\n        tool=\"foldseek\",\n        input=f\"\",\n        output=f\"\",\n        params=f\"createdb {fasta_aa} {foldseek_db_path}  --prostt5-model {prostt5_db_path}  \",\n        logdir=logdir,\n    )\n\n    ExternalTool.run_tool(foldseek_createdb_prostt5)\n</code></pre>"},{"location":"reference/features/#src.baktfold.features.create_foldseek_db.foldseek_tsv2db","title":"<code>foldseek_tsv2db(in_tsv, out_db_name, db_type, logdir)</code>","text":"<p>Convert a Foldseek TSV file to a Foldseek database.</p> <p>Parameters:</p> Name Type Description Default <code>in_tsv</code> <code>Path</code> <p>Path to the input TSV file.</p> required <code>out_db_name</code> <code>Path</code> <p>Path for the output Foldseek database.</p> required <code>db_type</code> <code>int</code> <p>Type of the output database.</p> required <code>logdir</code> <code>Path</code> <p>Path to the directory where logs will be stored.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/features/create_foldseek_db.py</code> <pre><code>def foldseek_tsv2db(\n    in_tsv: Path, out_db_name: Path, db_type: int, logdir: Path\n) -&gt; None:\n    \"\"\"\n    Convert a Foldseek TSV file to a Foldseek database.\n\n    Args:\n        in_tsv (Path): Path to the input TSV file.\n        out_db_name (Path): Path for the output Foldseek database.\n        db_type (int): Type of the output database.\n        logdir (Path): Path to the directory where logs will be stored.\n\n    Returns:\n        None\n    \"\"\"\n    foldseek_tsv2db = ExternalTool(\n        tool=\"foldseek\",\n        input=f\"\",\n        output=f\"\",\n        params=f\"tsv2db {in_tsv} {out_db_name}  --output-dbtype {str(db_type)} \",\n        logdir=logdir,\n    )\n\n    ExternalTool.run_tool(foldseek_tsv2db)\n</code></pre>"},{"location":"reference/features/#src.baktfold.features.create_foldseek_db.generate_foldseek_db_from_aa_3di","title":"<code>generate_foldseek_db_from_aa_3di(fasta_aa, fasta_3di, foldseek_db_path, logdir, prefix)</code>","text":"<p>Generate Foldseek database from amino-acid and 3Di sequences.</p> <p>Parameters:</p> Name Type Description Default <code>fasta_aa</code> <code>Path</code> <p>Path to the amino-acid FASTA file.</p> required <code>fasta_3di</code> <code>Path</code> <p>Path to the 3Di FASTA file.</p> required <code>foldseek_db_path</code> <code>Path</code> <p>Path to the directory where Foldseek database will be stored.</p> required <code>logdir</code> <code>Path</code> <p>Path to the directory where logs will be stored.</p> required <code>prefix</code> <code>str</code> <p>Prefix for the Foldseek database.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/features/create_foldseek_db.py</code> <pre><code>def generate_foldseek_db_from_aa_3di(\n    fasta_aa: Path, fasta_3di: Path, foldseek_db_path: Path, logdir: Path, prefix: str\n) -&gt; None:\n    \"\"\"\n    Generate Foldseek database from amino-acid and 3Di sequences.\n\n    Args:\n        fasta_aa (Path): Path to the amino-acid FASTA file.\n        fasta_3di (Path): Path to the 3Di FASTA file.\n        foldseek_db_path (Path): Path to the directory where Foldseek database will be stored.\n        logdir (Path): Path to the directory where logs will be stored.\n        prefix (str): Prefix for the Foldseek database.\n\n    Returns:\n        None\n    \"\"\"\n    # read in amino-acid sequences\n    sequences_aa = {}\n    for record in SeqIO.parse(fasta_aa, \"fasta\"):\n        sequences_aa[record.id] = str(record.seq)\n\n    # read in 3Di strings\n    sequences_3di = {}\n    for record in SeqIO.parse(fasta_3di, \"fasta\"):\n        if not record.id in sequences_aa.keys():\n            logger.warning(\n                \"Warning: ignoring 3Di entry {}, since it is not in the amino-acid FASTA file\".format(\n                    record.id\n                )\n            )\n        else:\n            sequences_3di[record.id] = str(record.seq)  #no upper if masked\n\n    # assert that we parsed 3Di strings for all sequences in the amino-acid FASTA file\n    for id in sequences_aa.keys():\n        if not id in sequences_3di.keys():\n            logger.warning(\n                \"Warning: entry {} in amino-acid FASTA file has no corresponding 3Di string\".format(\n                    id\n                )\n            )\n            logger.warning(\"Removing: entry {} from the Foldseek database \".format(id))\n            sequences_aa = {\n                id: sequence\n                for id, sequence in sequences_aa.items()\n                if id in sequences_3di\n            }\n\n    # generate TSV file contents\n    tsv_aa = \"\"\n    tsv_3di = \"\"\n    tsv_header = \"\"\n    for i, id in enumerate(sequences_aa.keys()):\n        tsv_aa += \"{}\\t{}\\n\".format(str(i + 1), sequences_aa[id])\n        tsv_3di += \"{}\\t{}\\n\".format(str(i + 1), sequences_3di[id])\n        tsv_header += \"{}\\t{}\\n\".format(str(i + 1), id)\n\n    #### write temp tsv files\n\n    # write TSV files\n    temp_aa_tsv: Path = Path(foldseek_db_path) / \"aa.tsv\"\n    temp_3di_tsv: Path = Path(foldseek_db_path) / \"3di.tsv\"\n    temp_header_tsv: Path = Path(foldseek_db_path) / \"header.tsv\"\n    with open(temp_aa_tsv, \"w\") as f:\n        f.write(tsv_aa)\n    with open(temp_3di_tsv, \"w\") as f:\n        f.write(tsv_3di)\n    with open(temp_header_tsv, \"w\") as f:\n        f.write(tsv_header)\n\n    # create foldseek db names\n\n    short_db_name = f\"{prefix}\"\n    aa_db_name: Path = Path(foldseek_db_path) / short_db_name\n    tsv_db_name: Path = Path(foldseek_db_path) / f\"{short_db_name}_ss\"\n    header_db_name: Path = Path(foldseek_db_path) / f\"{short_db_name}_h\"\n\n    # create Foldseek database with foldseek tsv2db\n\n    foldseek_tsv2db(temp_aa_tsv, aa_db_name, 0, logdir)\n    foldseek_tsv2db(temp_3di_tsv, tsv_db_name, 0, logdir)\n    foldseek_tsv2db(temp_header_tsv, header_db_name, 12, logdir)\n\n    # clean up\n    remove_file(temp_aa_tsv)\n    remove_file(temp_3di_tsv)\n    remove_file(temp_header_tsv)\n</code></pre>"},{"location":"reference/features/#src.baktfold.features.create_foldseek_db.generate_foldseek_db_from_structures","title":"<code>generate_foldseek_db_from_structures(fasta_aa, foldseek_db_path, structure_dir, logdir, prefix, proteins_flag)</code>","text":"<p>Generate Foldseek database from PDB files.</p> <p>Parameters:</p> Name Type Description Default <code>fasta_aa</code> <code>Path</code> <p>Path to the amino-acid FASTA file.</p> required <code>foldseek_db_path</code> <code>Path</code> <p>Path to the directory where Foldseek database will be stored.</p> required <code>structure_dir</code> <code>Path</code> <p>Path to the directory containing .pdb or .cif structure files.</p> required <code>logdir</code> <code>Path</code> <p>Path to the directory where logs will be stored.</p> required <code>prefix</code> <code>str</code> <p>Prefix for the Foldseek database.</p> required <code>proteins_flag</code> <code>bool</code> <p>Flag - True if proteins-compare is run</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/features/create_foldseek_db.py</code> <pre><code>def generate_foldseek_db_from_structures(\n    fasta_aa: Path,\n    foldseek_db_path: Path,\n    structure_dir: Path,\n    logdir: Path,\n    prefix: str,\n    proteins_flag: bool,\n) -&gt; None:\n    \"\"\"\n    Generate Foldseek database from PDB files.\n\n    Args:\n        fasta_aa (Path): Path to the amino-acid FASTA file.\n        foldseek_db_path (Path): Path to the directory where Foldseek database will be stored.\n        structure_dir (Path): Path to the directory containing .pdb or .cif structure files.\n        logdir (Path): Path to the directory where logs will be stored.\n        prefix (str): Prefix for the Foldseek database.\n        proteins_flag (bool): Flag - True if proteins-compare is run\n\n    Returns:\n        None\n    \"\"\"\n\n    # read in amino-acid sequences\n    sequences_aa = {}\n    for record in SeqIO.parse(fasta_aa, \"fasta\"):\n        sequences_aa[record.id] = str(record.seq)\n\n    # lists all the pdb files\n\n    structure_files = [\n        file\n        for file in os.listdir(structure_dir)\n        if file.endswith(\".pdb\") or file.endswith(\".cif\")\n    ]\n\n    num_structures = len(structure_files)\n\n    num_structures = 0\n\n    # Checks that ID is in the pdbs\n\n    no_structure_cds_ids = []\n\n    for cds_id in sequences_aa.keys():\n\n        matching_files = [\n            file\n            for file in structure_files\n            if f\"{cds_id}.pdb\" == file or f\"{cds_id}.cif\" == file\n        ]\n\n        if len(matching_files) == 1:\n            num_structures += 1\n\n        # should neve happen but in case\n        if len(matching_files) &gt; 1:\n            logger.warning(f\"More than 1 structures found for {cds_id}\")\n            logger.warning(\"Taking the first one\")\n            num_structures += 1\n        elif len(matching_files) == 0:\n            logger.warning(f\"No structure found for {cds_id}\")\n            logger.warning(f\"{cds_id} will be ignored in annotation\")\n            no_structure_cds_ids.append(cds_id)\n\n    if num_structures == 0:\n        logger.error(\n            f\"No structures with matching CDS ids were found at all. Check the {structure_dir} directory\"\n        )\n\n    # generate the db\n    short_db_name = f\"{prefix}\"\n    structure_db_name: Path = Path(foldseek_db_path) / short_db_name\n    query_structure_dir = structure_dir\n\n\n    foldseek_createdb_from_structures = ExternalTool(\n        tool=\"foldseek\",\n        input=f\"\",\n        output=f\"\",\n        params=f\"createdb {query_structure_dir} {structure_db_name} \",\n        logdir=logdir,\n    )\n\n    ExternalTool.run_tool(foldseek_createdb_from_structures)\n</code></pre>"},{"location":"reference/features/#src.baktfold.features.run_foldseek.create_result_tsv","title":"<code>create_result_tsv(query_db, target_db, result_db, result_tsv, logdir, foldseek_gpu, structures, threads)</code>","text":"<p>Create a TSV file containing the results of a Foldseek search.</p> <p>Parameters:</p> Name Type Description Default <code>query_db</code> <code>Path</code> <p>Path to the query database.</p> required <code>target_db</code> <code>Path</code> <p>Path to the target database.</p> required <code>result_db</code> <code>Path</code> <p>Path to the result database generated by the search.</p> required <code>result_tsv</code> <code>Path</code> <p>Path to save the resulting TSV file.</p> required <code>logdir</code> <code>Path</code> <p>Path to the directory where logs will be stored.</p> required <code>foldseek_gpu</code> <code>bool</code> <p>Run Foldseek-GPU with accelerate ungapped prefilter</p> required <code>structures</code> <code>bool</code> <p>Whether structures were input (not ProstT5)</p> required <code>threads</code> <code>int</code> <p>Number of threads to use.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/features/run_foldseek.py</code> <pre><code>def create_result_tsv(\n    query_db: Path, target_db: Path, result_db: Path, result_tsv: Path, logdir: Path, foldseek_gpu: bool, structures: bool, threads: int\n) -&gt; None:\n    \"\"\"\n    Create a TSV file containing the results of a Foldseek search.\n\n    Args:\n        query_db (Path): Path to the query database.\n        target_db (Path): Path to the target database.\n        result_db (Path): Path to the result database generated by the search.\n        result_tsv (Path): Path to save the resulting TSV file.\n        logdir (Path): Path to the directory where logs will be stored.\n        foldseek_gpu (bool): Run Foldseek-GPU with accelerate ungapped prefilter\n        structures (bool): Whether structures were input (not ProstT5)\n        threads (int): Number of threads to use.\n\n    Returns:\n        None\n    \"\"\"\n    if structures:\n        format_string= \"--format-output query,target,bits,fident,evalue,qstart,qend,qlen,tstart,tend,tlen,alntmscore,lddt\"\n    else:\n        format_string = \"--format-output query,target,bits,fident,evalue,qstart,qend,qlen,tstart,tend,tlen\"\n    if foldseek_gpu:\n        target_db = f\"{target_db}_gpu\"\n\n\n    cmd = f\"convertalis {query_db} {target_db} {result_db} {result_tsv} {format_string} --threads {threads}\"\n\n    foldseek_createtsv = ExternalTool(\n        tool=\"foldseek\",\n        input=f\"\",\n        output=f\"\",\n        params=f\"{cmd}\",\n        logdir=logdir,\n    )\n\n\n    ExternalTool.run_tool(foldseek_createtsv)\n</code></pre>"},{"location":"reference/features/#src.baktfold.features.run_foldseek.run_foldseek_search","title":"<code>run_foldseek_search(query_db, target_db, result_db, temp_db, threads, logdir, evalue, sensitivity, max_seqs, ultra_sensitive, extra_foldseek_params, foldseek_gpu, structures)</code>","text":"<p>Run a Foldseek search using given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>query_db</code> <code>Path</code> <p>Path to the query database.</p> required <code>target_db</code> <code>Path</code> <p>Path to the target database.</p> required <code>result_db</code> <code>Path</code> <p>Path to store the result database.</p> required <code>temp_db</code> <code>Path</code> <p>Path to store temporary files.</p> required <code>threads</code> <code>int</code> <p>Number of threads to use for the search.</p> required <code>logdir</code> <code>Path</code> <p>Path to the directory where logs will be stored.</p> required <code>evalue</code> <code>float</code> <p>E-value threshold for the search.</p> required <code>sensitivity</code> <code>float</code> <p>Sensitivity threshold for the search.</p> required <code>max_seqs</code> <code>int</code> <p>Maximum results per query sequence allowed to pass the prefilter for foldseek.</p> required <code>ultra_sensitive</code> <code>bool</code> <p>Whether to skip foldseek prefilter for maximum sensitivity</p> required <code>extra_foldseek_params</code> <code>str</code> <p>Extra foldseek search params</p> required <code>foldseek_gpu</code> <code>bool</code> <p>Run Foldseek-GPU with accelerate ungapped prefilter</p> required <code>structures</code> <code>bool</code> <p>Run Foldseek with structures, not ProstT5 3Dis</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/features/run_foldseek.py</code> <pre><code>def run_foldseek_search(\n    query_db: Path,\n    target_db: Path,\n    result_db: Path,\n    temp_db: Path,\n    threads: int,\n    logdir: Path,\n    evalue: float,\n    sensitivity: float,\n    max_seqs: int,\n    ultra_sensitive: bool,\n    extra_foldseek_params: str,\n    foldseek_gpu: bool,\n    structures: bool,\n) -&gt; None:\n    \"\"\"\n    Run a Foldseek search using given parameters.\n\n    Args:\n        query_db (Path): Path to the query database.\n        target_db (Path): Path to the target database.\n        result_db (Path): Path to store the result database.\n        temp_db (Path): Path to store temporary files.\n        threads (int): Number of threads to use for the search.\n        logdir (Path): Path to the directory where logs will be stored.\n        evalue (float): E-value threshold for the search.\n        sensitivity (float): Sensitivity threshold for the search.\n        max_seqs (int): Maximum results per query sequence allowed to pass the prefilter for foldseek.\n        ultra_sensitive (bool): Whether to skip foldseek prefilter for maximum sensitivity\n        extra_foldseek_params (str): Extra foldseek search params\n        foldseek_gpu (bool): Run Foldseek-GPU with accelerate ungapped prefilter\n        structures (bool): Run Foldseek with structures, not ProstT5 3Dis\n\n    Returns:\n        None\n    \"\"\"\n\n\n\n    if ultra_sensitive:\n        cmd = f\"search {query_db} {target_db} {result_db} {temp_db} --threads {str(threads)} -e {evalue} -s {sensitivity} --exhaustive-search\"\n    else:\n        cmd = f\"search {query_db} {target_db} {result_db} {temp_db} --threads {str(threads)} -e {evalue} -s {sensitivity} --max-seqs {max_seqs}\"\n\n    # support foldseek gpu only for the regular DB search for now\n    if foldseek_gpu:\n        cmd = f\"search {query_db} {target_db}_gpu {result_db} {temp_db} --threads {str(threads)} -e {evalue}  --gpu 1 --prefilter-mode 1 --max-seqs {max_seqs}\"\n\n    if extra_foldseek_params:\n        cmd += f\" {extra_foldseek_params}\"\n\n    # need -a 1 to compute the alignment so tmscore and lddt can be output (if using --structures)\n    if structures:\n        cmd += f\" -a 1\"\n\n    foldseek_search = ExternalTool(\n        tool=\"foldseek\",\n        input=f\"\",\n        output=f\"\",\n        params=f\"{cmd}\",\n        logdir=logdir,\n    )\n\n    ExternalTool.run_tool(foldseek_search)\n</code></pre>"},{"location":"reference/features/#src.baktfold.features.run_foldseek.summarise_hits","title":"<code>summarise_hits(result_db, result_db_greedy_best_hits, logdir, threads)</code>","text":"<p>Get all non-overlapping tophits covering a query (designed for CATH)</p> <p>Parameters:</p> Name Type Description Default <code>result_db</code> <code>Path</code> <p>Path to the result database generated by the search.</p> required <code>result_db_greedy_best_hits</code> <code>Path</code> <p>Path to save the greedy best hits results db.</p> required <code>logdir</code> <code>Path</code> <p>Path to the directory where logs will be stored.</p> required <code>threads</code> <code>int</code> <p>Number of threads to use.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/features/run_foldseek.py</code> <pre><code>def summarise_hits(result_db: Path, result_db_greedy_best_hits: Path, logdir: Path, threads: int) -&gt; None:\n    \"\"\"\n    Get all non-overlapping tophits covering a query (designed for CATH)\n\n    Args:\n        result_db (Path): Path to the result database generated by the search.\n        result_db_greedy_best_hits (Path): Path to save the greedy best hits results db.\n        logdir (Path): Path to the directory where logs will be stored.\n        threads (int): Number of threads to use.\n\n    Returns:\n        None\n    \"\"\"\n\n    cmd = f\"summarizeresult  {result_db} {result_db_greedy_best_hits} --threads {threads} -a 1\"\n\n    foldseek_summarizeresult = ExternalTool(\n        tool=\"foldseek\",\n        input=f\"\",\n        output=f\"\",\n        params=f\"{cmd}\",\n        logdir=logdir,\n    )\n\n    ExternalTool.run_tool(foldseek_summarizeresult)\n</code></pre>"},{"location":"reference/features/#src.baktfold.features.predict_3Di.CNN","title":"<code>CNN</code>","text":"<p>             Bases: <code>nn.Module</code></p> <p>Convolutional neural network (two convolutional layers).</p> <p>Parameters:</p> Name Type Description Default <code>nn.Module</code> <p>The base class for all neural network modules.</p> required <p>Returns:</p> Type Description <p>None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; CNN()\nNone\n</code></pre> Source code in <code>src/baktfold/features/predict_3Di.py</code> <pre><code>class CNN(nn.Module):\n    \"\"\"\n    Convolutional neural network (two convolutional layers).\n\n    Args:\n      nn.Module: The base class for all neural network modules.\n\n    Returns:\n      None.\n\n    Examples:\n      &gt;&gt;&gt; CNN()\n      None\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the Convolutional Neural Network (CNN) model.\n        \"\"\"\n        super(CNN, self).__init__()\n\n        self.classifier = nn.Sequential(\n            nn.Conv2d(1024, 32, kernel_size=(7, 1), padding=(3, 0)),  # 7x32\n            nn.ReLU(),\n            nn.Dropout(0.0),\n            nn.Conv2d(32, 20, kernel_size=(7, 1), padding=(3, 0)),\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Perform forward pass through the CNN.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, embedding_size).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, sequence_length, num_classes).\n\n        L = protein length\n        B = batch-size\n        F = number of features (1024 for embeddings)\n        N = number of classes (20 for 3Di)\n        \"\"\"\n\n        # Permute input tensor to match expected shape\n        # Input shape: (batch_size, sequence_length, embedding_size)\n        # Output shape: (batch_size, embedding_size, sequence_length, 1)\n\n        x = x.permute(0, 2, 1).unsqueeze(\n            dim=-1\n        )  # IN: X = (B x L x F); OUT: (B x F x L, 1)\n\n        # Pass the input through the classifier\n        # Output shape: (batch_size, num_classes, sequence_length, 1)\n        Yhat = self.classifier(x)  # OUT: Yhat_consurf = (B x N x L x 1)\n\n        # Remove the singleton dimension from the output tensor\n        # Output shape: (batch_size, num_classes, sequence_length)\n        Yhat = Yhat.squeeze(dim=-1)  # IN: (B x N x L x 1); OUT: ( B x L x N )\n        return Yhat\n</code></pre>"},{"location":"reference/features/#src.baktfold.features.predict_3Di.CNN.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the Convolutional Neural Network (CNN) model.</p> Source code in <code>src/baktfold/features/predict_3Di.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize the Convolutional Neural Network (CNN) model.\n    \"\"\"\n    super(CNN, self).__init__()\n\n    self.classifier = nn.Sequential(\n        nn.Conv2d(1024, 32, kernel_size=(7, 1), padding=(3, 0)),  # 7x32\n        nn.ReLU(),\n        nn.Dropout(0.0),\n        nn.Conv2d(32, 20, kernel_size=(7, 1), padding=(3, 0)),\n    )\n</code></pre>"},{"location":"reference/features/#src.baktfold.features.predict_3Di.CNN.forward","title":"<code>forward(x)</code>","text":"<p>Perform forward pass through the CNN.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>Input tensor of shape (batch_size, sequence_length, embedding_size).</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>torch.Tensor: Output tensor of shape (batch_size, sequence_length, num_classes).</p> <p>L = protein length B = batch-size F = number of features (1024 for embeddings) N = number of classes (20 for 3Di)</p> Source code in <code>src/baktfold/features/predict_3Di.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Perform forward pass through the CNN.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, embedding_size).\n\n    Returns:\n        torch.Tensor: Output tensor of shape (batch_size, sequence_length, num_classes).\n\n    L = protein length\n    B = batch-size\n    F = number of features (1024 for embeddings)\n    N = number of classes (20 for 3Di)\n    \"\"\"\n\n    # Permute input tensor to match expected shape\n    # Input shape: (batch_size, sequence_length, embedding_size)\n    # Output shape: (batch_size, embedding_size, sequence_length, 1)\n\n    x = x.permute(0, 2, 1).unsqueeze(\n        dim=-1\n    )  # IN: X = (B x L x F); OUT: (B x F x L, 1)\n\n    # Pass the input through the classifier\n    # Output shape: (batch_size, num_classes, sequence_length, 1)\n    Yhat = self.classifier(x)  # OUT: Yhat_consurf = (B x N x L x 1)\n\n    # Remove the singleton dimension from the output tensor\n    # Output shape: (batch_size, num_classes, sequence_length)\n    Yhat = Yhat.squeeze(dim=-1)  # IN: (B x N x L x 1); OUT: ( B x L x N )\n    return Yhat\n</code></pre>"},{"location":"reference/features/#src.baktfold.features.predict_3Di.get_T5_model","title":"<code>get_T5_model(model_dir, model_name, cpu, threads)</code>","text":"<p>Loads a T5 model and tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>Path</code> <p>Directory where the model and tokenizer is be stored.</p> required <code>model_name</code> <code>str</code> <p>Name of the pre-trained T5 model.</p> required <code>cpu</code> <code>bool</code> <p>Whether to use CPU only.</p> required <code>threads</code> <code>int</code> <p>Number of cpu threads.</p> required <p>Returns:</p> Type Description <code>(T5EncoderModel, T5Tokenizer)</code> <p>Tuple[T5EncoderModel, T5Tokenizer]: Tuple containing the loaded T5 model and tokenizer.</p> Source code in <code>src/baktfold/features/predict_3Di.py</code> <pre><code>def get_T5_model(\n    model_dir: Path, model_name: str, cpu: bool, threads: int\n) -&gt; (T5EncoderModel, T5Tokenizer):\n    \"\"\"\n    Loads a T5 model and tokenizer.\n\n    Args:\n        model_dir (Path): Directory where the model and tokenizer is be stored.\n        model_name (str): Name of the pre-trained T5 model.\n        cpu (bool): Whether to use CPU only.\n        threads (int): Number of cpu threads.\n\n    Returns:\n        Tuple[T5EncoderModel, T5Tokenizer]: Tuple containing the loaded T5 model and tokenizer.\n    \"\"\"\n\n    # sets the device\n\n    # Torch load will map back to device from state, which often is GPU:0.\n    # to overcome, need to explicitly map to active device\n\n    global device\n\n    torch.set_num_threads(threads)\n\n    if cpu is True:\n        device = torch.device(\"cpu\")\n        dev_name = \"cpu\"\n    else:\n        # check for NVIDIA/cuda\n        if torch.cuda.is_available():\n            device = torch.device(\"cuda:0\")\n            dev_name = \"cuda:0\"\n        # check for apple silicon/metal\n        elif torch.backends.mps.is_available():\n            device = torch.device(\"mps\")\n            dev_name = \"mps\"\n        else:\n            device = torch.device(\"cpu\")\n            dev_name = \"cpu\"\n            if cpu is not True:\n                logger.warning(\n                    \"No available GPU was found, but --cpu was not specified\"\n                )\n                logger.warning(\"ProstT5 will be run with CPU only\")\n\n    # logger device only if the function is called\n    logger.info(\"Using device: {}\".format(dev_name))\n\n    # make dir if doesnt exist\n    Path(model_dir).mkdir(parents=True, exist_ok=True)\n\n    # load\n    logger.info(f\"Loading T5 from: {model_dir}/{model_name}\")\n    logger.info(f\"If {model_dir}/{model_name} is not found, it will be downloaded\")\n\n    # check ProstT5 is downloaded\n    # flag assumes transformers takes from local file (see #44)\n    localfile = True\n    download = False\n\n    download = check_prostT5_download(model_dir, model_name)\n    if download:\n        localfile = False\n        logger.info(\"ProstT5 not found. Downloading ProstT5 from Hugging Face\")\n    try:\n        model = T5EncoderModel.from_pretrained(\n            model_name,\n            cache_dir=f\"{model_dir}/\",\n            force_download=download,\n            local_files_only=localfile,\n        ).to(device)\n\n    except:\n        logger.warning(\"Download from Hugging Face failed. Trying backup from Zenodo.\")\n        logdir = f\"{model_dir}/logdir\"\n        download_zenodo_prostT5(model_dir, logdir, threads )\n\n        model = T5EncoderModel.from_pretrained(\n            model_name,\n            cache_dir=f\"{model_dir}/\",\n            force_download=False,\n            local_files_only=True,\n        ).to(device)\n\n    model = model.eval()\n    vocab = T5Tokenizer.from_pretrained(\n        model_name, cache_dir=f\"{model_dir}/\", do_lower_case=False,\n        use_fast=False\n    )\n\n    model = T5EncoderModel.from_pretrained(\n                model_name,\n                cache_dir=f\"{model_dir}/\",\n                force_download=False,\n                local_files_only=True,\n            ).to(device)\n\n    logger.info(f\"{model_name} loaded\")\n\n    return model, vocab\n</code></pre>"},{"location":"reference/features/#src.baktfold.features.predict_3Di.get_embeddings","title":"<code>get_embeddings(hypotheticals, cds_dict, out_path, prefix, model_dir, model_name, checkpoint_path, output_3di, output_h5_per_residue, output_h5_per_protein, half_precision, max_residues=100000, max_seq_len=30000, max_batch=10000, cpu=False, output_probs=True, save_per_residue_embeddings=False, save_per_protein_embeddings=False, threads=1, mask_threshold=0, has_duplicate_locus=False)</code>","text":"<p>Generate embeddings and predictions for protein sequences using ProstT5 encoder &amp; CNN prediction head.</p> <p>Parameters:</p> Name Type Description Default <code>hypotheticals</code> <code> Dict[str, Tuple[str, ...]]</code> <p>dictionary containing CDS IDs feature information</p> required <code>cds_dict</code> <code> Dict[str, Tuple[str, ...]]</code> <p>dictionary containing CDS IDs and corresponding protein sequences.</p> required <code>out_path</code> <code>Path</code> <p>Path to the output directory.</p> required <code>prefix</code> <code>str</code> <p>Prefix for the output files.</p> required <code>model_dir</code> <code>Path</code> <p>Directory containing the pre-trained model.</p> required <code>model_name</code> <code>str</code> <p>Name of the pre-trained model.</p> required <code>output_3di</code> <code>Path</code> <p>Path to the output 3Di file.</p> required <code>output_h5_per_residue</code> <code>Path</code> <p>Path to the output h5 per residue embeddings file.</p> required <code>output_h5_per_protein</code> <code>Path</code> <p>Path to the output h5 per proteins embeddings file.</p> required <code>half_precision</code> <code>bool</code> <p>Whether to use half precision for the models.</p> required <code>max_residues</code> <code>int</code> <p>Maximum number of residues allowed in a batch. Defaults to 3000.</p> <code>100000</code> <code>max_seq_len</code> <code>int</code> <p>Maximum sequence length allowed. Defaults to 1000.</p> <code>30000</code> <code>max_batch</code> <code>int</code> <p>Maximum batch size. Defaults to 100.</p> <code>10000</code> <code>cpu</code> <code>bool</code> <p>Whether to use CPU for processing. Defaults to False.</p> <code>False</code> <code>output_probs</code> <code>bool</code> <p>Whether to output probabilities. Defaults to True.</p> <code>True</code> <code>save_embeddings</code> <code>bool</code> <p>Whether to save embeddings to h5 file. Defaults to False. Will  save per residue embeddings</p> required <code>per_protein_embeddings</code> <code>bool</code> <p>Whether to save per protein mean embeddings to h5 file. Defaults to False.</p> required <code>threads</code> <code>int</code> <p>number of cpu threads</p> <code>1</code> <code>mask_threshold</code> <code>float) </code> <p>0-100 - below this ProstT5 confidence threshold, these residues are masked</p> <code>0</code> <code>has_duplicate_locus</code> <code>bool) </code> <p>some euks have dupe locus tags</p> <code>False</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if embeddings and predictions are generated successfully.</p> Source code in <code>src/baktfold/features/predict_3Di.py</code> <pre><code>def get_embeddings(\n    hypotheticals: Dict[str, Tuple[str, ...]],\n    cds_dict: Dict[str, Tuple[str, ...]],\n    out_path: Path,\n    prefix: str,\n    model_dir: Path,\n    model_name: str,\n    checkpoint_path: Path,\n    output_3di: Path,\n    output_h5_per_residue: Path,\n    output_h5_per_protein: Path,\n    half_precision: bool,\n    max_residues: int = 100000,\n    max_seq_len: int = 30000,\n    max_batch: int = 10000,\n    cpu: bool = False,\n    output_probs: bool = True,\n    save_per_residue_embeddings: bool = False,\n    save_per_protein_embeddings: bool = False,\n    threads: int = 1,\n    mask_threshold: float = 0,\n    has_duplicate_locus: bool = False\n) -&gt; bool:\n    \"\"\"\n    Generate embeddings and predictions for protein sequences using ProstT5 encoder &amp; CNN prediction head.\n\n    Args:\n        hypotheticals ( Dict[str, Tuple[str, ...]]):  dictionary containing CDS IDs feature information\n        cds_dict ( Dict[str, Tuple[str, ...]]):  dictionary containing CDS IDs and corresponding protein sequences.\n        out_path (Path): Path to the output directory.\n        prefix (str): Prefix for the output files.\n        model_dir (Path): Directory containing the pre-trained model.\n        model_name (str): Name of the pre-trained model.\n        output_3di (Path): Path to the output 3Di file.\n        output_h5_per_residue (Path): Path to the output h5 per residue embeddings file.\n        output_h5_per_protein (Path): Path to the output h5 per proteins embeddings file.\n        half_precision (bool): Whether to use half precision for the models.\n        max_residues (int, optional): Maximum number of residues allowed in a batch. Defaults to 3000.\n        max_seq_len (int, optional): Maximum sequence length allowed. Defaults to 1000.\n        max_batch (int, optional): Maximum batch size. Defaults to 100.\n        cpu (bool, optional): Whether to use CPU for processing. Defaults to False.\n        output_probs (bool, optional): Whether to output probabilities. Defaults to True.\n        save_embeddings (bool, optional): Whether to save embeddings to h5 file. Defaults to False. Will  save per residue embeddings\n        per_protein_embeddings (bool, optional): Whether to save per protein mean embeddings to h5 file. Defaults to False.\n        threads (int): number of cpu threads\n        mask_threshold (float) : 0-100 - below this ProstT5 confidence threshold, these residues are masked\n        has_duplicate_locus (bool) : some euks have dupe locus tags\n\n    Returns:\n        bool: True if embeddings and predictions are generated successfully.\n    \"\"\"\n\n    predictions = {}\n    batch_predictions = {}\n\n    if save_per_residue_embeddings:\n        embeddings_per_residue = {}\n        batch_embeddings_per_residue = {}\n    if save_per_protein_embeddings:\n        embeddings_per_protein = {}\n        batch_embeddings_per_protein = {}\n\n\n    prostt5_prefix = \"&lt;AA2fold&gt;\"\n\n\n    model, vocab = get_T5_model(model_dir, model_name, cpu, threads)\n    predictor = load_predictor(checkpoint_path)\n\n    logger.info(\"Beginning ProstT5 predictions\")\n\n    if half_precision:\n        model = model.half()\n        predictor = predictor.half()\n        logger.info(\"Using models in half-precision\")\n    else:\n        logger.info(\"Using models in full-precision\")\n\n    # 3Di predictions\n\n    fail_ids = []\n\n    for k, v in list(cds_dict.items()):\n        if len(v) == 0:\n            logger.info(f\"Skipping empty CDS entry as it has no amino acid string associated (likely pseudo): key={k}\")\n            del cds_dict[k]\n\n    # sort sequences by length\n    seq_dict = []\n    fail_ids = []\n\n    for k, seq in cds_dict.items():\n        if isinstance(seq, str) and seq:\n            clean_seq = (\n                seq.replace(\"U\", \"X\")\n                .replace(\"Z\", \"X\")\n                .replace(\"O\", \"X\")\n            )\n            seq_dict.append((k, clean_seq, len(clean_seq))) # in the correct format for the remainder of the code\n        else:\n            logger.warning(\n                f\"Protein header {k} is corrupt. It will be saved in fails.tsv\"\n            )\n            fail_ids.append(k)\n\n\n    original_keys = list(cds_dict.keys())\n        # --- sort once ---\n    seq_dict.sort(key=lambda x: x[2], reverse=True)\n\n    batch = list()\n    for seq_idx, (pdb_id, seq, slen) in enumerate(tqdm(seq_dict, desc=f\"Predicting 3Di\"), 1):\n\n        # replace non-standard AAs\n        seq = seq.replace(\"U\", \"X\").replace(\"Z\", \"X\").replace(\"O\", \"X\")\n        seq_len = len(seq)\n        seq = prostt5_prefix + \" \" + \" \".join(list(seq))\n        batch.append((pdb_id, seq, seq_len))\n\n        # count residues in current batch and add the last sequence length to\n        # avoid that batches with (n_res_batch &gt; max_residues) get processed\n        n_res_batch = sum([s_len for _, _, s_len in batch]) + seq_len\n        if (\n            len(batch) &gt;= max_batch\n            or n_res_batch &gt;= max_residues\n            or seq_idx == len(seq_dict)\n            or seq_len &gt; max_seq_len\n        ):\n            pdb_ids, seqs, seq_lens = zip(*batch)\n            batch = list()\n\n            token_encoding = vocab(\n                seqs,\n                add_special_tokens=True,\n                padding=\"longest\",\n                truncation=False,\n                return_tensors=\"pt\"\n                ).to(device)\n\n            try:\n                with torch.no_grad():\n                    embedding_repr = model(\n                        token_encoding.input_ids,\n                        attention_mask=token_encoding.attention_mask,\n                    )\n            except RuntimeError:\n                logger.warning(f\" number of residues in batch {n_res_batch}\")\n                logger.warning(f\" seq length is {seq_len}\")\n                logger.warning(f\" ids are {pdb_ids}\")\n                logger.warning(\n                    \"RuntimeError during embedding for {} (L={})\".format(\n                        pdb_id, seq_len\n                    )\n                )\n                for id in pdb_ids:\n                    fail_ids.append(id)\n                continue\n\n\n            # ProtT5 appends a special tokens at the end of each sequence\n            # Mask this also out during inference while taking into account the prostt5 prefix\n            try:\n                for idx, s_len in enumerate(seq_lens):\n                    token_encoding.attention_mask[idx, s_len + 1] = 0\n\n                # extract last hidden states (=embeddings)\n                residue_embedding = embedding_repr.last_hidden_state.detach()\n                # mask out padded elements in the attention output (can be non-zero) for further processing/prediction\n                residue_embedding = (\n                    residue_embedding\n                    * token_encoding.attention_mask.unsqueeze(dim=-1)\n                )\n                # slice off embedding of special token prepended before to each sequence\n                residue_embedding = residue_embedding[:, 1:]\n                prediction = predictor(residue_embedding)\n\n\n                # compute max probabilities per token/residue\n                probabilities = toCPU(\n                    torch.max(\n                        F.softmax(prediction, dim=1), dim=1, keepdim=True\n                    )[0]\n                )\n\n                prediction = toCPU(\n                    torch.max(prediction, dim=1, keepdim=True)[1]\n                ).astype(np.byte)\n\n\n                    # to get logits\n                # t = prediction.transpose(1, 2)  # changes ( B x L x N ) to ( B x N x L ) \n                # logits = t.detach().cpu().numpy()\n                # print(logits[0])\n                # print(logits[0].shape)\n\n                    # prob_tensor = F.softmax(prediction, dim=1).cpu()\n                    # #print(prob_tensor.shape)\n                    # prob_matrix = prob_tensor.squeeze(0).transpose(0, 1)\n                    # #print(prob_matrix.shape)\n                    # all_sampled_states = {}\n                    # i = 1\n                    # while i &lt; 11:\n                    #     all_sampled_states[i] = torch.multinomial(prob_matrix, 1).squeeze(1)\n                    #     i += 1\n\n                    #sampled_states = torch.multinomial(prob_matrix, 1).squeeze(1)\n                    #sampled_states = torch.multinomial(prob_matrix, 100, replacement=True)\n                    #print(sampled_states)\n                    #print(sampled_states.shape)\n\n                # batch-size x seq_len x embedding_dim\n                # extra token is added at the end of the seq\n                for batch_idx, identifier in enumerate(pdb_ids):\n                    s_len = seq_lens[batch_idx]\n\n                    # save embeddings\n                    if save_per_residue_embeddings or save_per_protein_embeddings:\n                        try:\n                            # account for prefix in offset\n                            emb = embedding_repr.last_hidden_state[\n                                batch_idx, 1 : s_len + 1\n                            ]\n\n                            if save_per_residue_embeddings:\n                                batch_embeddings_per_residue[identifier] = (\n                                    emb.detach().cpu().numpy().squeeze()\n                                )\n\n                            if save_per_protein_embeddings:\n                                batch_embeddings_per_protein[identifier] = (\n                                    emb.mean(dim=0).detach().cpu().numpy().squeeze()\n                                )\n\n                        except:\n                            logger.warning(\n                                f\"Saving embeddings failed for {identifier}\"\n                            )\n\n                    # slice off padding and special token appended to the end of the sequence\n                    pred = prediction[batch_idx, :, 0:s_len].squeeze()\n\n                    # always return the mean probs\n                    mean_prob = round(\n                            100 * np.mean(probabilities[batch_idx, :, 0:s_len]), 2\n                        )\n\n                    if output_probs:  # if you want the per-residue probs\n                        all_prob = probabilities[batch_idx, :, 0:s_len]\n                        batch_predictions[identifier] = (\n                            pred,\n                            mean_prob,\n                            all_prob,\n                        )\n                    else:\n                        batch_predictions[identifier] = (pred, mean_prob, None)\n\n                    try:\n                        len(batch_predictions[identifier][0])\n                    except:\n                        logger.warning(\n                            f\"{identifier} prediction has length 0\"\n                        )\n                        fail_ids.append(identifier)\n                        continue\n\n                    if s_len != len(batch_predictions[identifier][0]):\n                        logger.warning(\n                            f\"Length mismatch for {identifier}: is:{len(batch_predictions[identifier][0])} vs should:{s_len}\"\n                        )\n\n                    for k in original_keys:\n                        if k in batch_predictions:\n                            predictions[k] = batch_predictions[k]\n\n                    if save_per_residue_embeddings:\n                        for k in original_keys:\n                            if k in batch_predictions:\n                                embeddings_per_residue[k] = batch_embeddings_per_residue[k]\n\n                    if save_per_protein_embeddings:\n                        for k in original_keys:\n                            if k in batch_predictions:\n                                embeddings_per_protein[k] = batch_embeddings_per_protein[k]\n\n            except IndexError:\n                logger.warning(\n                    \"Index error during prediction for {} (L={})\".format(\n                        pdb_id, seq_len\n                    )\n                )\n                for id in pdb_ids:\n                    fail_ids.append(id)\n                continue\n\n    # write list of fails if length &gt; 0\n    if len(fail_ids) &gt; 0:\n        fail_tsv: Path = Path(out_path) / \"fails.tsv\"\n\n        # Convert the list to a list of lists\n        data_as_list_of_lists = [[str(item)] for item in fail_ids]\n\n        # Write the list to a TSV file\n        with open(fail_tsv, \"w\", newline=\"\") as file:\n            tsv_writer = csv.writer(file, delimiter=\"\\t\")\n            tsv_writer.writerows(data_as_list_of_lists)\n\n    write_predictions(hypotheticals, predictions, output_3di,  mask_threshold, has_duplicate_locus)\n\n    if save_per_residue_embeddings:\n        write_embeddings(embeddings_per_residue, output_h5_per_residue)\n\n    if save_per_protein_embeddings:\n        write_embeddings(embeddings_per_protein, output_h5_per_protein)\n\n    # always write the mean embeddings\n    mean_probs_out_path: Path = (\n        Path(out_path) / f\"{prefix}_prostT5_3di_mean_probabilities.csv\"\n    )\n\n    # output per residue probs\n    if output_probs:\n        all_probs_out_path: Path = (\n            Path(out_path) / f\"{prefix}_prostT5_3di_all_probabilities.json\"\n        )\n    else:\n        all_probs_out_path = None\n\n    write_probs(predictions, mean_probs_out_path, all_probs_out_path, original_keys)\n\n    return predictions\n</code></pre>"},{"location":"reference/features/#src.baktfold.features.predict_3Di.load_predictor","title":"<code>load_predictor(checkpoint_path)</code>","text":"<p>Load a pre-trained CNN ProstT5 prediction head weights from a checkpoint file.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_path</code> <code>Union[str, Path]</code> <p>Path to the checkpoint file.</p> required <p>Returns:</p> Name Type Description <code>CNN</code> <code>CNN</code> <p>Loaded CNN model.</p> Source code in <code>src/baktfold/features/predict_3Di.py</code> <pre><code>def load_predictor(checkpoint_path: Union[str, Path]) -&gt; CNN:\n    \"\"\"\n    Load a pre-trained CNN ProstT5 prediction head weights from a checkpoint file.\n\n    Args:\n        checkpoint_path (Union[str, Path]): Path to the checkpoint file.\n\n    Returns:\n        CNN: Loaded CNN model.\n    \"\"\"\n\n    model = CNN()\n\n    # checkpoint_path = Path(CNN_DIR) / \"cnn_chkpnt\" / \"model.pt\"\n\n    state = torch.load(checkpoint_path, map_location=device)\n\n    # regular ProstT5 CNN \n    if checkpoint_path.suffix == '.pt':\n        model.load_state_dict(state[\"state_dict\"])\n    # finetuned\n    else:\n        model.load_state_dict(state)\n\n\n    model = model.eval()\n    model = model.to(device)\n\n    return model\n</code></pre>"},{"location":"reference/features/#src.baktfold.features.predict_3Di.toCPU","title":"<code>toCPU(tensor)</code>","text":"<p>Move a tensor to CPU and convert it to a NumPy array.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>torch.Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: NumPy array.</p> Source code in <code>src/baktfold/features/predict_3Di.py</code> <pre><code>def toCPU(tensor: torch.Tensor) -&gt; np.ndarray:\n    \"\"\"\n    Move a tensor to CPU and convert it to a NumPy array.\n\n    Args:\n        tensor (torch.Tensor): Input tensor.\n\n    Returns:\n        np.ndarray: NumPy array.\n    \"\"\"\n    if len(tensor.shape) &gt; 1:\n        return tensor.detach().cpu().squeeze(dim=-1).numpy()\n    else:\n        return tensor.detach().cpu().numpy()\n</code></pre>"},{"location":"reference/features/#src.baktfold.features.predict_3Di.write_embeddings","title":"<code>write_embeddings(embeddings, out_path)</code>","text":"<p>Write embeddings to an output file.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>Dict[str, Dict[str, Tuple[List[str], Any, Any]]]</code> <p>Predictions dictionary containing contig IDs, sequence IDs, predictions, and additional information.</p> required <code>out_path</code> <code>Path</code> <p>Path to the output file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/features/predict_3Di.py</code> <pre><code>def write_embeddings(\n    embeddings: Dict[str, Dict[str, Tuple[List[str], Any, Any]]],\n    out_path: Path,\n) -&gt; None:\n    \"\"\"\n    Write embeddings to an output file.\n\n    Args:\n        embeddings (Dict[str, Dict[str, Tuple[List[str], Any, Any]]]): Predictions dictionary containing contig IDs, sequence IDs, predictions, and additional information.\n        out_path (Path): Path to the output file.\n\n    Returns:\n        None\n    \"\"\"\n\n    with h5py.File(str(out_path), \"w\") as hf:\n        for sequence_id, embedding in embeddings.items():\n            hf.create_dataset(sequence_id, data=embedding)\n</code></pre>"},{"location":"reference/features/#src.baktfold.features.predict_3Di.write_predictions","title":"<code>write_predictions(hypotheticals, predictions, out_path, mask_threshold, has_duplicate_locus)</code>","text":"<p>Write predictions to an output file.</p> <p>Parameters:</p> Name Type Description Default <code>hypotheticals</code> <code>Dict</code> <p>Hypothetical protein feature dictionary from Bakta</p> required <code>predictions</code> <code>Dict[str, Tuple[List[str], Any, Any]]</code> <p>Predictions dictionary containing sequence IDs, predictions, and additional information.</p> required <code>out_path</code> <code>Path</code> <p>Path to the output file.</p> required <code>mask_threshold</code> <code>float</code> <p>between 0 and 100 - below this ProstT5 confidence, 3Di predictions are masked</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/features/predict_3Di.py</code> <pre><code>def write_predictions(\n    hypotheticals: Dict,\n    predictions: Dict[str, Tuple[List[str], Any, Any]],\n    out_path: Path,\n    mask_threshold: float,\n    has_duplicate_locus: bool\n) -&gt; None:\n    \"\"\"\n    Write predictions to an output file.\n\n    Args:\n        hypotheticals Dict: Hypothetical protein feature dictionary from Bakta\n        predictions (Dict[str,  Tuple[List[str], Any, Any]]): Predictions dictionary containing sequence IDs, predictions, and additional information.\n        out_path (Path): Path to the output file.\n        mask_threshold (float): between 0 and 100 - below this ProstT5 confidence, 3Di predictions are masked\n\n\n    Returns:\n        None\n    \"\"\"\n    ss_mapping = {\n        0: \"A\",\n        1: \"C\",\n        2: \"D\",\n        3: \"E\",\n        4: \"F\",\n        5: \"G\",\n        6: \"H\",\n        7: \"I\",\n        8: \"K\",\n        9: \"L\",\n        10: \"M\",\n        11: \"N\",\n        12: \"P\",\n        13: \"Q\",\n        14: \"R\",\n        15: \"S\",\n        16: \"T\",\n        17: \"V\",\n        18: \"W\",\n        19: \"Y\",\n        20: \"X\" # fully mask the low confidence 3Di residues with X not lower case (not working for Foldseek v10, but X does)\n    }\n\n    mask_prop_threshold = mask_threshold/100\n\n    # Filter out entries where the length of the value is 0\n    # Issue #47\n    predictions = {\n        k: v for k, v in predictions.items() if len(v[0]) &gt; 0\n    }\n\n\n    # mask low confidence residues\n    for seq_id, (pred, mean_prob, all_prob) in predictions.items():\n\n        # mask to X\n        for i in range(len(pred)):\n            if all_prob[0][i] &lt; mask_prop_threshold:\n                pred[i] = 20\n\n    with open(out_path, \"w+\") as out_f:\n        for feat in hypotheticals:\n            if has_duplicate_locus:\n                seq_id = feat[\"id\"]\n            else:\n                seq_id = feat[\"locus\"]\n            pred = predictions.get(seq_id)  # predictions = {seq_id: (yhats, _, _)} or None\n\n            if pred is not None:\n                yhats = pred[0]  \n                threedi_seq = \"\".join(ss_mapping[int(yhat)] for yhat in yhats)\n                feat[\"3di\"] = threedi_seq  # update the feature dictionary\n                out_f.write(f\"&gt;{seq_id}\\n{threedi_seq}\\n\")\n            else:\n                feat[\"3di\"] = None  # missing prediction\n\n    logger.info(f\"Finished writing results to {out_path}\")\n    return None\n</code></pre>"},{"location":"reference/features/#src.baktfold.features.predict_3Di.write_probs","title":"<code>write_probs(predictions, output_path_mean, output_path_all, original_keys)</code>","text":"<p>Write all ProstT5 encoder + CNN probabilities and mean probabilities to output files.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>Dict[str, Tuple[int, float, Union[int, np.ndarray]]]</code> required <code>output_path_mean</code> <code>str</code> <p>Path to the output file for mean probabilities.</p> required <code>output_path_all</code> <code>str</code> <p>Path to the output file for all probabilities.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/features/predict_3Di.py</code> <pre><code>def write_probs(\n    predictions: Dict[str, Tuple[int, float, Union[int, np.ndarray]]],\n    output_path_mean: Path,\n    output_path_all: Path,\n    original_keys: list[str],\n) -&gt; None:\n    \"\"\"\n    Write all ProstT5 encoder + CNN probabilities and mean probabilities to output files.\n\n    Args:\n        predictions (Dict[str, Tuple[int, float, Union[int, np.ndarray]]]):\n        Predictions dictionary containing  sequence IDs, probabilities, and additional information.\n        output_path_mean (str): Path to the output file for mean probabilities.\n        output_path_all (str): Path to the output file for all probabilities.\n\n    Returns:\n        None\n    \"\"\"\n\n    with open(output_path_mean, \"w+\") as out_f:\n        for seq_id in original_keys:\n            if seq_id not in predictions:\n                logger.warning(f\"Missing ProstT5 mean confidence for {seq_id}\")\n                continue\n            _, mean_prob, _ = predictions[seq_id]\n            out_f.write(f\"{seq_id},{mean_prob}\\n\")\n\n    if output_path_all is not None:\n        with open(output_path_all, \"w+\") as out_f:\n            for seq_id in original_keys:\n                if seq_id not in predictions:\n                    logger.warning(f\"Missing ProstT5 confidence for {seq_id}\")\n                    continue\n\n                _, _, all_probs = predictions[seq_id]\n\n                # convert to percentage\n                all_probs = all_probs * 100\n\n                # flatten\n                if isinstance(all_probs, np.ndarray):\n                    all_probs_list = all_probs.flatten().tolist()\n                else:\n                    all_probs_list = all_probs\n\n                rounded_list = [round(num, 2) for num in all_probs_list]\n\n                json_data = json.dumps(\n                    {\"seq_id\": seq_id, \"probability\": rounded_list}\n                )\n                out_f.write(json_data + \"\\n\")\n</code></pre>"},{"location":"reference/features/#src.baktfold.features.autotune.autotune_batching_real_data","title":"<code>autotune_batching_real_data(model_dir, model_name, cpu, threads, probe_seqs, start_bs=1, max_bs=100, step=5)</code>","text":"<p>Autotunes the batch size for a given model and set of sequences.</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>str</code> <p>The directory where the model is stored.</p> required <code>model_name</code> <code>str</code> <p>The name of the model.</p> required <code>cpu</code> <code>bool</code> <p>Whether to use the CPU or not.</p> required <code>threads</code> <code>int</code> <p>The number of threads to use.</p> required <code>probe_seqs</code> <code>list</code> <p>A list of sequences to use for probing.</p> required <code>start_bs</code> <code>int</code> <p>The starting batch size to use.</p> <code>1</code> <code>max_bs</code> <code>int</code> <p>The maximum batch size to use.</p> <code>100</code> <code>step</code> <code>int</code> <p>The step size to use when increasing the batch size.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>int</code> <p>The optimal batch size.</p> <code>int</code> <p>The maximum number of residues per batch.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; autotune_batching_real_data(\"model_dir\", \"model_name\", True, 4, [\"ATCG\", \"GCTA\"], 1, 100, 5)\n(10, 100)\n</code></pre> Source code in <code>src/baktfold/features/autotune.py</code> <pre><code>def autotune_batching_real_data(\n    model_dir,\n    model_name,\n    cpu,\n    threads,\n    probe_seqs,\n    start_bs=1,\n    max_bs=100,\n    step=5 # step size\n):\n    \"\"\"\n    Autotunes the batch size for a given model and set of sequences.\n\n    Args:\n      model_dir (str): The directory where the model is stored.\n      model_name (str): The name of the model.\n      cpu (bool): Whether to use the CPU or not.\n      threads (int): The number of threads to use.\n      probe_seqs (list): A list of sequences to use for probing.\n      start_bs (int): The starting batch size to use.\n      max_bs (int): The maximum batch size to use.\n      step (int): The step size to use when increasing the batch size.\n\n    Returns:\n      int: The optimal batch size.\n      int: The maximum number of residues per batch.\n\n    Examples:\n      &gt;&gt;&gt; autotune_batching_real_data(\"model_dir\", \"model_name\", True, 4, [\"ATCG\", \"GCTA\"], 1, 100, 5)\n      (10, 100)\n    \"\"\"\n\n    model, tokenizer = get_T5_model(model_dir, model_name, cpu, threads)\n    model.eval()\n    model.half()\n\n    bs = start_bs\n    results = []\n\n    # get device\n    device = None\n\n    if cpu is True:\n        device = torch.device(\"cpu\")\n    else:\n        # check for NVIDIA/cuda\n        if torch.cuda.is_available():\n            device = torch.device(\"cuda:0\")\n        # check for apple silicon/metal\n        elif torch.backends.mps.is_available():\n            device = torch.device(\"mps\")\n        else:\n            device = torch.device(\"cpu\")\n\n\n    while bs &lt;= max_bs:\n        try:\n\n            # seqs = probe_seqs\n            n_tokens = sum(len(s) for s in probe_seqs)\n\n            logger.info(f\"Running with batch size {bs}\")\n\n            model.eval()\n\n            total_tokens = 0\n            total_time = 0.0\n            batches = 0\n\n            # iterate over real sequences in batches\n            for i in tqdm(range(0, len(probe_seqs), bs), desc=\"Processing\"):\n                batch_seqs = probe_seqs[i : i + bs]\n\n                n_tokens = sum(len(s) for s in batch_seqs)\n                total_tokens += n_tokens\n\n                inputs = tokenizer(\n                    batch_seqs,\n                    padding=True,\n                    return_tensors=\"pt\",\n                )\n                inputs.pop(\"token_type_ids\", None)\n                inputs = {k: v.to(device) for k, v in inputs.items()}\n\n                # timing\n                torch.cuda.synchronize()\n                t0 = time.perf_counter()\n                with torch.no_grad():\n                    _ = model(**inputs)\n                torch.cuda.synchronize()\n\n                total_time += time.perf_counter() - t0\n\n                batches += 1\n\n            time_per_token = total_time / total_tokens\n\n\n            token_per_batch = math.floor(total_tokens / batches)\n\n\n            results.append({\n                \"bs\": bs,\n                \"tokens_per_batch\": token_per_batch,\n                \"time\": total_time,\n                \"time_per_token\": time_per_token,\n            })\n\n            logger.info(f\"Time elapsed {round(total_time,5)}\")\n            logger.info(f\"Tokens per batch {token_per_batch}\")\n\n            bs += step\n\n        except torch.cuda.OutOfMemoryError:\n            torch.cuda.empty_cache()\n            break\n\n\n    if not results:\n        raise RuntimeError(\"No batch size fits on this GPU\")\n\n    best_entry = min(results, key=lambda x: x[\"time_per_token\"])\n\n    best_bs = best_entry[\"bs\"]\n    best_residues = best_entry[\"tokens_per_batch\"]\n    # best_tpt = best_bs[\"time_per_token\"]\n\n    logger.info(f\"##########################\")\n    logger.info(f\"Best batch size: {best_bs}\")\n    # logger.info(f\"best max residues: {best_residues}\")\n\n    return best_bs, best_residues\n</code></pre>"},{"location":"reference/features/#src.baktfold.features.autotune.run_autotune","title":"<code>run_autotune(input_path, model_dir, model_name, cpu, threads, step, min_batch, max_batch, sample_seqs)</code>","text":"<p>Runs the batch size autotuning process.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>The path to the input file.</p> required <code>model_dir</code> <code>str</code> <p>The directory where the model is stored.</p> required <code>model_name</code> <code>str</code> <p>The name of the model.</p> required <code>cpu</code> <code>bool</code> <p>Whether to use the CPU or not.</p> required <code>threads</code> <code>int</code> <p>The number of threads to use.</p> required <code>step</code> <code>int</code> <p>The step size to use when increasing the batch size.</p> required <code>min_batch</code> <code>int</code> <p>The minimum batch size to use.</p> required <code>max_batch</code> <code>int</code> <p>The maximum batch size to use.</p> required <code>sample_seqs</code> <code>int</code> <p>The number of sequences to sample for probing.</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>The optimal batch size.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; run_autotune(\"input_path\", \"model_dir\", \"model_name\", True, 4, 5, 1, 100, 10)\n10\n</code></pre> Source code in <code>src/baktfold/features/autotune.py</code> <pre><code>def run_autotune(    \n    input_path,\n    model_dir,\n    model_name,\n    cpu,\n    threads,\n    step, \n    min_batch,\n    max_batch, \n    sample_seqs):\n    \"\"\"\n    Runs the batch size autotuning process.\n\n    Args:\n      input_path (str): The path to the input file.\n      model_dir (str): The directory where the model is stored.\n      model_name (str): The name of the model.\n      cpu (bool): Whether to use the CPU or not.\n      threads (int): The number of threads to use.\n      step (int): The step size to use when increasing the batch size.\n      min_batch (int): The minimum batch size to use.\n      max_batch (int): The maximum batch size to use.\n      sample_seqs (int): The number of sequences to sample for probing.\n\n    Returns:\n      int: The optimal batch size.\n\n    Examples:\n      &gt;&gt;&gt; run_autotune(\"input_path\", \"model_dir\", \"model_name\", True, 4, 5, 1, 100, 10)\n      10\n    \"\"\"\n\n    # Dictionary to store the records\n    cds_dict = {}\n\n\n    with open_protein_fasta_file(input_path) as handle:  # handles gzip too\n        records = list(SeqIO.parse(handle, \"fasta\"))\n        if not records:\n            logger.warning(f\"No proteins were found in your input file {input_path}.\")\n            logger.error(\n                f\"Your input file {input_path} is likely not a amino acid FASTA file. Please check this.\"\n            )\n        for record in records:\n            prot_id = record.id\n            feature_location = FeatureLocation(0, len(record.seq))\n            # Seq needs to be saved as the first element in list hence the closed brackets [str(record.seq)]\n            seq_feature = SeqFeature(\n                feature_location,\n                type=\"CDS\",\n                qualifiers={\n                    \"ID\": record.id,\n                    \"description\": record.description,\n                    \"translation\": str(record.seq),\n                },\n            )\n\n            cds_dict[prot_id] = seq_feature\n\n    if not cds_dict:\n        logger.error(f\"Error: no AA protein sequences found in {input_path} file\")\n\n\n    seqs = []\n    for feat in cds_dict.values():\n        v = feat.qualifiers.get(\"translation\")\n        if v and isinstance(v, str):\n            seqs.append(v)\n\n    logger.info(\"Beginning batch size tuning\")\n    logger.info(f\"Using minimum batch size of 1 and maximum batch size of {max_batch}\")\n\n    # define the sampling\n\n    probe_seqs = sample_probe_sequences(seqs, n=sample_seqs)\n\n    batch_size, max_residues = autotune_batching_real_data(\n        model_dir,\n        model_name,\n        cpu,\n        threads,\n        probe_seqs,\n        start_bs=min_batch,\n        max_bs=max_batch,\n        step=step # step size\n    )\n\n    logger.info(f\"Optimal batch size is {batch_size} (residues per batch {max_residues})\")\n\n    return batch_size\n</code></pre>"},{"location":"reference/features/#src.baktfold.features.autotune.sample_probe_sequences","title":"<code>sample_probe_sequences(seqs, n=5000, seed=0)</code>","text":"<p>samples sequences</p> Source code in <code>src/baktfold/features/autotune.py</code> <pre><code>def sample_probe_sequences(seqs, n=5000, seed=0):\n    \"\"\"\n    samples sequences \n\n    \"\"\"\n\n    rng = random.Random(seed)\n\n    if n &gt;= len(seqs):\n        sampled = list(seqs)\n    else:\n        sampled = rng.sample(seqs, n)\n\n    # sort by sequence length\n    sampled.sort(key=len, reverse=True)\n\n    return sampled\n</code></pre>"},{"location":"reference/io/","title":"Io","text":"<p>Module for manipulating genbank files some taken from phynteny https://github.com/susiegriggo/Phynteny</p>"},{"location":"reference/io/#src.baktfold.io.insdc.move_product_to_note_if_exists","title":"<code>move_product_to_note_if_exists(qualifiers)</code>","text":"<p>If a 'product' qualifier exists, append it to 'note' and remove 'product'.</p> <p>Designed for the eukaryotic entries</p>"},{"location":"reference/io/#src.baktfold.io.insdc.move_product_to_note_if_exists--parameters","title":"Parameters","text":"dict <p>Feature qualifiers dictionary (values are usually lists).</p>"},{"location":"reference/io/#src.baktfold.io.insdc.move_product_to_note_if_exists--returns","title":"Returns","text":"<p>None     Modifies qualifiers in place.</p> Source code in <code>src/baktfold/io/insdc.py</code> <pre><code>def move_product_to_note_if_exists(qualifiers):\n    \"\"\"\n    If a 'product' qualifier exists, append it to 'note' and remove 'product'.\n\n    Designed for the eukaryotic entries\n\n    Parameters\n    ----------\n    qualifiers : dict\n        Feature qualifiers dictionary (values are usually lists).\n\n    Returns\n    -------\n    None\n        Modifies qualifiers in place.\n    \"\"\"\n    product = qualifiers.get(\"product\")\n    if not product:\n        return\n\n    # Ensure note exists and is a list\n    if \"note\" not in qualifiers:\n        qualifiers[\"note\"] = []\n\n    if isinstance(product, list):\n        qualifiers[\"note\"].extend(product)\n    else:\n        qualifiers[\"note\"].append(product)\n\n    qualifiers.pop(\"product\", None)\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.insdc.revise_dbxref_insdc","title":"<code>revise_dbxref_insdc(dbxrefs)</code>","text":"<p>Remove INSDC non-compliant DbXrefs.</p> Source code in <code>src/baktfold/io/insdc.py</code> <pre><code>def revise_dbxref_insdc(dbxrefs: Sequence[str]) -&gt; Tuple[Sequence[str], Sequence[str]]:\n    \"\"\"Remove INSDC non-compliant DbXrefs.\"\"\"\n    insdc_valid_dbxrefs = [bc.DB_XREF_UNIPROTKB, bc.DB_XREF_GO, bc.DB_XREF_PFAM, bc.DB_XREF_RFAM]\n    valid_dbxrefs = []\n    invalid_dbxrefs = []\n    for dbxref in dbxrefs:\n        if(dbxref.split(':')[0] in insdc_valid_dbxrefs):\n            valid_dbxrefs.append(dbxref)\n        else:\n            invalid_dbxrefs.append(dbxref)\n    return valid_dbxrefs, invalid_dbxrefs\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.insdc.revise_product_insdc","title":"<code>revise_product_insdc(product)</code>","text":"<p>Revise product name for INSDC compliant submissions</p> Source code in <code>src/baktfold/io/insdc.py</code> <pre><code>def revise_product_insdc(product: str):\n    \"\"\"Revise product name for INSDC compliant submissions\"\"\"\n\n    old_product = product\n    if(re.search(r'(uncharacteri[sz]ed)', product, flags=re.IGNORECASE)):  # replace putative synonyms)\n        product = re.sub(r'(uncharacteri[sz]ed)', 'putative', product, flags=re.IGNORECASE)\n        logger.info('fix product: replace putative synonyms. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    if(product.count('(') != product.count(')')):  # remove unbalanced parentheses\n        product = product.replace('(', '').replace(')', '')  # ToDo: find and replace only legend parentheses\n        logger.info('fix product: remove unbalanced parantheses. new=%s, old=%s', product, old_product)\n\n    old_product = product\n    if(product.count('[') != product.count(']')):  # remove unbalanced brackets\n        product = product.replace('[', '').replace(']', '')  # ToDo: find and replace only legend bracket\n        logger.info('fix product: remove unbalanced brackets. new=%s, old=%s', product, old_product)\n\n    return product\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.json_in.parse_json_input","title":"<code>parse_json_input(input_path, faa_path, all_proteins)</code>","text":"<p>Parses genome annotations from input JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to input JSON file.</p> required <code>faa_path</code> <code>str</code> <p>Path to output file for hypothetical proteins.</p> required <code>all_proteins</code> <code>bool</code> <p>Whether to keep all proteins or only hypothetical ones.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the data, features, and whether there are duplicate locus tags.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; parse_json_input('input.json', 'hypotheticals.faa', False)\n(data, features, False)\n</code></pre> Source code in <code>src/baktfold/io/json_in.py</code> <pre><code>def parse_json_input(input_path, faa_path, all_proteins):\n    \"\"\"\n    Parses genome annotations from input JSON file.\n\n    Args:\n      input_path (str): Path to input JSON file.\n      faa_path (str): Path to output file for hypothetical proteins.\n      all_proteins (bool): Whether to keep all proteins or only hypothetical ones.\n\n    Returns:\n      tuple: A tuple containing the data, features, and whether there are duplicate locus tags.\n\n    Examples:\n      &gt;&gt;&gt; parse_json_input('input.json', 'hypotheticals.faa', False)\n      (data, features, False)\n    \"\"\"\n\n    ############################################################################\n    # Checks and configurations\n    # - check parameters and setup global configuration\n    # - test database\n    # - test binary dependencies\n    ############################################################################\n\n    try:\n        if input_path == '':\n            raise ValueError('File path argument must be non-empty')\n        annotation_path = Path(input_path).resolve()\n        cfg.check_readability('annotation', annotation_path)\n        cfg.check_content_size('annotation', annotation_path)\n    except:\n        logger.error(f'ERROR: annotation file {annotation_path} not valid!')\n\n    #print(f'baktfold v{cfg.version}')\n\n    logger.info(f'Parsing genome annotations from input: {annotation_path}')\n    with xopen(str(annotation_path), threads=0) as fh:\n        data = json.load(fh)\n    features = data['features']\n    features_by_sequence = {seq['id']: [] for seq in data['sequences']}\n    for feature in data['features']:\n        seq_id = feature['sequence'] if 'sequence' in feature else feature['contig']  # &lt;1.10.0 compatibility\n        sequence_features = features_by_sequence.get(seq_id)\n        sequence_features.append(feature)\n\n    # keep all proteins\n    if all_proteins:\n        hypotheticals = [feat for feat in features if feat['type'] == bc.FEATURE_CDS ]\n    else:\n\n        hypotheticals = [feat for feat in features if feat['type'] == bc.FEATURE_CDS and 'hypothetical' in feat]\n\n\n    # check if dupe locus tags (euks can have multiple CDS same locus tag e.g. Cladocopium goreaui CAMXCT020000001.1)\n    seen_loci = set()\n    has_duplicate_locus = False\n\n    for feat in hypotheticals:\n        locus = feat['locus']\n        if locus in seen_loci:\n            has_duplicate_locus = True\n            logger.warning(\"Multiple CDS per locus tag were detected in your input JSON.\")\n            logger.warning(\"CDS id (which is unique) rather than locus tag will be used for ProstT5+Foldseek searches.\")\n            break\n        seen_loci.add(locus)\n\n    if has_duplicate_locus:\n        # write hypothetical proteins to file with id (not locus) as guaranteed exists and unique\n        with faa_path.open('wt') as fh:\n            for feat in hypotheticals:\n                fh.write(f\"&gt;{feat['id']}\\n{feat['aa']}\\n\")\n\n    else:\n        # write hypothetical proteins to file - almost always\n        with faa_path.open('wt') as fh:\n            for feat in hypotheticals:\n                fh.write(f\"&gt;{feat['locus']}\\n{feat['aa']}\\n\")\n\n    try:\n        genome_block = data.get(\"genome\")\n\n        if genome_block is None:\n            logger.error(\"No 'genome' block found in input JSON. Please check.\")\n            translation_table = None\n        else:\n            if \"translation_table\" not in genome_block:\n                logger.error(\"No translation table found in input JSON. Please check your input.\")\n            else:\n                raw_value = genome_block[\"translation_table\"]\n\n                try:\n                    translation_table = int(raw_value)\n                    logger.info(\n                        f\"Translation table {translation_table} detected from input JSON\"\n                    )\n\n                except (ValueError, TypeError):\n                    translation_table = str(raw_value)\n                    logger.warning(\n                        f\"Translation table '{raw_value}' is not an integer. \"\n                        f\"Parsing it as a string.\"\n                    )\n\n    except Exception as e:\n        logger.exception(\n            f\"Unexpected error while parsing translation table: {e}\"\n        )\n        translation_table = None\n\n    logger.info('Parsing complete')\n\n    return data, features, has_duplicate_locus, translation_table\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.prokka_gbk_to_json.build_bakta_sequence_entry","title":"<code>build_bakta_sequence_entry(rec)</code>","text":"<p>Convert a  SeqRecord into a Bakta-style sequence entry. Missing fields are filled with None.</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def build_bakta_sequence_entry(rec):\n    \"\"\"\n    Convert a  SeqRecord into a Bakta-style sequence entry.\n    Missing fields are filled with None.\n    \"\"\"\n\n    seq = str(rec.seq)\n\n    # -----------------------------------------\n    # Extract source feature qualifiers - genbank always has source field\n    # -----------------------------------------\n    source_feat = next((f for f in rec.features if f.type == \"source\"), None)\n\n    source_qualifiers = {}\n\n    # Defaults (None) for all fields\n    mol_type = None\n    organism = None\n    strain = None\n    db_xref = None\n    note = None\n\n    plasmid = None\n    chromosome = None\n    completeness_hint = None\n\n    if source_feat:\n        q = source_feat.qualifiers\n\n        mol_type = q.get(\"mol_type\", [None])[0]\n        organism = q.get(\"organism\", [None])[0]\n        strain = q.get(\"strain\", [None])[0]\n        note = q.get(\"note\", [None])[0]\n\n        if \"db_xref\" in q:\n            val = q[\"db_xref\"]\n            db_xref = val[0] if len(val) == 1 else val\n\n        plasmid = q.get(\"plasmid\", [None])[0]\n        chromosome = q.get(\"chromosome\", [None])[0]\n        completeness_hint = q.get(\"completeness\", [None])[0]\n\n    # -----------------------------------------\n    # Infer topology\n    # -----------------------------------------\n    topology = rec.annotations.get(\"topology\")\n    if topology not in {\"linear\", \"circular\"}:\n        topology = \"linear\"\n\n    # -----------------------------------------\n    # Infer type\n    # -----------------------------------------\n    if plasmid is not None or \"plasmid\" in rec.annotations:\n        seq_type = \"plasmid\"\n    elif chromosome is not None or \"chromosome\" in rec.annotations:\n        seq_type = \"chromosome\"\n    else:\n        seq_type = \"contig\"\n\n    # -----------------------------------------\n    # Infer completeness (conservative)\n    # -----------------------------------------\n    complete = False\n\n    if topology == \"circular\":\n        complete = True\n    elif completeness_hint is not None and completeness_hint.lower() == \"complete\":\n        complete = True\n    elif note and \"complete genome\" in note.lower():\n        complete = True\n\n    # -----------------------------------------\n    # Infer genetic codefor description\n    # -----------------------------------------\n    gcode = None\n\n    if \"genetic_code\" in rec.annotations:\n        gcode = rec.annotations[\"genetic_code\"]\n    elif \"gcode\" in rec.annotations:\n        gcode = rec.annotations[\"gcode\"]\n    elif source_feat and \"transl_table\" in source_feat.qualifiers:\n        gcode = source_feat.qualifiers[\"transl_table\"][0]\n\n    # Conservative fallback to 11 for prokka\n    if gcode is None:\n        gcode = 11 \n\n    description_parts = [\n        f\"[gcode={gcode}]\",\n        f\"[topology={topology}]\",\n    ]\n\n    description = \" \".join(description_parts)\n\n    # -----------------------------------------\n    # Build entry\n    # -----------------------------------------\n    entry = {\n        \"id\": rec.id,\n        \"description\": description,\n        \"nt\": seq,\n        \"length\": len(seq),\n        \"complete\": complete,\n        \"type\": seq_type,\n        \"topology\": topology,\n        \"simple_id\": rec.id,\n        \"orig_id\": rec.id,\n        \"orig_description\": None,\n    }\n\n    # -----------------------------------------\n    # Add source qualifiers if present\n    # -----------------------------------------\n    if organism is not None:\n        entry[\"organism\"] = organism\n    if mol_type is not None:\n        entry[\"mol_type\"] = mol_type\n    if strain is not None:\n        entry[\"strain\"] = strain\n    if db_xref is not None:\n        entry[\"db_xref\"] = db_xref\n    if note is not None:\n        entry[\"note\"] = note\n\n\n    # this is from bakta\n    # \"id\": \"contig_1\",\n    # \"description\": \"[gcode=11] [topology=linear]\",\n    # \"nt\": \"AT\"\n    # \"length\": 5165988,\n    # \"complete\": false,\n    # \"type\": \"contig\",\n    # \"topology\": \"linear\",\n    # \"simple_id\": \"contig_1\",\n    # \"orig_id\": \"GCF_002368115_000000000001\",\n    # \"orig_description\": \"\"\n\n    # Add source qualifiers only if they exist\n    if organism is not None:\n        entry[\"organism\"] = organism\n\n    if mol_type is not None:\n        entry[\"mol_type\"] = mol_type\n\n    if strain is not None:\n        entry[\"strain\"] = strain\n\n    if db_xref is not None:\n        entry[\"db_xref\"] = db_xref\n\n    if note is not None:\n        entry[\"note\"] = note\n\n    return entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.prokka_gbk_to_json.calc_genome_stats","title":"<code>calc_genome_stats(records)</code>","text":"<p>Compute correct genome stats (size, GC, N-ratio, N50, N90) for records from a multi-contig Prokka GenBank file.</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def calc_genome_stats(records):\n    \"\"\"\n    Compute correct genome stats (size, GC, N-ratio, N50, N90) for records from a multi-contig\n    Prokka GenBank file.\n    \"\"\"\n\n    if not records:\n        raise ValueError(\"No GenBank records found.\")\n\n    # lengths of all contigs\n    contig_lengths = [len(r.seq) for r in records]\n    total_length = sum(contig_lengths)\n\n    # concatenate sequences for global GC + N calculation\n    full_seq = \"\".join(str(r.seq) for r in records)\n\n    # GC as fraction (Bakta wants 0\u20131)\n    gc_perc = gc_fraction(full_seq)\n\n    # N-ratio\n    n_ratio = full_seq.count(\"N\") / total_length\n\n    # ---------- N50 / N90 ----------\n    sorted_lengths = sorted(contig_lengths, reverse=True)\n\n    def nx_metric(sorted_lens, total, threshold):\n        \"\"\"\n        Generic N{threshold} function.\n        threshold: 0.5 for N50, 0.9 for N90\n        \"\"\"\n        cutoff = total * threshold\n        running = 0\n        for l in sorted_lens:\n            running += l\n            if running &gt;= cutoff:\n                return l\n        return sorted_lens[-1]  # fallback (should not happen)\n\n    n50 = nx_metric(sorted_lengths, total_length, 0.5)\n    n90 = nx_metric(sorted_lengths, total_length, 0.9)\n\n    return {\n        \"size\": total_length,\n        \"gc\": gc_perc,\n        \"n_ratio\": n_ratio,\n        \"n50\": n50,\n        \"n90\": n90,\n        \"coding_ratio\": None  \n    }\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.prokka_gbk_to_json.convert_assembly_gap_feature","title":"<code>convert_assembly_gap_feature(feature, rec, id)</code>","text":"<p>Convert a Prokka GenBank assembly_gap feature to a simplified Bakta-style 'gap' feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The assembly_gap feature from the Prokka GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The full GenBank record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Simplified Bakta-style gap feature.</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def convert_assembly_gap_feature(feature, rec, id):\n    \"\"\"\n    Convert a Prokka GenBank assembly_gap feature to a simplified Bakta-style 'gap' feature.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The assembly_gap feature from the Prokka GBK.\n        rec: Bio.SeqRecord\n            The full GenBank record containing the sequence.\n\n    Returns:\n        dict: Simplified Bakta-style gap feature.\n    \"\"\"\n\n    # Coordinates (1-based)\n    start = int(feature.location.start) + 1\n    stop = int(feature.location.end)\n\n    qualifiers = feature.qualifiers\n\n    # Prokka may provide estimated_length but coordinates already give an exact span\n    est_len = qualifiers.get(\"estimated_length\", [None])[0]\n    if est_len is not None:\n        length = int(est_len)\n    else:\n        length = stop - start + 1  # fallback from coordinates\n\n    # Bakta always uses \".\" for strand on gaps\n    strand = \".\"\n\n    gap_entry = {\n        \"type\": \"gap\",\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"length\": length,\n        \"id\": id\n    }\n\n    return gap_entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.prokka_gbk_to_json.convert_cds_feature","title":"<code>convert_cds_feature(feature, seq_record, translation_table, id)</code>","text":"<p>Convert a Prokka CDS Biopython SeqFeature to a Bakta CDS JSON entry.</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def convert_cds_feature(feature, seq_record, translation_table, id):\n    \"\"\"\n    Convert a Prokka CDS Biopython SeqFeature to a Bakta CDS JSON entry.\n    \"\"\"\n\n    # ----------- Location info -----------\n    start = int(feature.location.start) + 1     # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)           # already one past in BioPython\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n\n    # frame: Bakta uses 1/2/3; Prokka codon_start is [\"1\",\"2\",\"3\"]\n    codon_start = int(feature.qualifiers.get(\"codon_start\", [\"1\"])[0])\n    frame = codon_start\n\n    # ----------- Basic qualifiers -----------\n    gene = feature.qualifiers.get(\"gene\", [None])[0]\n    product = feature.qualifiers.get(\"product\", [None])[0]\n\n    locus_tag = feature.qualifiers.get(\"locus_tag\", [None])[0]\n    locus = locus_tag\n\n    # ----------- Extract nucleotides -----------\n    nt_seq = feature.extract(seq_record.seq)\n    nt = str(nt_seq)\n\n    # ----------- Extract amino acids -----------\n    aa = feature.qualifiers.get(\"translation\", [\"\"])[0]\n\n    # Compute translation if Prokka didn't provide it\n    if not aa:\n        try:\n            aa = str(nt_seq.translate(table=translation_table, cds=True))\n        except Exception:\n            aa = \"\"\n\n    # ----------- aa MD5 hexdigest -----------\n    aa_hexdigest = hashlib.md5(aa.encode()).hexdigest()\n\n    # ----------- Hypothetical? -----------\n    hypothetical = product is None or \"hypothetical protein\" in product.lower()\n\n    # ----------- Compute protein stats -----------\n    seq_stats = None\n    if aa:\n        try:\n            analysed = ProteinAnalysis(aa)\n            seq_stats = {\n                \"molecular_weight\": analysed.molecular_weight(),\n                \"isoelectric_point\": analysed.isoelectric_point()\n            }\n        except Exception:\n            seq_stats = None\n\n    # ----------- Make Bakta-format dict -----------\n    bakta_cds = {\n        \"type\": \"cds\",\n        \"sequence\": seq_record.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"frame\": frame,\n        \"gene\": gene,\n        \"product\": product,\n        \"db_xrefs\": feature.qualifiers.get(\"db_xref\", [so.SO_CDS.id]),  # there will be no other db_xref \n        \"nt\": nt,\n        \"aa\": aa,\n        \"aa_hexdigest\": aa_hexdigest,\n        \"start_type\": None,\n        \"rbs_motif\": None,\n        \"genes\": [],\n        \"seq_stats\": seq_stats,\n        \"id\": id,\n        \"locus\": locus,\n    }\n\n    if hypothetical:\n        bakta_cds[\"hypothetical\"] = True\n\n    return bakta_cds\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.prokka_gbk_to_json.convert_misc_rna_feature","title":"<code>convert_misc_rna_feature(feature, rec, id)</code>","text":"<p>Convert a Prokka GenBank misc_RNA (nc_rna) feature to a Bakta-style feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The misc_RNA feature from the Prokka GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Bakta-style misc_RNA feature.</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def convert_misc_rna_feature(feature, rec, id):\n    \"\"\"\n    Convert a Prokka GenBank misc_RNA (nc_rna) feature to a Bakta-style feature.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The misc_RNA feature from the Prokka GBK.\n        rec: Bio.SeqRecord\n            The record containing the sequence.\n\n    Returns:\n        dict: Bakta-style misc_RNA feature.\n    \"\"\"\n\n    seq = str(rec.seq)\n\n    # Coordinates (GBK is 0-based, Bakta is 1-based)\n    start = int(feature.location.start) + 1\n    stop = int(feature.location.end)\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n\n    qualifiers = feature.qualifiers\n\n    # Fields that Prokka may or may not include\n    gene = qualifiers.get(\"gene\", [None])[0]\n    product = qualifiers.get(\"product\", [None])[0]\n    locus_tag = qualifiers.get(\"locus_tag\", [None])[0]\n\n    # If gene missing, use product (Bakta often fills 'gene' for sRNA/tmRNA/etc.)\n    if gene is None:\n        gene = product\n\n    # Extract nucleotide sequence\n    nt_seq = seq[start-1:stop]\n    if strand == \"-\":\n        comp = str.maketrans(\"ACGTacgt\", \"TGCAtgca\")\n        nt_seq = nt_seq.translate(comp)[::-1]\n\n    misc_entry = {\n        \"type\": \"ncRNA\", # bakta uses ncRNA -&gt; will be misc_rna in Prokka\n        \"class\": None,             # bakta's classes are not in Prokka\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"gene\": gene,\n        \"product\": product,\n        \"score\": None,\n        \"evalue\": None,\n        \"db_xrefs\": [so.SO_NCRNA_GENE.id],        \n        \"nt\": nt_seq,\n        \"id\": id,\n        \"locus\": locus_tag\n    }\n\n    return misc_entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.prokka_gbk_to_json.convert_repeat_region_feature","title":"<code>convert_repeat_region_feature(feature, rec, id)</code>","text":"<p>Convert a Prokka GenBank repeat_region (CRISPR) feature to a simplified Bakta-style feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The repeat_region feature (crispr) from the Prokka GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The full GenBank record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Simplified Bakta-style CRISPR feature.</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def convert_repeat_region_feature(feature, rec, id):\n    \"\"\"\n    Convert a Prokka GenBank repeat_region (CRISPR) feature to a simplified Bakta-style feature.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The repeat_region feature (crispr) from the Prokka GBK.\n        rec: Bio.SeqRecord\n            The full GenBank record containing the sequence.\n\n    Returns:\n        dict: Simplified Bakta-style CRISPR feature.\n    \"\"\"\n\n    # Coordinates (Bakta uses 1-based)\n    start = int(feature.location.start) + 1\n    stop = int(feature.location.end)\n\n    qualifiers = feature.qualifiers\n    note = qualifiers.get(\"note\", [None])[0]\n    rpt_family = qualifiers.get(\"rpt_family\", [None])[0]\n    rpt_type = qualifiers.get(\"rpt_type\", [None])[0]\n    rpt_unit_seq = qualifiers.get(\"rpt_unit_seq\", [None])[0]\n\n    strand = \"?\"\n\n    # always just take the positive strand to get the NT seq (crispr repeat region)\n    seq =  str(rec.seq)\n    nt_seq = seq[start-1:stop]\n\n    # Minimal Bakta-like CRISPR structure\n    crispr_entry = {\n        \"type\": \"crispr\",\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand, # matches Bakta and is required\n        \"family\": rpt_family,       # e.g., \"CRISPR\"\n        \"rpt_type\": rpt_type,       # e.g., \"direct\"\n        \"repeat_unit\": rpt_unit_seq, # the actual consensus repeat\n        \"product\": note, # won't be the same as Bakta as different lookup method used - but needed for the gff writing\n        \"nt\": nt_seq, # needed for batka .ffn writeout\n        \"id\": id, # bakta_id needed \n        # \"locus\": None, # no locus tag like Bakta\n        \"db_xrefs\": [so.SO_CRISPR.id]\n    }\n\n    return crispr_entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.prokka_gbk_to_json.convert_rrna_feature","title":"<code>convert_rrna_feature(feature, rec, id)</code>","text":"<p>Convert a Prokka GenBank rRNA feature to Bakta-style JSON.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The rRNA feature from the Prokka GBK.</p> required <code>rec</code> <p>str The record from the GBK.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Bakta-style rRNA feature</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def convert_rrna_feature(feature, rec, id):\n    \"\"\"\n    Convert a Prokka GenBank rRNA feature to Bakta-style JSON.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The rRNA feature from the Prokka GBK.\n        rec: str\n            The record from the GBK.\n    Returns:\n        dict: Bakta-style rRNA feature\n    \"\"\"\n    start = int(feature.location.start) + 1  # GBK is 0-based\n    stop = int(feature.location.end)\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n\n    qualifiers = feature.qualifiers\n    product = qualifiers.get(\"product\", [None])[0]\n    locus_tag = qualifiers.get(\"locus_tag\", [None])[0]\n\n    # Infer gene type from product if possible\n    gene_map = {\n        \"16S ribosomal RNA\": \"rrs\",\n        \"23S ribosomal RNA\": \"rrl\",\n        \"5S ribosomal RNA\": \"rrf\"\n    }\n    gene = gene_map.get(product, None)\n\n    so_map = {\n        \"16S ribosomal RNA\": so.SO_RRNA_16S.id,\n        \"23S ribosomal RNA\": so.SO_RRNA_23S.id,\n        \"5S ribosomal RNA\": so.SO_RRNA_5S.id\n    }\n\n    specific_so = so_map.get(product, None)\n\n    contig_seq = str(rec.seq)\n\n    nt_seq = contig_seq[start-1:stop]\n    if strand == \"-\":\n        comp = str.maketrans(\"ACGTacgt\", \"TGCAtgca\")\n        nt_seq = nt_seq.translate(comp)[::-1]\n\n    rrna_entry = {\n        \"type\": \"rRNA\",\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"gene\": gene,\n        \"product\": product,\n        \"coverage\": None,  # Prokka does not provide\n        \"score\": None,     # Prokka does not provide\n        \"evalue\": None,    # Prokka does not provide\n        \"db_xrefs\": [so.SO_RRNA.id, specific_so], \n        \"nt\": nt_seq,\n        \"id\": id,\n        \"locus\": locus_tag\n    }\n\n    return rrna_entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.prokka_gbk_to_json.convert_tmrna_feature","title":"<code>convert_tmrna_feature(feature, rec, id)</code>","text":"<p>Convert a Prokka GenBank tmRNA feature to Bakta-style feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The tmRNA feature from the Prokka GBK.</p> required <code>rec</code> <p>str The record from the GBK</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Bakta-style tmRNA feature</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def convert_tmrna_feature(feature, rec, id):\n    \"\"\"\n    Convert a Prokka GenBank tmRNA feature to Bakta-style feature.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The tmRNA feature from the Prokka GBK.\n        rec: str\n            The record from the GBK\n\n    Returns:\n        dict: Bakta-style tmRNA feature\n    \"\"\"\n\n    seq =  str(rec.seq)\n\n    start = int(feature.location.start) + 1  # GBK is 0-based\n    stop = int(feature.location.end)\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n\n    qualifiers = feature.qualifiers\n    gene = qualifiers.get(\"gene\", [None])[0]\n    locus_tag = qualifiers.get(\"locus_tag\", [None])[0]\n    product = qualifiers.get(\"product\", [None])[0]\n\n    # Extract the nucleotide sequence of the tmRNA\n    nt_seq = seq[start-1:stop]\n    if strand == \"-\":\n        comp = str.maketrans(\"ACGTacgt\", \"TGCAtgca\")\n        nt_seq = nt_seq.translate(comp)[::-1]\n\n\n\n    tmrna_entry = {\n        \"type\": \"tmRNA\",\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"gene\": gene,\n        \"product\": product,\n        \"db_xrefs\": [so.SO_TMRNA.id], \n        # \"tag\": tag_info,  no tag in tmrna for prokka - no information on it in the output\n        \"nt\": nt_seq,\n        \"id\": id,\n        \"locus\": locus_tag\n    }\n\n    return tmrna_entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.prokka_gbk_to_json.convert_trna_feature","title":"<code>convert_trna_feature(feature, seq_record, id)</code>","text":"<p>Convert a Prokka tRNA SeqFeature to a Bakta tRNA JSON entry.</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def convert_trna_feature(feature, seq_record, id):\n    \"\"\"\n    Convert a Prokka tRNA SeqFeature to a Bakta tRNA JSON entry.\n    \"\"\"\n\n    # ------------ Location ------------\n    start = int(feature.location.start) + 1\n    stop  = int(feature.location.end)\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n\n    # ------------ Extract nt sequence ------------\n    nt_seq = feature.extract(seq_record.seq)\n    nt = str(nt_seq)\n\n    # ------------ Basic qualifiers ------------\n    product = feature.qualifiers.get(\"product\", [None])[0]\n    locus = feature.qualifiers.get(\"locus_tag\", [None])[0]\n\n    # ------------ amino acid ------------\n    # Prokka product examples:\n    #   \"tRNA-Trp\"\n    #   \"tRNA-Leu\"\n    amino_acid = None\n    if product and product.startswith(\"tRNA-\"):\n        amino_acid = product.split(\"-\")[1]\n\n\n    # ------------ anticodon ------------\n    anti_codon = None\n\n    # anticodons are in notes\n\n    notes = feature.qualifiers.get(\"note\", [])\n\n    # Expect a note like: \"tRNA-Ser(gga)\"\n    for note in notes:\n        # Remove spaces for safety\n        n = note.replace(\" \", \"\")\n\n        # Extract part inside parentheses (anticodon)\n        if \"(\" in n and \")\" in n:\n            anti_codon = n.split(\"(\")[1].split(\")\")[0].lower()\n\n        # Extract amino acid:\n        # tRNA-Ser(gga) \u2192 \"Ser\"\n        if \"tRNA-\" in n:\n            try:\n                # tRNA-Ser(gga) \u2192 \"Ser(gga)\" \u2192 split('(')[0] \u2192 \"Ser\"\n                aa_section = n.split(\"tRNA-\")[1]\n                aa_clean = aa_section.split(\"(\")[0]\n                amino_acid = aa_clean\n            except Exception:\n                pass\n\n    # ------------ Anti-codon position detection ------------\n    # Prokka doesnt have it - dont include\n    # anti_codon_pos = None\n\n    # ------------ score ------------\n    # nothing in prokka\n    score = None\n\n    # ------------ db_xrefs ------------\n    # doesnt exist for prokka\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [])\n    # add so_term\n    so_term = AMINO_ACID_DICT.get(amino_acid.lower(), ('', None))[1]\n\n    if (so_term):\n        db_xrefs.append(so_term.id)\n\n    # ------------ final Bakta-form dict ------------\n    bakta_trna = {\n        \"type\": \"tRNA\",\n        \"sequence\": seq_record.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"gene\": \"trn\" + (amino_acid[0].lower() if amino_acid else \"?\"),\n        \"product\": product,\n        \"amino_acid\": amino_acid,\n        \"anti_codon\": anti_codon,\n        \"score\": score,\n        \"nt\": nt,\n        \"db_xrefs\": db_xrefs,\n       #  \"anti_codon_pos\": anti_codon_pos,  dont include, not in prokka output\n        \"id\": id,\n        \"locus\": locus\n    }\n\n    return bakta_trna\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.prokka_gbk_to_json.get_bakta_style_id_from_locus_tag","title":"<code>get_bakta_style_id_from_locus_tag(records)</code>","text":"<p>Gets 10 char bakta-style ID tag based off the 8 char locus tag in first CDS on the first Prokka record + 2 random chars</p> <p>Assumes all records will have the same locus tag prefix</p> <p>Will always add 2 chars to make ID unique vs locus tag</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def get_bakta_style_id_from_locus_tag(records):\n    \"\"\"\n    Gets 10 char bakta-style ID tag based off the 8 char locus tag in first CDS on the first Prokka record + 2 random chars\n\n    Assumes all records will have the same locus tag prefix\n\n    Will always add 2 chars to make ID unique vs locus tag\n    \"\"\"\n\n    if not records:\n        raise ValueError(\"No GenBank records found.\")\n\n    for record in records:\n\n        for feat in record.features:\n            if feat.type == \"CDS\":\n                locus_tag_list = feat.qualifiers.get(\"locus_tag\") # returns None if doesn't exist\n\n                if locus_tag_list:\n                    locus_tag = locus_tag_list[0]\n\n                    if len(locus_tag) &gt; 6:\n\n                        locus_tag_prefix = locus_tag[:-6] # trims off _00001 from CDS\n\n                        rand_two_chars = random_n_letter_id(2)\n\n                        # by default prokka locus tag is 8 chars. So this returns a 10 char string (same as bakta defaults)\n\n                        id_tag = f\"{locus_tag_prefix}{rand_two_chars}\"\n\n                        return id_tag\n\n\n                    else:\n                        return random_n_letter_id(10)\n\n                # fallback if locus_tag missing or too short\n                return random_n_letter_id(10)\n\n    # No CDS feature found at all (shouldn't happen)\n    return random_n_letter_id(10)\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.prokka_gbk_to_json.get_transl_table","title":"<code>get_transl_table(records)</code>","text":"<p>Gets translation table based off the first CDS on the first record</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def get_transl_table(records):\n    \"\"\"\n    Gets translation table based off the first CDS on the first record\n    \"\"\"\n\n    if not records:\n        raise ValueError(\"No GenBank records found.\")\n\n    record_1 = records[0]\n\n    for feat in record_1.features:\n        if feat.type == \"CDS\":\n            # Translation table may be string \u2192 convert to int\n            transl = feat.qualifiers.get(\"transl_table\", [\"11\"])[0]\n            try:\n                return int(transl)\n            except ValueError:\n                return 11\n\n        # If no CDS found, default to 11\n        return 11\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.prokka_gbk_to_json.parse_prokka_version","title":"<code>parse_prokka_version(record)</code>","text":"<p>Extract Prokka version from COMMENT field: Example COMMENT: 'Annotated using prokka 1.14.6 ...'</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def parse_prokka_version(record):\n    \"\"\"\n    Extract Prokka version from COMMENT field:\n    Example COMMENT:\n    'Annotated using prokka 1.14.6 ...'\n    \"\"\"\n\n    comments = record.annotations.get(\"comment\", \"\") or record.annotations.get(\"comments\", \"\")\n    if not comments:\n        return \"unknown\"\n\n    m = re.search(r\"[Pp]rokka[\\s_]?v?(\\d+\\.\\d+\\.\\d+)\", comments)\n    if m:\n        return m.group(1)\n\n    # fallback pattern\n    m = re.search(r\"prokka[^0-9]*(\\d+\\.\\d+\\.\\d+)\", comments)\n    if m:\n        return m.group(1)\n\n    return \"unknown\"\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.prokka_gbk_to_json.prokka_gbk_to_json","title":"<code>prokka_gbk_to_json(records, output_json)</code>","text":"<p>Convert Prokka-generated GenBank SeqRecord objects into a Bakta-style JSON annotation file.</p> <p>This function takes one or more Biopython SeqRecord objects (typically parsed from a Prokka GenBank file) and reconstructs a JSON structure following the Bakta output schema. It extracts genome metadata, statistics, annotated features, nucleotide sequences, and Prokka version information. Features are converted to Bakta-compatible dictionaries and sorted in the same order Bakta expects.</p> The JSON file contains <p>\u2022 <code>genome</code> block \u2013 high-level organism metadata (genus, species, strain, etc.) \u2022 <code>stats</code> block \u2013 genome statistics derived from all records \u2022 <code>features</code> block \u2013 all features converted to Bakta-style objects, sorted   by feature type and genomic position \u2022 <code>sequences</code> block \u2013 contig/sequence entries in Bakta format \u2022 <code>run</code> block \u2013 timestamps and duration placeholder \u2022 <code>version</code> block \u2013 Prokka version and database metadata</p>"},{"location":"reference/io/#src.baktfold.io.prokka_gbk_to_json.prokka_gbk_to_json--parameters","title":"Parameters","text":"list of SeqRecord <p>A list of Biopython SeqRecord objects already parsed from a Prokka GenBank file. Must contain at least one record. The COMMENT field is expected to contain Prokka metadata.</p> str <p>Path to the output JSON file to be written.</p>"},{"location":"reference/io/#src.baktfold.io.prokka_gbk_to_json.prokka_gbk_to_json--returns","title":"Returns","text":"<p>bool     True if the JSON was successfully written.</p>"},{"location":"reference/io/#src.baktfold.io.prokka_gbk_to_json.prokka_gbk_to_json--raises","title":"Raises","text":"<p>ValueError     If <code>records</code> is empty.</p>"},{"location":"reference/io/#src.baktfold.io.prokka_gbk_to_json.prokka_gbk_to_json--notes","title":"Notes","text":"<p>\u2022 Features are processed in a fixed Bakta-like order:     [\"tRNA\", \"tmRNA\", \"rRNA\", \"misc_RNA\", \"repeat_region\", \"CDS\", \"assembly_gap\"] \u2022 Feature IDs are generated in Bakta-style using the locus tag prefix. \u2022 Per-contig sorting is performed by genomic start coordinate. \u2022 Runtime values in the <code>run</code> block are placeholders (duration = \"0.00 min\"). \u2022 The function does not validate that the GenBank file truly originates from   Prokka; that should be checked beforehand.</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def prokka_gbk_to_json(records, output_json):\n    \"\"\"\n    Convert Prokka-generated GenBank SeqRecord objects into a Bakta-style JSON\n    annotation file.\n\n    This function takes one or more Biopython SeqRecord objects (typically parsed\n    from a Prokka GenBank file) and reconstructs a JSON structure following the\n    Bakta output schema. It extracts genome metadata, statistics, annotated features,\n    nucleotide sequences, and Prokka version information. Features are converted\n    to Bakta-compatible dictionaries and sorted in the same order Bakta expects.\n\n    The JSON file contains:\n      \u2022 `genome` block \u2013 high-level organism metadata (genus, species, strain, etc.)\n      \u2022 `stats` block \u2013 genome statistics derived from all records\n      \u2022 `features` block \u2013 all features converted to Bakta-style objects, sorted\n        by feature type and genomic position\n      \u2022 `sequences` block \u2013 contig/sequence entries in Bakta format\n      \u2022 `run` block \u2013 timestamps and duration placeholder\n      \u2022 `version` block \u2013 Prokka version and database metadata\n\n    Parameters\n    ----------\n    records : list of SeqRecord\n        A list of Biopython SeqRecord objects already parsed from a Prokka GenBank\n        file. Must contain at least one record. The COMMENT field is expected to\n        contain Prokka metadata.\n\n    output_json : str\n        Path to the output JSON file to be written.\n\n    Returns\n    -------\n    bool\n        True if the JSON was successfully written.\n\n    Raises\n    ------\n    ValueError\n        If `records` is empty.\n\n    Notes\n    -----\n    \u2022 Features are processed in a fixed Bakta-like order:\n        [\"tRNA\", \"tmRNA\", \"rRNA\", \"misc_RNA\", \"repeat_region\", \"CDS\", \"assembly_gap\"]\n    \u2022 Feature IDs are generated in Bakta-style using the locus tag prefix.\n    \u2022 Per-contig sorting is performed by genomic start coordinate.\n    \u2022 Runtime values in the `run` block are placeholders (duration = \"0.00 min\").\n    \u2022 The function does not validate that the GenBank file truly originates from\n      Prokka; that should be checked beforehand.\n    \"\"\"\n\n    complete = False\n\n    # records = list(SeqIO.parse(genbank_path, \"genbank\"))\n\n    if len(records) == 0:\n        raise ValueError(\"No GenBank records found.\")\n    elif len(records) &gt;= 1:\n        prokka_version = parse_prokka_version(records[0])\n\n    translation_table = get_transl_table(records)\n\n    bakta_id_prefix = get_bakta_style_id_from_locus_tag(records)\n\n    # ----------------------------\n    # Genome block \n    # ----------------------------\n\n    genome_block = {\n        \"genus\": None,\n        \"species\": None,\n        \"strain\": None,\n        \"taxon\": None,\n        \"complete\": True,\n        \"gram\": \"?\",\n        \"translation_table\": translation_table\n    }\n\n    # ----------------------------\n    # Stats - on whole GBK\n    # ----------------------------\n    stats_block = calc_genome_stats(records)\n\n    # ----------------------------\n    # Features block\n    # ----------------------------\n\n    # order by id creation block to match how ids are generated bakta\n    ORDER = [\"tRNA\", \"tmRNA\", \"rRNA\", \"misc_RNA\", \"repeat_region\", \"CDS\", \"assembly_gap\"]\n    # bakta has oriC detection too - prokka doesn't I think so leaving it out \n\n    features = []\n    i = 1\n    for rec in records:\n        for ftype in ORDER:\n            for feat in rec.features:\n                if feat.type != ftype:\n                    continue\n\n                id = f\"{bakta_id_prefix}_{i}\"\n\n                if ftype == \"CDS\":\n                    features.append(convert_cds_feature(feat, rec, translation_table, id))\n                elif ftype == \"tRNA\":\n                    features.append(convert_trna_feature(feat, rec, id))\n                elif ftype == \"tmRNA\":\n                    features.append(convert_tmrna_feature(feat, rec, id))\n                elif ftype == \"rRNA\":\n                    features.append(convert_rrna_feature(feat, rec, id))\n                elif ftype == \"misc_RNA\":\n                    features.append(convert_misc_rna_feature(feat, rec, id))\n                elif ftype == \"repeat_region\":\n                    features.append(convert_repeat_region_feature(feat, rec, id))\n                elif ftype == \"assembly_gap\":\n                    features.append(convert_assembly_gap_feature(feat, rec, id))\n\n                i +=1\n\n\n    # ----------------------------\n    # Sort features within each contig like Bakta\n    # ----------------------------\n\n    features_by_contig = defaultdict(list)\n    for f in features:\n        features_by_contig[f[\"sequence\"]].append(f)\n\n    # Sort each contig's features by start and flatten back\n    sorted_features = []\n    for contig_id in features_by_contig:\n        sorted_features.extend(sorted(features_by_contig[contig_id], key=lambda x: x[\"start\"]))\n\n    # Replace the original features list\n    features = sorted_features\n\n    # ----------------------------\n    # Sequences block\n    # ----------------------------\n\n    sequences = []\n    for rec in records:\n        sequences.append(build_bakta_sequence_entry(rec))\n\n    # just to put in a time\n    start_time = datetime.now()\n\n    bakta_json = {\n        \"genome\": genome_block,\n        \"stats\": stats_block,\n        \"features\": features,\n        \"sequences\": sequences,\n        \"run\": {\n            \"start\": start_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"end\": start_time.strftime(\"%Y-%m-%d %H:%M:%S\"),       \n            \"duration\": \"0.00 min\"   \n        },\n        \"version\": {\n            \"prokka\": prokka_version,  \n            \"db\": {\n                \"version\": prokka_version,\n                \"type\": \"prokka_dbs\"\n            }\n        }\n    }\n\n\n    Path(output_json).parent.mkdir(parents=True, exist_ok=True)\n    with open(output_json, \"w\") as fh:\n        json.dump(bakta_json, fh, indent=4)\n        complete = True\n\n    return complete\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.prokka_gbk_to_json.random_n_letter_id","title":"<code>random_n_letter_id(n=4)</code>","text":"<p>generates a n letter id prefix </p> <p>n=2 to append to  Prokka locus tag  for bakta id to make it different n=10 if the locus tag is somehow missing (should never happen)</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def random_n_letter_id(n=4):\n    \"\"\"\n    generates a n letter id prefix \n\n    n=2 to append to  Prokka locus tag  for bakta id to make it different\n    n=10 if the locus tag is somehow missing (should never happen) \n    \"\"\"\n    return ''.join(random.choices(string.ascii_uppercase, k=n))\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.handle_genbank.get_fasta_run_pyrodigal_gv","title":"<code>get_fasta_run_pyrodigal_gv(input, threads)</code>","text":"<p>Check if a file is in the nucleotide FASTA format. If so, run pyrodigal-gv and convert the CDS to a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Path</code> <p>Path to the FASTA file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the CDS in the FASTA file.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided file is not a FASTA file.</p> Source code in <code>src/baktfold/io/handle_genbank.py</code> <pre><code>def get_fasta_run_pyrodigal_gv(input: Path, threads: int) -&gt; dict:\n    \"\"\"\n\n    Check if a file is in the nucleotide FASTA format. If so, run pyrodigal-gv and convert the CDS to a dictionary.\n\n    Args:\n        input (Path): Path to the FASTA file.\n\n    Returns:\n        dict: A dictionary representation of the CDS in the FASTA file.\n\n    Raises:\n        ValueError: If the provided file is not a FASTA file.\n    \"\"\"\n\n    if is_gzip_file(input.strip()):\n        try:\n            with gzip.open(input.strip(), \"rt\") as handle:\n                # gb_dict = SeqIO.to_dict(SeqIO.parse(handle, \"gb\"))\n                fasta_dict = SeqIO.to_dict(SeqIO.parse(handle, \"fasta\"))\n        except ValueError:\n            logger.warning(f\"{input} is not a FASTA file\")\n            logger.error(\n                f\"Your input {input} is neither Genbank nor FASTA format. Please check your input\"\n            )\n            raise\n    else:\n        try:\n            with open(input.strip(), \"rt\") as handle:\n                fasta_dict = SeqIO.to_dict(SeqIO.parse(handle, \"fasta\"))\n        except ValueError:\n            logger.warning(f\"{input} is not a FASTA file\")\n            logger.error(\n                f\"Your input {input} is neither Genbank nor FASTA format. Please check your input\"\n            )\n            raise\n\n    # then run pyrodigal\n\n    gb_dict = {}\n\n    orf_finder = pyrodigal_gv.ViralGeneFinder(meta=True)\n\n    def _find_genes(record):\n        \"\"\"\n    Finds all genes in a given SeqRecord.\n\n    Args:\n      record (SeqRecord): The SeqRecord to search for genes in.\n\n    Returns:\n      list: A list of SeqFeatures representing the genes found in the SeqRecord.\n\n    Examples:\n      &gt;&gt;&gt; _find_genes(SeqRecord(seq=Seq('ATGC'), id='example', name='example', description='example', dbxrefs=[]))\n      [SeqFeature(FeatureLocation(ExactPosition(0), ExactPosition(3), strand=1), type='gene', qualifiers={'locus_tag': ['example_1']}), ...]\n    \"\"\"\n        genes = orf_finder.find_genes(str(record.seq))\n        return (record.id, record.seq, genes)\n\n    def run_pool(pool, records):\n        \"\"\"\n    Runs a multiprocessing pool to process a list of SeqRecords.\n\n    Args:\n      records (list): A list of SeqRecords to process.\n      func (function): The function to apply to each SeqRecord.\n      num_processes (int): The number of processes to use in the pool. Defaults to the number of CPUs on the system.\n\n    Returns:\n      list: A list of results from applying the function to each SeqRecord.\n\n    Examples:\n      &gt;&gt;&gt; run_pool([SeqRecord(seq=Seq('ATGC'), id='example', name='example', description='example', dbxrefs=[]), ...], _find_genes, 4)\n      [[SeqFeature(FeatureLocation(ExactPosition(0), ExactPosition(3), strand=1), type='gene', qualifiers={'locus_tag': ['example_1']}), ...], ...]\n    \"\"\"\n        for record_id, record_seq, genes in pool.imap(_find_genes, records):\n            i = 0\n            all_features = []\n            for gene in genes:\n                i += 1\n                location = FeatureLocation(\n                    start=gene.begin, end=gene.end, strand=gene.strand\n                )\n                feature = SeqFeature(location, type=\"CDS\")\n                counter = \"{:04d}\".format(i)\n                cds_id = f\"{record_id}_CDS_\" + counter\n                feature.qualifiers[\"ID\"] = cds_id\n                feature.qualifiers[\"function\"] = \"unknown function\"\n                feature.qualifiers[\"product\"] = \"hypothetical protein\"\n                feature.qualifiers[\"phrog\"] = \"No_PHROG\"\n                feature.qualifiers[\"source\"] = (\n                    f\"Pyrodigal-gv_{pyrodigal_gv.__version__}\"\n                )\n                feature.qualifiers[\"transl_table\"] = gene.translation_table\n                # from the API\n                # translation_table (int, optional) \u2013 An alternative translation table to use to translate the gene.\n                # Use None (the default) to translate using the translation table this gene was found with.\n                feature.qualifiers[\"translation\"] = gene.translate(\n                    include_stop=False\n                ).upper()\n                all_features.append(feature)\n\n            seq_record = SeqIO.SeqRecord(\n                seq=Seq(record_seq), id=record_id, description=\"\", features=all_features\n            )\n            gb_dict[record_id] = seq_record\n\n        return gb_dict\n\n    with multiprocessing.pool.ThreadPool(threads) as pool:\n        if is_gzip_file(input.strip()):\n            with gzip.open(input.strip(), \"rt\") as handle:\n                records = SeqIO.parse(handle, \"fasta\")\n                gb_dict = run_pool(pool, records)\n        else:\n            with open(input.strip(), \"rt\") as handle:\n                records = SeqIO.parse(handle, \"fasta\")\n                gb_dict = run_pool(pool, records)\n\n    return gb_dict\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.handle_genbank.get_genbank","title":"<code>get_genbank(genbank)</code>","text":"<p>Convert a GenBank file to a dictionary.</p> <p>This function reads a GenBank file and converts it into a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>genbank</code> <code>Path</code> <p>Path to the GenBank file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the GenBank file.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided file is not a GenBank file.</p> Source code in <code>src/baktfold/io/handle_genbank.py</code> <pre><code>def get_genbank(genbank: Path) -&gt; dict:\n    \"\"\"\n    Convert a GenBank file to a dictionary.\n\n    This function reads a GenBank file and converts it into a dictionary.\n\n    Args:\n        genbank (Path): Path to the GenBank file.\n\n    Returns:\n        dict: A dictionary representation of the GenBank file.\n\n    Raises:\n        ValueError: If the provided file is not a GenBank file.\n    \"\"\"\n\n    logger.info(f\"Checking if input {genbank} is a Genbank format file\")\n    logger.info(f\"If so, also detecting the likely input style out of Pharokka, Bakta and NCBI Refseq style.\")\n    def parse_records(handle):\n        \"\"\"\n    Parses a genbank file and returns a list of SeqRecords.\n\n    Args:\n      file_path (str): The path to the genbank file to parse.\n      file_format (str): The format of the genbank file. Defaults to 'genbank'.\n\n    Returns:\n      list: A list of SeqRecords parsed from the genbank file.\n\n    Examples:\n      &gt;&gt;&gt; parse_records('example.gb')\n      [SeqRecord(seq=Seq('ATGC'), id='example', name='example', description='example', dbxrefs=[]), ...]\n    \"\"\"\n        try:\n            records = list(SeqIO.parse(handle, \"gb\"))\n            if not records:\n                return {}, None\n            gb_dict = {record.id: record for record in records}\n            record = records[0]\n\n            comment = record.annotations.get(\"comment\", \"\")\n            cds_feature = next((f for f in record.features if f.type == \"CDS\"), None)\n\n            if cds_feature is None:\n                logger.error(f\"{genbank} appears to be a Genbank formatted file but no CDS was found. Please check your input.\")\n                return gb_dict, None\n\n            # Check if 'Bakta' appears in the Comment - will appear there\n            if \"Bakta\" in comment and \"locus_tag\" in cds_feature.qualifiers:\n                logger.info(f\"Detected Bakta style input Genbank. Using locus_tag qualifier from Bakta as the CDS IDs for Phold.\")\n                method = \"Bakta\"\n            else:\n                if \"phrog\" not in cds_feature.qualifiers and \"protein_id\" in cds_feature.qualifiers:\n                    logger.info(f\"Detected NCBI Refseq style input Genbank. Using protein_id qualifier as the CDS IDs for Phold.\")\n                    method = \"NCBI\"\n                elif \"phrog\" in cds_feature.qualifiers and \"ID\" in cds_feature.qualifiers:\n                    logger.info(f\"Detected Pharokka style input Genbank. Using ID qualifier from Pharokka as the CDS IDs for Phold.\")\n                    method = \"Pharokka\"\n                else:\n                    logger.error(\n                                f\"Feature {cds_feature} could not be parsed. Therefore, the input style format for {genbank} could not be detected. Please check your input.\"\n                            )\n            return identify_long_ids(gb_dict), method\n        except Exception as e:\n            logger.warning(f\"{genbank} is not a genbank file\")\n            return {}, None\n\n    try:\n        if is_gzip_file(genbank.strip()):\n            with gzip.open(genbank.strip(), \"rt\") as handle:\n                return parse_records(handle)\n        else:\n            with open(genbank.strip(), \"rt\") as handle:\n                return parse_records(handle)\n    except Exception as e:\n        logger.warning(f\"{genbank} is not a genbank file\")\n        return {}, None\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.handle_genbank.get_proteins","title":"<code>get_proteins(fasta)</code>","text":"<p>Convert an Amino Acid FASTA file to a dictionary.</p> <p>This function reads a AA FASTA file and converts it into a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fasta</code> <code>Path</code> <p>Path to the FASTA file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the FASTA file.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided file is not a FASTA file.</p> Source code in <code>src/baktfold/io/handle_genbank.py</code> <pre><code>def get_proteins(fasta: Path) -&gt; dict:\n    \"\"\"\n    Convert an Amino Acid FASTA file to a dictionary.\n\n    This function reads a AA FASTA file and converts it into a dictionary.\n\n    Args:\n        fasta (Path): Path to the FASTA file.\n\n    Returns:\n        dict: A dictionary representation of the FASTA file.\n\n    Raises:\n        ValueError: If the provided file is not a FASTA file.\n    \"\"\"\n\n    if is_gzip_file(fasta.strip()):\n        try:\n            fasta_dict = {}\n            with gzip.open(fasta.strip(), \"rt\") as handle:\n                sequence_id = \"\"\n                sequence = \"\"\n                for line in handle:\n                    line = line.strip()\n                    if line.startswith(\"&gt;\"):\n                        if sequence_id:\n                            fasta_dict[sequence_id] = sequence\n                        sequence_id = line[1:]\n                        sequence = \"\"\n                    else:\n                        sequence += line\n                if sequence_id:\n                    fasta_dict[sequence_id] = sequence\n            handle.close()\n        except ValueError:\n            logger.error(f\"{fasta.strip()} is not a FASTA file!\")\n            raise\n\n    else:\n        try:\n            fasta_dict = {}\n            with open(fasta.strip(), \"rt\", errors=\"ignore\") as handle:\n                sequence_id = \"\"\n                sequence = \"\"\n                for line in handle:\n                    line = line.strip()\n                    if line.startswith(\"&gt;\"):\n                        if sequence_id:\n                            fasta_dict[sequence_id] = sequence\n                        sequence_id = line[1:]\n                        sequence = \"\"\n                    else:\n                        sequence += line\n                if sequence_id:\n                    fasta_dict[sequence_id] = sequence\n            handle.close()\n        except ValueError:\n            logger.error(f\"{fasta.strip()} is not a FASTA file!\")\n            raise\n\n    return fasta_dict\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.handle_genbank.identify_long_ids","title":"<code>identify_long_ids(gb_dict)</code>","text":"<p>Checks all feature IDs in gb_dict. If longer than 54 chars (line break from Pharokka/biopython reading GBK files), removes the space</p> <p>Parameters:</p> Name Type Description Default <code>dict</code> <p>A dictionary representation of the GenBank file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the GenBank file.</p> Source code in <code>src/baktfold/io/handle_genbank.py</code> <pre><code>def identify_long_ids(gb_dict: dict) -&gt; dict:\n    \"\"\"\n\n    Checks all feature IDs in gb_dict. If longer than 54 chars (line break from Pharokka/biopython reading GBK files), removes the space\n\n    Args:\n        dict: A dictionary representation of the GenBank file.\n\n    Returns:\n        dict: A dictionary representation of the GenBank file.\n    \"\"\"\n\n    # remove spaces in ID/locus tag\n    for record_id, record in gb_dict.items():\n        for cds_feature in record.features:\n            try:\n                # if pharokka &gt; 54 char IDs/locus tage, phold/biopython will parse with a space\n                # no spaces in\n                # for really long CDS IDs (over 54 chars), a space will be introduced\n                # this is because the ID will go over a second line\n                # weird bug noticed it on the Mgnify contigs annotated with Pharokka\n                cds_id = cds_feature.qualifiers[\"ID\"][0]\n                if len(cds_id) &gt;= 54:\n                    logger.warning(\n                        f\"The CDS ID is {cds_id} is longer than 54 characters. It is recommended that you use short contig headers (which will therefore lead to shorter CDS ids).\"\n                    )\n                    cds_feature.qualifiers[\"ID\"][0] = cds_feature.qualifiers[\"ID\"][\n                        0\n                    ].replace(\" \", \"\")\n            except:\n                # will be GenBank/NCBI formatted\n                # ID isn't a field and should be properly formatted - famous last words probably\n                continue\n\n    return gb_dict\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.handle_genbank.is_gzip_file","title":"<code>is_gzip_file(f)</code>","text":"<p>Method copied from Phispy see https://github.com/linsalrob/PhiSpy/blob/master/PhiSpyModules/helper_functions.py</p> <p>This is an elegant solution to test whether a file is gzipped by reading the first two characters. I also use a version of this in fastq_pair if you want a C version :) See https://stackoverflow.com/questions/3703276/how-to-tell-if-a-file-is-gzip-compressed for inspiration</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Path</code> <p>The file to test.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the file is gzip compressed, otherwise False.</p> Source code in <code>src/baktfold/io/handle_genbank.py</code> <pre><code>def is_gzip_file(f: Path) -&gt; bool:\n    \"\"\"\n    Method copied from Phispy see https://github.com/linsalrob/PhiSpy/blob/master/PhiSpyModules/helper_functions.py\n\n    This is an elegant solution to test whether a file is gzipped by reading the first two characters.\n    I also use a version of this in fastq_pair if you want a C version :)\n    See https://stackoverflow.com/questions/3703276/how-to-tell-if-a-file-is-gzip-compressed for inspiration\n    Args:\n        f (Path): The file to test.\n\n    Returns:\n        bool: True if the file is gzip compressed, otherwise False.\n    \"\"\"\n    with open(f, \"rb\") as i:\n        return binascii.hexlify(i.read(2)) == b\"1f8b\"\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.handle_genbank.open_protein_fasta_file","title":"<code>open_protein_fasta_file(input_file)</code>","text":"<p>Open a fasta file, whether it is gzipped or plain text.</p> <p>input_file (str): The path to the fasta file, either gzipped or plain.</p> <p>Union[IO[str], gzip.GzipFile]: A file handle to the opened fasta file.</p> Source code in <code>src/baktfold/io/handle_genbank.py</code> <pre><code>def open_protein_fasta_file(input_file: str) -&gt; Union[IO[str], gzip.GzipFile]:\n    \"\"\"\n    Open a fasta file, whether it is gzipped or plain text.\n\n    Parameters:\n    input_file (str): The path to the fasta file, either gzipped or plain.\n\n    Returns:\n    Union[IO[str], gzip.GzipFile]: A file handle to the opened fasta file.\n    \"\"\"\n    input_file = Path(input_file)\n\n    if input_file.suffix == \".gz\":\n        return gzip.open(input_file, \"rt\")\n    else:\n        return open(input_file, \"r\")\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.add_optional_qualifiers","title":"<code>add_optional_qualifiers(entry, qualifiers, single_valued=None, multi_valued=None)</code>","text":"<p>Add optional INSDC qualifiers to a feature entry dict in Bakta style.</p>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.add_optional_qualifiers--parameters","title":"Parameters","text":"dict <p>The feature dictionary being built.</p> dict <p>The qualifiers dictionary from Bio.SeqFeature.</p> set or list <p>Qualifiers expected to be single-valued (take the first if multiple).</p> set or list <p>Qualifiers that can have multiple values (keep as list if &gt;1, else single value).</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def add_optional_qualifiers(entry, qualifiers, single_valued=None, multi_valued=None):\n    \"\"\"\n    Add optional INSDC qualifiers to a feature entry dict in Bakta style.\n\n    Parameters\n    ----------\n    entry : dict\n        The feature dictionary being built.\n    qualifiers : dict\n        The qualifiers dictionary from Bio.SeqFeature.\n    single_valued : set or list\n        Qualifiers expected to be single-valued (take the first if multiple).\n    multi_valued : set or list\n        Qualifiers that can have multiple values (keep as list if &gt;1, else single value).\n    \"\"\"\n\n    single_valued = single_valued or set()\n    multi_valued = multi_valued or set()\n\n    # Multi-valued qualifiers\n    for key in multi_valued:\n        vals = qualifiers.get(key)\n        if vals:\n            entry[key] = vals if len(vals) &gt; 1 else vals[0]\n\n    # Single-valued qualifiers\n    for key in single_valued:\n        vals = qualifiers.get(key)\n        if vals:\n            if key == \"locus_tag\":\n                entry[\"locus\"] = vals[0] # this is what bakta needs\n            else:\n                entry[key] = vals[0]\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.build_bakta_sequence_entry","title":"<code>build_bakta_sequence_entry(rec)</code>","text":"<p>Convert a  SeqRecord into a Bakta-style sequence entry. Missing fields are filled with None.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def build_bakta_sequence_entry(rec):\n    \"\"\"\n    Convert a  SeqRecord into a Bakta-style sequence entry.\n    Missing fields are filled with None.\n    \"\"\"\n\n    seq = str(rec.seq)\n\n    # -----------------------------------------\n    # Extract source feature qualifiers - genbank always has source field\n    # -----------------------------------------\n    source_feat = next((f for f in rec.features if f.type == \"source\"), None)\n\n    source_qualifiers = {}\n\n    # Defaults (None) for all fields\n    mol_type = None\n    organism = None\n    strain = None\n    db_xref = None\n    note = None\n\n    plasmid = None\n    chromosome = None\n    completeness_hint = None\n\n    if source_feat:\n        q = source_feat.qualifiers\n\n        mol_type = q.get(\"mol_type\", [None])[0]\n        organism = q.get(\"organism\", [None])[0]\n        strain = q.get(\"strain\", [None])[0]\n        note = q.get(\"note\", [None])[0]\n\n        if \"db_xref\" in q:\n            val = q[\"db_xref\"]\n            db_xref = val[0] if len(val) == 1 else val\n\n        plasmid = q.get(\"plasmid\", [None])[0]\n        chromosome = q.get(\"chromosome\", [None])[0]\n        completeness_hint = q.get(\"completeness\", [None])[0]\n\n    # -----------------------------------------\n    # Infer topology\n    # -----------------------------------------\n    topology = rec.annotations.get(\"topology\")\n    if topology not in {\"linear\", \"circular\"}:\n        topology = \"linear\"\n\n    # -----------------------------------------\n    # Infer type\n    # -----------------------------------------\n    if plasmid is not None or \"plasmid\" in rec.annotations:\n        seq_type = \"plasmid\"\n    elif chromosome is not None or \"chromosome\" in rec.annotations:\n        seq_type = \"chromosome\"\n    else:\n        seq_type = \"contig\"\n\n    # -----------------------------------------\n    # Infer completeness (conservative)\n    # -----------------------------------------\n    complete = False\n\n    if topology == \"circular\":\n        complete = True\n    elif completeness_hint is not None and completeness_hint.lower() == \"complete\":\n        complete = True\n    elif note and \"complete genome\" in note.lower():\n        complete = True\n\n    # -----------------------------------------\n    # Infer genetic codefor description\n    # -----------------------------------------\n    gcode = None\n\n    if \"genetic_code\" in rec.annotations:\n        gcode = rec.annotations[\"genetic_code\"]\n    elif \"gcode\" in rec.annotations:\n        gcode = rec.annotations[\"gcode\"]\n    elif source_feat and \"transl_table\" in source_feat.qualifiers:\n        gcode = source_feat.qualifiers[\"transl_table\"][0]\n\n    # Conservative fallback to 1 for euks\n    if gcode is None:\n        gcode = 1 \n\n    description_parts = [\n        f\"[gcode={gcode}]\",\n        f\"[topology={topology}]\",\n    ]\n\n    description = \" \".join(description_parts)\n\n    # -----------------------------------------\n    # Build entry\n    # -----------------------------------------\n    entry = {\n        \"id\": rec.id,\n        \"description\": description,\n        \"nt\": seq,\n        \"length\": len(seq),\n        \"complete\": complete,\n        \"type\": seq_type,\n        \"topology\": topology,\n        \"simple_id\": rec.id,\n        \"orig_id\": rec.id,\n        \"orig_description\": None,\n    }\n\n    # -----------------------------------------\n    # Add source qualifiers if present\n    # -----------------------------------------\n    if organism is not None:\n        entry[\"organism\"] = organism\n    if mol_type is not None:\n        entry[\"mol_type\"] = mol_type\n    if strain is not None:\n        entry[\"strain\"] = strain\n    if db_xref is not None:\n        entry[\"db_xref\"] = db_xref\n    if note is not None:\n        entry[\"note\"] = note\n\n\n    # this is from bakta\n    # \"id\": \"contig_1\",\n    # \"description\": \"[gcode=11] [topology=linear]\",\n    # \"nt\": \"AT\"\n    # \"length\": 5165988,\n    # \"complete\": false,\n    # \"type\": \"contig\",\n    # \"topology\": \"linear\",\n    # \"simple_id\": \"contig_1\",\n    # \"orig_id\": \"GCF_002368115_000000000001\",\n    # \"orig_description\": \"\"\n\n    # Add source qualifiers only if they exist\n    if organism is not None:\n        entry[\"organism\"] = organism\n\n    if mol_type is not None:\n        entry[\"mol_type\"] = mol_type\n\n    if strain is not None:\n        entry[\"strain\"] = strain\n\n    if db_xref is not None:\n        entry[\"db_xref\"] = db_xref\n\n    if note is not None:\n        entry[\"note\"] = note\n\n    return entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.calc_genome_stats","title":"<code>calc_genome_stats(records)</code>","text":"<p>Compute correct genome stats (size, GC, N-ratio, N50, N90) for records from a multi-contig  GenBank file.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def calc_genome_stats(records):\n    \"\"\"\n    Compute correct genome stats (size, GC, N-ratio, N50, N90) for records from a multi-contig\n     GenBank file.\n    \"\"\"\n\n    if not records:\n        raise ValueError(\"No GenBank records found.\")\n\n    # lengths of all contigs\n    contig_lengths = [len(r.seq) for r in records]\n    total_length = sum(contig_lengths)\n\n    # concatenate sequences for global GC + N calculation\n    full_seq = \"\".join(str(r.seq) for r in records)\n\n    # GC as fraction (Bakta wants 0\u20131)\n    gc_perc = gc_fraction(full_seq)\n\n    # N-ratio\n    n_ratio = full_seq.count(\"N\") / total_length\n\n    # ---------- N50 / N90 ----------\n    sorted_lengths = sorted(contig_lengths, reverse=True)\n\n    def nx_metric(sorted_lens, total, threshold):\n        \"\"\"\n        Generic N{threshold} function.\n        threshold: 0.5 for N50, 0.9 for N90\n        \"\"\"\n        cutoff = total * threshold\n        running = 0\n        for l in sorted_lens:\n            running += l\n            if running &gt;= cutoff:\n                return l\n        return sorted_lens[-1]  # fallback (should not happen)\n\n    n50 = nx_metric(sorted_lengths, total_length, 0.5)\n    n90 = nx_metric(sorted_lengths, total_length, 0.9)\n\n    return {\n        \"size\": total_length,\n        \"gc\": gc_perc,\n        \"n_ratio\": n_ratio,\n        \"n50\": n50,\n        \"n90\": n90,\n        \"coding_ratio\": None  \n    }\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.convert_assembly_gap_feature","title":"<code>convert_assembly_gap_feature(feature, rec, id)</code>","text":"<p>Convert a GenBank assembly_gap feature to a simplified Bakta-style 'gap' feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The assembly_gap feature from the GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The full GenBank record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Simplified Bakta-style gap feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_assembly_gap_feature(feature, rec, id):\n    \"\"\"\n    Convert a GenBank assembly_gap feature to a simplified Bakta-style 'gap' feature.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The assembly_gap feature from the GBK.\n        rec: Bio.SeqRecord\n            The full GenBank record containing the sequence.\n\n    Returns:\n        dict: Simplified Bakta-style gap feature.\n    \"\"\"\n\n    # Coordinates (1-based)\n    strand = \".\" # bakta uses \".\" for strand on gaps\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    qualifiers = feature.qualifiers\n\n    #  may provide estimated_length but coordinates already give an exact span\n    est_len = qualifiers.get(\"estimated_length\", [None])[0]\n    if est_len is not None:\n        length = int(est_len)\n    else:\n        length = stop - start + 1  # fallback from coordinates\n\n\n    gap_entry = {\n        \"type\": \"gap\",\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"length\": length,\n        \"id\": id,\n    }\n\n    # no need to add estimated length separately - it is covered by length in the json \n\n    # if est_len:\n    #     gap_entry[\"estimated_length\"] = est_len\n\n    return gap_entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.convert_cds_feature","title":"<code>convert_cds_feature(feature, seq_record, translation_table, id)</code>","text":"<p>Convert a Prokka CDS Biopython SeqFeature to a Bakta CDS JSON entry.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_cds_feature(feature, seq_record, translation_table, id):\n    \"\"\"\n    Convert a Prokka CDS Biopython SeqFeature to a Bakta CDS JSON entry.\n    \"\"\"\n\n    # ----------- Location info -----------\n\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    # Handle CompoundLocation (join)\n    starts = None\n    stops = None\n\n    if feature.location.__class__.__name__ == \"CompoundLocation\":\n        starts = []\n        stops = []\n        for part in feature.location.parts:\n            starts.append(int(part.start) + 1)\n            stops.append(int(part.end))\n\n\n    # frame: Bakta uses 1/2/3; Prokka codon_start is [\"1\",\"2\",\"3\"]\n    codon_start = int(feature.qualifiers.get(\"codon_start\", [\"1\"])[0])\n    frame = codon_start\n\n    qualifiers = feature.qualifiers\n\n    # ----------- Basic qualifiers -----------\n    gene = qualifiers.get(\"gene\", [None])[0]\n    product = qualifiers.get(\"product\", [None])[0]\n\n\n    # fall back to start_stop_strand if there is no locus tag\n    if 'locus_tag' in qualifiers and qualifiers['locus_tag']:\n        locus_tag = qualifiers['locus_tag'][0]\n    else:\n        logger.warning(f\"No locus_tag found for feature {id}\")\n        locus_tag = f\"{GENOME_RANDOM_BACKUP_LOCUSTAG_STR}_{start}_{stop}\"\n        logger.warning(f\"Generating a locus_tag: {locus_tag}\")\n\n    note = qualifiers.get(\"note\", [None])[0]\n    locus = locus_tag\n\n    # pseudo\n\n    protein_id = qualifiers.get(\"protein_id\", [None])[0]\n\n    # ----------- Extract nucleotides -----------\n    nt_seq = feature.extract(seq_record.seq)\n    nt = str(nt_seq)\n\n    # ----------- Extract amino acids -----------\n    aa = feature.qualifiers.get(\"translation\", [\"\"])[0]\n\n    # Compute translation if Prokka didn't provide it\n    if not aa:\n        try:\n            aa = str(nt_seq.translate(table=translation_table, cds=True))\n        except Exception:\n            aa = \"\"\n\n    # ----------- aa MD5 hexdigest -----------\n    aa_hexdigest = hashlib.md5(aa.encode()).hexdigest()\n\n    # ----------- Hypothetical? -----------\n    hypothetical = product is None or \"hypothetical protein\" in product.lower()\n\n    # ----------- Compute protein stats -----------\n    seq_stats = None\n    if aa:\n        try:\n            analysed = ProteinAnalysis(aa)\n            seq_stats = {\n                \"molecular_weight\": analysed.molecular_weight(),\n                \"isoelectric_point\": analysed.isoelectric_point()\n            }\n        except Exception:\n            seq_stats = None\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xref = qualifiers.get(\"db_xref\", [so.SO_CDS.id])\n\n    # Append so.SO_CDS.id only if it\u2019s not already present\n    if so.SO_CDS.id not in db_xref:\n        db_xref.append(so.SO_CDS.id)\n\n    # ----------- Make Bakta-format dict -----------\n    bakta_cds = {\n        \"type\": \"cds\",\n        \"sequence\": seq_record.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"starts\": starts,\n        \"stops\": stops,\n        \"strand\": strand,\n        \"frame\": frame,\n        \"gene\": gene,\n        \"product\": product,\n        \"db_xrefs\": db_xref,  \n        \"nt\": nt,\n        \"aa\": aa,\n        \"aa_hexdigest\": aa_hexdigest,\n        \"start_type\": None,\n        \"rbs_motif\": None,\n        \"genes\": [],\n        \"note\": note,\n        \"seq_stats\": seq_stats,\n        \"id\": id,\n        \"locus\": locus,\n        \"protein_id\": protein_id\n    }\n\n# Feature Key           CDS\n\n# Definition            coding sequence; sequence of nucleotides that\n#                       corresponds with the sequence of amino acids in a\n#                       protein (location includes stop codon); \n#                       feature includes amino acid conceptual translation.\n\n# Optional qualifiers   /allele=\"text\"\n#                       /artificial_location=\"[artificial_location_value]\"\n#                       /circular_RNA\n#                       /codon_start=&lt;1 or 2 or 3&gt;\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /EC_number=\"text\"\n#                       /exception=\"[exception_value]\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /number=unquoted text (single token)\n#                       /old_locus_tag=\"text\" (single token)\n#                       /operon=\"text\"\n#                       /product=\"text\"\n#                       /protein_id=\"&lt;identifier&gt;\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /ribosomal_slippage\n#                       /standard_name=\"text\"\n#                       /translation=\"text\"\n#                       /transl_except=(pos:&lt;location&gt;,aa:&lt;amino_acid&gt;)\n#                       /transl_table =&lt;integer&gt;\n#                       /trans_splicing\n\n    multi_valued = {\"EC_number\", \"exception\", \"experiment\", \"function\",  \"gene_synonym\",  \"inference\", }\n    single_valued = {\"allele\", \"artificial_location\",  \"map\", \"number\",  \"old_locus_tag\", \"operon\", \"phenotype\", \"pseudogene\", \"standard_name\", \"transl_except\", \"transl_table\"}\n\n    add_optional_qualifiers(bakta_cds, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"circular_RNA\", \"pseudo\", \"ribosomal_slippage\", \"trans_splicing\"]:\n        if flag in qualifiers:\n            bakta_cds[flag] = flag in qualifiers\n\n    if hypothetical:\n        bakta_cds[\"hypothetical\"] = True\n\n    return bakta_cds\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.convert_exon_feature","title":"<code>convert_exon_feature(feature, rec, id)</code>","text":"<p>Convert a GenBank exon feature to a simplified Bakta-style 'exon' feature.</p>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.convert_exon_feature--parameters","title":"Parameters","text":"Bio.SeqFeature <p>The exon feature from the GenBank record.</p> Bio.SeqRecord <p>The full GenBank record.</p> str <p>Unique feature ID.</p>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.convert_exon_feature--returns","title":"Returns","text":"<p>dict     Bakta-style exon feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_exon_feature(feature, rec, id):\n    \"\"\"\n    Convert a GenBank exon feature to a simplified Bakta-style 'exon' feature.\n\n    Parameters\n    ----------\n    feature : Bio.SeqFeature\n        The exon feature from the GenBank record.\n    rec : Bio.SeqRecord\n        The full GenBank record.\n    id : str\n        Unique feature ID.\n\n    Returns\n    -------\n    dict\n        Bakta-style exon feature.\n    \"\"\"\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    qualifiers = feature.qualifiers\n\n    db_xrefs = qualifiers.get(\"db_xref\", [])\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /EC_number=\"text\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /number=unquoted text (single token)\n#                       /old_locus_tag=\"text\" (single token)\n#                       /product=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /standard_name=\"text\"\n#                       /trans_splicing\n\n\n    # Extract commonly used INSDC qualifiers\n    exon_entry = {\n            \"type\": \"exon\",\n            \"sequence\": rec.id,\n            \"start\": start,\n            \"stop\": stop,\n            \"strand\": strand,\n            \"id\": id,\n            \"db_xrefs\": db_xrefs\n        }\n\n    multi_valued = {\"EC_number\",\"experiment\",\"function\",  \"gene_synonym\",  \"inference\",\"note\" }\n    single_valued = {\"allele\", \"gene\", \"locus_tag\", \"map\", \"number\",   \"old_locus_tag\", \"operon\", \"pseudogene\", \"standard_name\"   }\n\n    add_optional_qualifiers(exon_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"pseudo\", \"trans_splicing\"]:\n        if flag in qualifiers:\n            exon_entry[flag] = True\n\n    return exon_entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.convert_gene_feature","title":"<code>convert_gene_feature(feature, rec, id)</code>","text":"<p>Convert a Funannotate GenBank gene feature to Bakta-style JSON.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The rRNA feature from the GBK.</p> required <code>rec</code> <p>str The record from the GBK.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Bakta-style rRNA feature</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_gene_feature(feature, rec, id):\n    \"\"\"\n    Convert a Funannotate GenBank gene feature to Bakta-style JSON.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The rRNA feature from the GBK.\n        rec: str\n            The record from the GBK.\n    Returns:\n        dict: Bakta-style rRNA feature\n    \"\"\"\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n\n    qualifiers = feature.qualifiers\n\n    # fall back to start_stop_strand if there is no locus tag\n    if 'locus_tag' in qualifiers and qualifiers['locus_tag']:\n        locus_tag = qualifiers['locus_tag'][0]\n    else:\n        logger.warning(f\"No locus_tag found for feature {id}\")\n        locus_tag = f\"{GENOME_RANDOM_BACKUP_LOCUSTAG_STR}_{start}_{stop}\"\n        logger.warning(f\"Generating a locus_tag: {locus_tag}\")\n\n\n\n    gene_entry = {\n        \"type\": \"gene\",\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"db_xrefs\": [so.SO_GENE.id], \n        \"id\": id,\n        \"locus\": locus_tag\n    }\n\n\n# Feature Key           gene \n\n\n# Definition            region of biological interest identified as a gene \n#                       and for which a name has been assigned;\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /operon=\"text\"\n#                       /product=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /phenotype=\"text\"\n#                       /standard_name=\"text\"\n#                       /trans_splicing\n\n\n# Comment               the gene feature describes the interval of DNA that \n#                       corresponds to a genetic trait or phenotype; the feature is,\n#                       by definition, not strictly bound to it's positions at the \n#                       ends;  it is meant to represent a region where the gene is \n#                       located.\n\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\" }\n    single_valued = {\"allele\",  \"map\",  \"old_locus_tag\", \"operon\", \"phenotype\", \"standard_name\"}\n\n    qualifiers = feature.qualifiers\n\n    add_optional_qualifiers(gene_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"pseudo\", \"trans_splicing\"]:\n        if flag in qualifiers:\n            gene_entry[flag] = flag in qualifiers\n\n    return gene_entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.convert_mat_peptide_feature","title":"<code>convert_mat_peptide_feature(feature, rec, id)</code>","text":"<p>Convert a mat_peptide feature to a Bakta-style feature.</p> <p>mus musculus chrom 1 NC_000067</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The mRNA feature from the GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Bakta-style misc_RNA feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_mat_peptide_feature(feature, rec, id):\n    \"\"\"\n    Convert a mat_peptide feature to a Bakta-style feature.\n\n    mus musculus chrom 1 NC_000067\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The mRNA feature from the GBK.\n        rec: Bio.SeqRecord\n            The record containing the sequence.\n\n    Returns:\n        dict: Bakta-style misc_RNA feature.\n    \"\"\"\n\n    seq = str(rec.seq)\n\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    # Handle CompoundLocation (join)\n    starts = None\n    stops = None\n\n    if feature.location.__class__.__name__ == \"CompoundLocation\":\n        starts = []\n        stops = []\n        for part in feature.location.parts:\n            starts.append(int(part.start) + 1)\n            stops.append(int(part.end))\n\n    so_code =  so.SO_MAT_PEPTIDE.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n    qualifiers = feature.qualifiers\n\n\n    # Extract commonly used INSDC qualifiers\n    mat_peptide_entry = {\n            \"type\": \"mat_peptide\",\n            \"sequence\": rec.id,\n            \"start\": start,\n            \"stop\": stop,\n            # Join support\n            \"starts\": starts,\n            \"stops\": stops,\n            \"strand\": strand,\n            \"id\": id,\n            \"db_xrefs\": db_xrefs\n        }\n\n\n# Feature Key           mat_peptide\n\n\n# Definition            mature peptide or protein coding sequence; coding\n#                       sequence for the mature or final peptide or protein\n#                       product following post-translational modification; the\n#                       location does not include the stop codon (unlike the\n#                       corresponding CDS);\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /EC_number=\"text\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /product=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /standard_name=\"text\"\n\n    multi_valued = {\"EC_number\",\"experiment\", \"function\",  \"gene_synonym\",  \"inference\",\"note\" }\n    single_valued = {\"allele\", \"gene\", \"locus_tag\", \"map\", \"number\",   \"old_locus_tag\", \"operon\", \"pseudogene\", \"standard_name\"}\n\n    add_optional_qualifiers(mat_peptide_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers) - no flags\n    # for flag in [\"pseudo\"]:\n    #     if flag in qualifiers:\n    #         mat_peptide_entry[flag] = True\n\n\n    #  mat_peptide     complement(join(194724303..194724321,194744661..194744721,\n    #                  194746996..194747031,194750435..194750476,\n    #                  194757818..194757865,194759962..194760144,\n    #                  194764890..194765087,194765856..194765944,\n    #                  194767641..194767743,194768400..194768583))\n    #                  /gene=\"Cd46\"\n    #                  /gene_synonym=\"Mcp\"\n    #                  /product=\"Membrane cofactor protein. /id=PRO_0000238971\"\n    #                  /note=\"propagated from UniProtKB/Swiss-Prot (O88174.1)\"\n\n    return mat_peptide_entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.convert_misc_feature","title":"<code>convert_misc_feature(feature, rec, id)</code>","text":"<p>Convert a misc feature to a Bakta-style feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The mRNA feature from the GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Bakta-style misc_feature feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_misc_feature(feature, rec, id):\n    \"\"\"\n    Convert a misc feature to a Bakta-style feature.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The mRNA feature from the GBK.\n        rec: Bio.SeqRecord\n            The record containing the sequence.\n\n    Returns:\n        dict: Bakta-style misc_feature feature.\n    \"\"\"\n\n    seq = str(rec.seq)\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    # Handle CompoundLocation (join)\n    starts = None\n    stops = None\n\n    if feature.location.__class__.__name__ == \"CompoundLocation\":\n        starts = []\n        stops = []\n        for part in feature.location.parts:\n            starts.append(int(part.start) + 1)\n            stops.append(int(part.end))\n\n    qualifiers = feature.qualifiers\n\n    so_code =  so.SO_MISC_REGION.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append so.SO_CDS.id only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n\n    misc_feature_entry = {\n            \"type\": \"misc_feature\",\n            \"sequence\": rec.id,\n            \"start\": start,\n            \"stop\": stop,\n            \"strand\": strand,\n            \"id\": id,\n\n            # Join support\n            \"starts\": starts,\n            \"stops\": stops,\n\n            # Multi-valued\n            \"db_xrefs\": db_xrefs,\n\n\n        }\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\", \"phenotype\"}\n    single_valued = {\"allele\", \"gene\", \"locus_tag\", \"map\",    \"old_locus_tag\", \"operon\", \"product\", \"standard_name\",  \"pseudogene\"}\n\n    add_optional_qualifiers(misc_feature_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"pseudo\",]:\n        if flag in qualifiers:\n            misc_feature_entry[flag] = True\n\n# Feature Key           misc_feature\n\n\n# Definition            region of biological interest which cannot be described\n#                       by any other feature key; a new or rare feature;\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /number=unquoted text (single token)\n#                       /old_locus_tag=\"text\" (single token)\n#                       /phenotype=\"text\"\n#                       /product=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /standard_name=\"text\"\n\n# Comment               this key should not be used when the need is merely to \n#                       mark a region in order to comment on it or to use it in \n#                       another feature's location\n\n    #  misc_feature    join(78488668..78488692,78499322..78499359)\n    #                  /gene=\"Mogat1\"\n    #                  /gene_synonym=\"0610030A14Rik; 1110064N14Rik; Dgat2l;\n    #                  Dgat2l1; mDC2; MGAT1; WI1-2612I11.1\"\n    #                  /note=\"propagated from UniProtKB/Swiss-Prot (Q91ZV4.2);\n    #                  transmembrane region\"\n\n    #  misc_feature    78179419..78180585\n    #                  /standard_name=\"Pax3 upstream hypaxial enhancer\"\n    #                  /note=\"Region: biological region; Derived by automated\n    #                  computational analysis using gene prediction method:\n    #                  RefSeqFE.\"\n    #                  /function=\"regulatory_interactions: LOC107980439 | Pax3\"\n    #                  /db_xref=\"GeneID:107980442\"    \n\n    return misc_feature_entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.convert_misc_rna_feature","title":"<code>convert_misc_rna_feature(feature, rec, id)</code>","text":"<p>Convert a GenBank misc_rna feature to a simplified Bakta-style 'misc_rna' feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The assembly_gap feature from the GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The full GenBank record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Simplified Bakta-style gap feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_misc_rna_feature(feature, rec, id):\n    \"\"\"\n    Convert a GenBank misc_rna feature to a simplified Bakta-style 'misc_rna' feature.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The assembly_gap feature from the GBK.\n        rec: Bio.SeqRecord\n            The full GenBank record containing the sequence.\n\n    Returns:\n        dict: Simplified Bakta-style gap feature.\n\n    \"\"\"\n\n        # from ensemble genomes\n        # misc_RNA        complement(437333..442742)\n        #             /gene=\"YPL060C-A\"\n        #             /note=\"transposable_element\"\n        #             /standard_name=\"YPL060C-A\"\n\n    # Coordinates (1-based)\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n\n    qualifiers = feature.qualifiers\n    gene = qualifiers.get(\"gene\", [None])[0]\n\n# Feature Key           misc_RNA\n\n\n# Definition            any transcript or RNA product that cannot be defined by\n#                       other RNA keys (prim_transcript, precursor_RNA, mRNA,\n#                       5'UTR, 3'UTR, exon, CDS, sig_peptide, transit_peptide,\n#                       mat_peptide, intron, polyA_site, ncRNA, rRNA and tRNA);\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /operon=\"text\"\n#                       /product=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /standard_name=\"text\"\n#                       /trans_splicing\n\n    misc_rna_entry = {\n        \"type\": \"misc_RNA\", # expects lowercase \n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand, # matches Bakta and is required\n        \"gene\": gene,\n        \"id\": id\n    }\n\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\" }\n    single_valued = {\"allele\", \"gene\",  \"locus_tag\", \"map\",  \"old_locus_tag\", \"operon\", \"product\", \"phenotype\", \"standard_name\"}\n\n    qualifiers = feature.qualifiers\n\n    add_optional_qualifiers(misc_rna_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"pseudo\", \"trans_splicing\"]:\n        if flag in qualifiers:\n            misc_rna_entry[flag] = flag in qualifiers\n\n    return misc_rna_entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.convert_mobile_element_feature","title":"<code>convert_mobile_element_feature(feature, rec, id)</code>","text":"<p>Convert a GenBank mobile_element feature to a Bakta-style feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_mobile_element_feature(feature, rec, id):\n    \"\"\"\n    Convert a GenBank mobile_element feature to a Bakta-style feature.\n    \"\"\"\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    qualifiers = feature.qualifiers\n\n    # Mandatory qualifier check (INSDC requirement)\n    mobile_element_type = qualifiers.get(\"mobile_element_type\", [None])[0]\n    if mobile_element_type is None:\n        raise ValueError(\n            f\"mobile_element feature {id} is missing mandatory \"\n            \"/mobile_element_type qualifier\"\n        )\n\n    so_code =  so.SO_MOBILE_ELEMENT.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n\n# Feature Key           mobile_element\n\n\n# Definition            region of genome containing mobile elements;\n\n# Mandatory qualifiers  /mobile_element_type=\"&lt;mobile_element_type&gt;\n#                       [:&lt;mobile_element_name&gt;]\"\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\" \n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /rpt_family=\"text\"\n#                       /rpt_type=&lt;repeat_type&gt;\n#                       /standard_name=\"text\"\n\n\n    # Extract commonly used INSDC qualifiers\n    mobile_element_entry = {\n            \"type\": \"mobile_element\",\n            \"sequence\": rec.id,\n            \"start\": start,\n            \"stop\": stop,\n            \"strand\": strand,\n            \"id\": id,\n            \"db_xrefs\": db_xrefs,\n                    # Mandatory\n            \"mobile_element_type\": mobile_element_type,\n        }\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\" }\n    single_valued = {\"allele\", \"gene\", \"locus_tag\", \"map\",    \"old_locus_tag\", \"standard_name\", \"rpt_family\", \"rpt_type\"}\n\n    add_optional_qualifiers(mobile_element_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    # for flag in [\"pseudo\"]:\n    #   if flag in qualifiers:\n    #     mobile_element_entry[flag] = True\n\n\n    #  mobile_element  57369551..57369723\n    #                  /note=\"Derived by automated computational analysis using\n    #                  gene prediction method: RefSeqFE.\"\n    #                  /mobile_element_type=\"SINE:AmnSINE1\"\n    #                  /db_xref=\"GeneID:106707176\"\n\n    return mobile_element_entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.convert_mrna_feature","title":"<code>convert_mrna_feature(feature, rec, id)</code>","text":"<p>Convert a funannotate mrna feature to a Bakta-style feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The mRNA feature from the GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Bakta-style mRNA feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_mrna_feature(feature, rec, id):\n    \"\"\"\n    Convert a funannotate mrna feature to a Bakta-style feature.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The mRNA feature from the GBK.\n        rec: Bio.SeqRecord\n            The record containing the sequence.\n\n    Returns:\n        dict: Bakta-style mRNA feature.\n    \"\"\"\n\n    # seq = str(rec.seq)\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    # Handle CompoundLocation (join)\n    starts = None\n    stops = None\n\n    if feature.location.__class__.__name__ == \"CompoundLocation\":\n        starts = []\n        stops = []\n        for part in feature.location.parts:\n            starts.append(int(part.start) + 1)\n            stops.append(int(part.end))\n\n    else:\n        starts = None\n        stops = None\n\n\n    qualifiers = feature.qualifiers\n\n\n    # fall back to start_stop_strand if there is no locus tag\n    if 'locus_tag' in qualifiers and qualifiers['locus_tag']:\n        locus_tag = qualifiers['locus_tag'][0]\n    else:\n        logger.warning(f\"No locus_tag found for feature {id}\")\n        locus_tag = f\"{GENOME_RANDOM_BACKUP_LOCUSTAG_STR}_{start}_{stop}\"\n        logger.warning(f\"Generating a locus_tag: {locus_tag}\")\n\n\n    mrna_entry = {\n        \"type\": \"mRNA\", \n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"starts\": starts,\n        \"stops\": stops,\n        \"strand\": strand,\n        \"db_xrefs\": [so.SO_MRNA.id],        \n        \"id\": id,\n        \"locus\": locus_tag\n    }\n\n\n\n# Feature Key           mRNA\n\n\n# Definition            messenger RNA; includes 5'untranslated region (5'UTR),\n#                       coding sequences (CDS, exon) and 3'untranslated region\n#                       (3'UTR);\n\n# Optional qualifiers   /allele=\"text\"\n#                       /artificial_location=\"[artificial_location_value]\"\n#                       /circular_RNA\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /operon=\"text\"\n#                       /product=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /standard_name=\"text\"\n#                       /trans_splicing\n\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\" }\n    single_valued = {\"allele\", \"artificial_location\", \"gene\",  \"locus_tag\", \"map\",  \"old_locus_tag\", \"operon\", \"phenotype\", \"product\", \"standard_name\"}\n\n    qualifiers = feature.qualifiers\n\n    add_optional_qualifiers(mrna_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"circular_RNA\", \"pseudo\", \"trans_splicing\"]:\n        if flag in qualifiers:\n            mrna_entry[flag] = flag in qualifiers\n\n    return mrna_entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.convert_ncrna_feature","title":"<code>convert_ncrna_feature(feature, rec, id)</code>","text":"<p>Convert a ncrna feature to a Bakta-style feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The mRNA feature from the GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Bakta-style misc_RNA feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_ncrna_feature(feature, rec, id):\n    \"\"\"\n    Convert a ncrna feature to a Bakta-style feature.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The mRNA feature from the GBK.\n        rec: Bio.SeqRecord\n            The record containing the sequence.\n\n    Returns:\n        dict: Bakta-style misc_RNA feature.\n    \"\"\"\n\n    # seq = str(rec.seq)\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    # Handle CompoundLocation (join)\n    starts = None\n    stops = None\n\n    if feature.location.__class__.__name__ == \"CompoundLocation\":\n        starts = []\n        stops = []\n        for part in feature.location.parts:\n            starts.append(int(part.start) + 1)\n            stops.append(int(part.end))\n\n    qualifiers = feature.qualifiers\n\n    so_code =  so.SO_NCRNA_GENE.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append so only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n    # Mandatory qualifier (INSDC requirement)\n    ncrna_class = qualifiers.get(\"ncRNA_class\", [None])[0]\n    if ncrna_class is None:\n        raise ValueError(\n            f\"ncRNA feature {id} is missing mandatory /ncRNA_class qualifier\"\n        )\n\n    ncrna_entry = {\n        \"type\": \"ncRNA\",\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"id\": id,\n\n        # Join support\n        \"starts\": starts,\n        \"stops\": stops,\n\n        # Mandatory\n        \"ncRNA_class\": ncrna_class,\n\n        # Multi-valued qualifiers\n        \"db_xrefs\": db_xrefs,\n\n\n    }\n\n# Feature Key           ncRNA\n\n# Definition            a non-protein-coding gene, other than ribosomal RNA and\n#                       transfer RNA, the functional molecule of which is the RNA\n#                       transcript;\n\n# Mandatory qualifiers  /ncRNA_class=\"TYPE\"\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /operon=\"text\"\n#                       /product=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /standard_name=\"text\"\n#                       /trans_splicing\n\n# Example               /ncRNA_class=\"miRNA\"\n#                       /ncRNA_class=\"siRNA\"\n#                       /ncRNA_class=\"scRNA\"       \n\n# Comment               the ncRNA feature is not used for ribosomal and transfer\n#                       RNA annotation, for which the rRNA and tRNA feature keys\n#                       should be used, respectively;\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\" }\n    single_valued = {\"allele\", \"gene\", \"locus_tag\", \"map\",    \"old_locus_tag\", \"operon\", \"product\", \"standard_name\", \"pseudogene\"}\n\n    add_optional_qualifiers(ncrna_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"pseudo\", \"trans_splicing\"]:\n        if flag in qualifiers:\n            ncrna_entry[flag] = flag in qualifiers\n\n    #  ncRNA           join(189791085..189791793,189798997..189799081,\n    #                  189819873..189820364,189821703..189822337)\n    #                  /ncRNA_class=\"lncRNA\"\n    #                  /gene=\"Gm30446\"\n    #                  /product=\"predicted gene, 30446, transcript variant X6\"\n    #                  /note=\"Derived by automated computational analysis using\n    #                  gene prediction method: Gnomon. Supporting evidence\n    #                  includes similarity to: 100% coverage of the annotated\n    #                  genomic feature by RNAseq alignments, including 2 samples\n    #                  with support for all annotated introns\"\n    #                  /transcript_id=\"XR_001779629.1\"\n    #                  /db_xref=\"GeneID:102632350\"\n    #                  /db_xref=\"MGI:MGI:5589605\"\n\n    return ncrna_entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.convert_precursor_rna_feature","title":"<code>convert_precursor_rna_feature(feature, rec, id)</code>","text":"<p>Convert a GenBank precursor_RNA feature to a Bakta-style feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_precursor_rna_feature(feature, rec, id):\n    \"\"\"\n    Convert a GenBank precursor_RNA feature to a Bakta-style feature.\n    \"\"\"\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    qualifiers = feature.qualifiers\n\n    so_code =  so.SO_PRECURSOR_RNA.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n    precursor_rna_entry = {\n            \"type\": \"precursor_RNA\",\n            \"sequence\": rec.id,\n            \"start\": start,\n            \"stop\": stop,\n            \"strand\": strand,\n            \"db_xrefs\": db_xrefs,\n            \"id\": id,\n        }\n\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\"}\n    single_valued = {\"allele\", \"gene\", \"locus_tag\", \"map\",  \"operon\",  \"old_locus_tag\", \"product\", \"standard_name\"}\n\n    add_optional_qualifiers(precursor_rna_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"trans_splicing\"]:\n        if flag in qualifiers:\n            precursor_rna_entry[flag] = True\n\n#     Feature Key           precursor_RNA\n\n\n# Definition            any RNA species that is not yet the mature RNA product;\n#                       may include ncRNA, rRNA, tRNA, 5' untranslated region\n#                       (5'UTR), coding sequences (CDS, exon), intervening\n#                       sequences (intron) and 3' untranslated region (3'UTR);\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"  \n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /operon=\"text\"\n#                       /product=\"text\"\n#                       /standard_name=\"text\"\n#                       /trans_splicing\n\n\n    #  precursor_RNA   194719348..194719428\n    #                  /gene=\"Mir29b-2\"\n    #                  /gene_synonym=\"mir-29b-2; Mirn29b-2\"\n    #                  /product=\"microRNA 29b-2\"\n    #                  /note=\"Derived by automated computational analysis using\n    #                  gene prediction method: BestRefSeq.\"\n    #                  /transcript_id=\"NR_029809.1\"\n    #                  /db_xref=\"GeneID:723963\"\n    #                  /db_xref=\"MGI:MGI:3619047\"\n    #                  /db_xref=\"miRBase:MI0000712\"\n\n    return precursor_rna_entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.convert_proprotein_propeptide_feature","title":"<code>convert_proprotein_propeptide_feature(feature, rec, id)</code>","text":"<p>Convert a proprotein or propeptide feature to a Bakta-style feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The mRNA feature from the GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Bakta-style proprotein feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_proprotein_propeptide_feature(feature, rec, id):\n    \"\"\"\n    Convert a proprotein or propeptide feature to a Bakta-style feature.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The mRNA feature from the GBK.\n        rec: Bio.SeqRecord\n            The record containing the sequence.\n\n    Returns:\n        dict: Bakta-style proprotein feature.\n    \"\"\"\n\n    # seq = str(rec.seq)\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    # Handle CompoundLocation (join)\n    starts = None\n    stops = None\n\n    if feature.location.__class__.__name__ == \"CompoundLocation\":\n        starts = []\n        stops = []\n        for part in feature.location.parts:\n            starts.append(int(part.start) + 1)\n            stops.append(int(part.end))\n\n\n    qualifiers = feature.qualifiers\n\n    so_code =  so.SO_PROPEPTIDE.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append so.SO_CDS.id only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n\n    propeptide_entry = {\n        \"type\": \"propeptide\",\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"id\": id,\n\n        # Join support\n        \"starts\": starts,\n        \"stops\": stops,\n\n        # Multi-valued\n        \"db_xrefs\": qualifiers.get(\"db_xref\", []),\n\n    }\n\n\n# Feature Key           propeptide\n\n\n# Definition            propeptide coding sequence; coding sequence for the domain of a \n#                       proprotein that is cleaved to form the mature protein product.\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /product=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /standard_name=\"text\"\n\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\"}\n    single_valued = {\"allele\", \"gene\", \"locus_tag\", \"map\",    \"old_locus_tag\", \"product\", \"standard_name\", \"pseudogene\"}\n\n    add_optional_qualifiers(propeptide_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"pseudo\",]:\n        if flag in qualifiers:\n            propeptide_entry[flag] = True\n\n    #  proprotein      join(171053237..171053367,171053712..171053832)\n    #                  /gene=\"Apoa2\"\n    #                  /gene_synonym=\"Alp-2; Apo-AII; Apoa-2; ApoA-II; ApoAII;\n    #                  Hdl-1\"\n    #                  /product=\"apolipoprotein A-II proprotein\"  \n\n    return propeptide_entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.convert_protein_bind_feature","title":"<code>convert_protein_bind_feature(feature, rec, id)</code>","text":"<p>Convert a GenBank protein_bind feature to a Bakta-style feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_protein_bind_feature(feature, rec, id):\n    \"\"\"\n    Convert a GenBank protein_bind feature to a Bakta-style feature.\n    \"\"\"\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    qualifiers = feature.qualifiers\n\n    # Mandatory qualifier\n    bound_moiety = qualifiers.get(\"bound_moiety\", [None])[0]\n    if bound_moiety is None:\n        raise ValueError(\n            f\"protein_bind feature {id} is missing mandatory /bound_moiety qualifier\"\n        )\n\n    so_code =  so.SO_PROTEINBIND.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n    protein_bind_entry = {\n        \"type\": \"protein_bind\",\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"bound_moiety\": bound_moiety,\n        \"db_xrefs\": db_xrefs,\n        \"id\": id,\n    }\n\n\n# Feature Key           protein_bind\n\n\n# Definition            non-covalent protein binding site on nucleic acid;\n\n# Mandatory qualifiers  /bound_moiety=\"text\"\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /operon=\"text\"\n#                       /standard_name=\"text\"\n\n# Comment               note that feature key regulatory with /regulatory_class=\"ribosome_binding_site\"\n#                       should be used for ribosome binding sites.\n\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\"}\n    single_valued = {\"allele\", \"gene\", \"locus_tag\", \"map\",  \"operon\",  \"old_locus_tag\", \"product\", \"standard_name\"}\n\n    add_optional_qualifiers(protein_bind_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    # for flag in [\"trans_splicing\"]:\n    #     if flag in qualifiers:\n    #         protein_bind_entry[flag] = True\n\n    return protein_bind_entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.convert_regulatory_feature","title":"<code>convert_regulatory_feature(feature, rec, id)</code>","text":"<p>Convert a GenBank regulatory feature to a Bakta-style feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_regulatory_feature(feature, rec, id):\n    \"\"\"\n    Convert a GenBank regulatory feature to a Bakta-style feature.\n    \"\"\"\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    qualifiers = feature.qualifiers\n\n    # Mandatory qualifier\n    regulatory_class = qualifiers.get(\"regulatory_class\", [None])[0]\n    if regulatory_class is None:\n        raise ValueError(\n            f\"regulatory feature {id} is missing mandatory /regulatory_class qualifier\"\n        )\n\n    so_code =  so.SO_REGULATORY_REGION.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n\n    regulatory_entry = {\n            \"type\": \"regulatory\",\n            \"sequence\": rec.id,\n            \"start\": start,\n            \"stop\": stop,\n            \"strand\": strand,\n            \"regulatory_class\": regulatory_class,\n            \"db_xrefs\": db_xrefs,\n            \"id\": id,\n        }\n\n\n# Feature Key           regulatory\n\n\n# Definition            any region of sequence that functions in the regulation of\n#                       transcription, translation, replication, recombination, or chromatin structure;\n\n# Mandatory qualifiers  /regulatory_class=\"TYPE\"\n\n# Optional qualifiers   /allele=\"text\"\n#                       /bound_moiety=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /operon=\"text\"\n#                       /phenotype=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /standard_name=\"text\"\n\n# Comment\t              This feature has replaced the following Feature Keys on 15-DEC-2014:\n#                       enhancer, promoter, CAAT_signal, TATA_signal, -35_signal, -10_signal,\n#                       RBS, GC_signal, polyA_signal, attenuator, terminator, misc_signal.\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\"}\n    single_valued = {\"allele\", \"bound_moiety\", \"gene\", \"locus_tag\", \"map\",  \"operon\",  \"old_locus_tag\", \"phenotype\", \"product\", \"pseudogene\", \"standard_name\"}\n\n    add_optional_qualifiers(regulatory_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"pseudo\"]:\n        if flag in qualifiers:\n            regulatory_entry[flag] = True\n\n    #  regulatory      195030925..195032349\n    #                  /regulatory_class=\"enhancer\"\n    #                  /experiment=\"EXISTENCE:reporter gene assay evidence\n    #                  [ECO:0000049][PMID:32912294]\"\n    #                  /note=\"C2 STARR-seq-only enhancer starr_03508\"\n    #                  /function=\"activates a minimal SCP1 promoter by STARR-seq\n    #                  in ground-state (2iL) and metastable (SL) mouse embryonic\n    #                  stem cells {active_cell/tissue: mESC(E14 +2i+LIF or\n    #                  +serum+LIF)}\"\n    #                  /db_xref=\"GeneID:131296982\"\n\n    return regulatory_entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.convert_repeat_region_feature","title":"<code>convert_repeat_region_feature(feature, rec, id)</code>","text":"<p>Convert a Prokka GenBank repeat_region (CRISPR) feature to a simplified Bakta-style feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The repeat_region feature (crispr) from the Prokka GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The full GenBank record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Simplified Bakta-style CRISPR feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_repeat_region_feature(feature, rec, id):\n    \"\"\"\n    Convert a Prokka GenBank repeat_region (CRISPR) feature to a simplified Bakta-style feature.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The repeat_region feature (crispr) from the Prokka GBK.\n        rec: Bio.SeqRecord\n            The full GenBank record containing the sequence.\n\n    Returns:\n        dict: Simplified Bakta-style CRISPR feature.\n    \"\"\"\n\n    # Coordinates (Bakta uses 1-based)\n    strand = \".\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n\n    qualifiers = feature.qualifiers\n    note = qualifiers.get(\"note\", [None])[0]\n    rpt_family = qualifiers.get(\"rpt_family\", [None])[0]\n    rpt_type = qualifiers.get(\"rpt_type\", [None])[0]\n    rpt_unit_seq = qualifiers.get(\"rpt_unit_seq\", [None])[0]\n\n    # always just take the positive strand to get the NT seq (crispr repeat region)\n    seq =  str(rec.seq)\n    nt_seq = seq[start-1:stop]\n\n\n# Feature Key           repeat_region\n\n\n# Definition            region of genome containing repeating units;\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\" \n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /rpt_family=\"text\"\n#                       /rpt_type=&lt;repeat_type&gt;\n#                       /rpt_unit_range=&lt;base_range&gt;\n#                       /rpt_unit_seq=\"text\"\n#                       /satellite=\"&lt;satellite_type&gt;[:&lt;class&gt;][ &lt;identifier&gt;]\"\n#                       /standard_name=\"text\"\n\n    so_code =  so.SO_REPEAT.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n    # Minimal Bakta-like CRISPR structure\n    repeat_region_entry = {\n        \"type\": \"repeat_region\",\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand, # matches Bakta and is required\n        \"family\": rpt_family,       # e.g., \"LINE1\" - should always be there\n        \"rpt_type\": rpt_type,   \n        \"repeat_unit\": rpt_unit_seq, # the actual consensus repeat if crispr\n        \"product\": note, # won't be the same as Bakta as different lookup method used - but needed for the gff writing\n        \"nt\": nt_seq, # needed for batka .ffn writeout\n        \"id\": id, # bakta_id needed \n        # \"locus\": None, # no locus tag like Bakta\n        \"db_xrefs\": db_xrefs\n    }\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\" }\n    single_valued = {\"satellite\", \"gene\",  \"locus_tag\", \"map\",  \"old_locus_tag\", \"operon\", \"phenotype\", \"product\", \"standard_name\"}\n\n    qualifiers = feature.qualifiers\n\n    add_optional_qualifiers(repeat_region_entry, qualifiers, single_valued, multi_valued)\n\n\n    return repeat_region_entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.convert_rrna_feature","title":"<code>convert_rrna_feature(feature, rec, id)</code>","text":"<p>Convert a GenBank rRNA feature to a Bakta-style feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_rrna_feature(feature, rec, id):\n    \"\"\"\n    Convert a GenBank rRNA feature to a Bakta-style feature.\n    \"\"\"\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    qualifiers = feature.qualifiers\n\n    so_code =  so.SO_RRNA.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n# Feature Key           rRNA\n\n\n# Definition            mature ribosomal RNA; RNA component of the\n#                       ribonucleoprotein particle (ribosome) which assembles\n#                       amino acids into proteins.\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /operon=\"text\"\n#                       /product=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /standard_name=\"text\"\n\n# Comment               rRNA sizes should be annotated with the /product\n#                       qualifier.  \n\n\n    rrna_entry = {\n            \"type\": \"rRNA\",\n            \"sequence\": rec.id,\n            \"start\": start,\n            \"stop\": stop,\n            \"strand\": strand,\n            \"db_xrefs\": db_xrefs,\n            \"id\": id,\n        }\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\"}\n    single_valued = {\"allele\", \"gene\", \"locus_tag\", \"map\",  \"operon\",  \"old_locus_tag\", \"product\", \"pseudogene\", \"standard_name\"}\n\n    add_optional_qualifiers(rrna_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"pseudo\"]:\n        if flag in qualifiers:\n            rrna_entry[flag] = True\n\n\n    #  rRNA            46413357..46413475\n    #                  /gene=\"n-R5s211\"\n    #                  /product=\"5S ribosomal RNA\"\n    #                  /inference=\"COORDINATES: nucleotide\n    #                  motif:Rfam:12.0:RF00001\"\n    #                  /inference=\"COORDINATES: profile:INFERNAL:1.1.1\"\n    #                  /note=\"Derived by automated computational analysis using\n    #                  gene prediction method: cmsearch.\"\n    #                  /transcript_id=\"XR_004936691.1\"\n    #                  /db_xref=\"GeneID:115487577\"\n    #                  /db_xref=\"RFAM:RF00001\"\n    #                  /db_xref=\"MGI:MGI:4422076\"\n\n    return rrna_entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.convert_sig_peptide_feature","title":"<code>convert_sig_peptide_feature(feature, rec, id)</code>","text":"<p>Convert a sig_peptide feature to a Bakta-style feature.</p> <p>mus musculus chrom 1 NC_000067</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The mRNA feature from the GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Bakta-style sig_peptide feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_sig_peptide_feature(feature, rec, id):\n    \"\"\"\n    Convert a sig_peptide feature to a Bakta-style feature.\n\n    mus musculus chrom 1 NC_000067\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The mRNA feature from the GBK.\n        rec: Bio.SeqRecord\n            The record containing the sequence.\n\n    Returns:\n        dict: Bakta-style sig_peptide feature.\n    \"\"\"\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    # Handle CompoundLocation (join)\n    starts = None\n    stops = None\n\n    if feature.location.__class__.__name__ == \"CompoundLocation\":\n        starts = []\n        stops = []\n        for part in feature.location.parts:\n            starts.append(int(part.start) + 1)\n            stops.append(int(part.end))\n\n\n    qualifiers = feature.qualifiers\n\n    so_code =  so.SO_SIGNAL_PEPTIDE.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n\n    sig_peptide_entry = {\n        \"type\": \"sig_peptide\",\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"id\": id,\n\n        # Join support\n        \"starts\": starts,\n        \"stops\": stops,\n\n        # Multi-valued\n        \"db_xrefs\": qualifiers.get(\"db_xref\", []),\n\n    }\n\n\n\n\n# Feature Key           sig_peptide\n\n\n# Definition            signal peptide coding sequence; coding sequence for an\n#                       N-terminal domain of a secreted protein; this domain is\n#                       involved in attaching nascent polypeptide to the\n#                       membrane leader sequence;\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /product=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /standard_name=\"text\"\n\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\"}\n    single_valued = {\"allele\", \"gene\", \"locus_tag\", \"map\",  \"operon\",  \"old_locus_tag\", \"phenotype\", \"product\", \"pseudogene\", \"standard_name\"}\n\n    add_optional_qualifiers(sig_peptide_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"pseudo\"]:\n        if flag in qualifiers:\n            sig_peptide_entry[flag] = True\n\n\n    #  sig_peptide     complement(join(194768584..194768588,\n    #                  194774407..194774533))\n    #                  /gene=\"Cd46\"\n    #                  /gene_synonym=\"Mcp\"\n    #                  /inference=\"COORDINATES: ab initio prediction:SignalP:6.0\"\n\n    return sig_peptide_entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.convert_transit_peptide_feature","title":"<code>convert_transit_peptide_feature(feature, rec, id)</code>","text":"<p>Convert a transit_peptide feature to a Bakta-style feature.</p> <p>mus musculus chrom 1 NC_000067</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The mRNA feature from the GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Bakta-style transit_peptide feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_transit_peptide_feature(feature, rec, id):\n    \"\"\"\n    Convert a transit_peptide feature to a Bakta-style feature.\n\n    mus musculus chrom 1 NC_000067\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The mRNA feature from the GBK.\n        rec: Bio.SeqRecord\n            The record containing the sequence.\n\n    Returns:\n        dict: Bakta-style transit_peptide feature.\n    \"\"\"\n\n    # seq = str(rec.seq)\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    # Handle CompoundLocation (join)\n    starts = None\n    stops = None\n\n    if feature.location.__class__.__name__ == \"CompoundLocation\":\n        starts = []\n        stops = []\n        for part in feature.location.parts:\n            starts.append(int(part.start) + 1)\n            stops.append(int(part.end))\n\n\n    qualifiers = feature.qualifiers\n\n    so_code =  so.SO_TRANSIT_PEPTIDE.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n\n    transit_peptide_entry = {\n            \"type\": \"transit_peptide\",\n            \"sequence\": rec.id,\n            \"start\": start,\n            \"stop\": stop,\n            \"strand\": strand,\n            \"id\": id,\n\n            # Join support\n            \"starts\": starts,\n            \"stops\": stops,\n\n            # Multi-valued\n            \"db_xrefs\": qualifiers.get(\"db_xref\", []),\n\n        }\n\n\n\n# Feature Key           transit_peptide\n\n\n# Definition            transit peptide coding sequence; coding sequence for an\n#                       N-terminal domain of a nuclear-encoded organellar\n#                       protein; this domain is involved in post-translational\n#                       import of the protein into the organelle;\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /product=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /standard_name=\"text\"\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\"}\n    single_valued = {\"allele\", \"gene\", \"locus_tag\", \"map\",  \"operon\",  \"old_locus_tag\", \"phenotype\", \"product\", \"pseudogene\", \"standard_name\"}\n\n    add_optional_qualifiers(transit_peptide_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"pseudo\"]:\n        if flag in qualifiers:\n            transit_peptide_entry[flag] = True\n\n    #  transit_peptide complement(join(180006550..180006849,\n    #                  180009627..180009803))\n    #                  /gene=\"Coq8a\"\n    #                  /gene_synonym=\"4632432J16Rik; Adck3; Cabc1; mKIAA0451\"\n    #                  /note=\"Mitochondrion.\n    #                  /evidence=ECO:0000250|UniProtKB:Q8NI60; propagated from\n    #                  UniProtKB/Swiss-Prot (Q60936.2)\"\n\n    return transit_peptide_entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.convert_trna_feature","title":"<code>convert_trna_feature(feature, seq_record, id)</code>","text":"<p>Convert a funannotate tRNA SeqFeature to a Bakta tRNA JSON entry.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_trna_feature(feature, seq_record, id):\n    \"\"\"\n    Convert a funannotate tRNA SeqFeature to a Bakta tRNA JSON entry.\n    \"\"\"\n\n    # ------------ Location ------------\n\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n    # Handle CompoundLocation (join)\n    starts = None\n    stops = None\n\n    if feature.location.__class__.__name__ == \"CompoundLocation\":\n        starts = []\n        stops = []\n        for part in feature.location.parts:\n            starts.append(int(part.start) + 1)\n            stops.append(int(part.end))\n\n\n\n    # ------------ Extract nt sequence ------------\n    nt_seq = feature.extract(seq_record.seq)\n    nt = str(nt_seq)\n\n    # ------------ Basic qualifiers ------------\n    product = feature.qualifiers.get(\"product\", [None])[0]\n\n    qualifiers = feature.qualifiers\n\n    # fall back to start_stop_strand if there is no locus tag\n    if 'locus_tag' in qualifiers and qualifiers['locus_tag']:\n        locus_tag = qualifiers['locus_tag'][0]\n    else:\n        logger.warning(f\"No locus_tag found for feature {id}\")\n        locus_tag = f\"{GENOME_RANDOM_BACKUP_LOCUSTAG_STR}_{start}_{stop}\"\n        logger.warning(f\"Generating a locus_tag: {locus_tag}\")\n\n    # ------------ amino acid ------------\n    # Prokka product examples:\n    #   \"tRNA-Trp\"\n    #   \"tRNA-Leu\"\n    amino_acid = None\n    if product and product.startswith(\"tRNA-\"):\n        amino_acid = product.split(\"-\")[1]\n\n\n    # ------------ anticodon ------------\n    anti_codon = None\n\n    # anticodons are in notes\n\n    notes = feature.qualifiers.get(\"note\", [])\n\n    # Expect a note like: \"tRNA-Ser(gga)\"\n    for note in notes:\n        # Remove spaces for safety\n        n = note.replace(\" \", \"\")\n\n        # Extract part inside parentheses (anticodon)\n        if \"(\" in n and \")\" in n:\n            anti_codon = n.split(\"(\")[1].split(\")\")[0].lower()\n\n        # Extract amino acid:\n        # tRNA-Ser(gga) \u2192 \"Ser\"\n        if \"tRNA-\" in n:\n            try:\n                # tRNA-Ser(gga) \u2192 \"Ser(gga)\" \u2192 split('(')[0] \u2192 \"Ser\"\n                aa_section = n.split(\"tRNA-\")[1]\n                aa_clean = aa_section.split(\"(\")[0]\n                amino_acid = aa_clean\n            except Exception:\n                pass\n\n    # ------------ Anti-codon position detection ------------\n    # Prokka doesnt have it - dont include\n    # anti_codon_pos = None\n\n    # ------------ score ------------\n    # nothing in prokka\n    score = None\n\n    # ------------ db_xrefs ------------\n    # doesnt exist for prokka\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [])\n    # add so_term\n    so_term = AMINO_ACID_DICT.get(amino_acid.lower(), ('', None))[1]\n\n    if (so_term):\n        db_xrefs.append(so_term.id)\n\n    # ------------ final Bakta-form dict ------------\n    bakta_trna_entry = {\n        \"type\": \"tRNA\",\n        \"sequence\": seq_record.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"gene\": \"trn\" + (amino_acid[0].lower() if amino_acid else \"?\"),\n        \"product\": product,\n        \"amino_acid\": amino_acid,\n        \"anti_codon\": anti_codon,\n        \"score\": score,\n        \"nt\": nt,\n        \"db_xrefs\": db_xrefs,\n       #  \"anti_codon_pos\": anti_codon_pos,  dont include, not in output\n        \"locus\": locus_tag,\n        \"id\": id,\n    }\n\n# Feature Key           tRNA\n\n\n# Definition            mature transfer RNA, a small RNA molecule (75-85 bases\n#                       long) that mediates the translation of a nucleic acid\n#                       sequence into an amino acid sequence;\n\n# Optional qualifiers   /allele=\"text\"\n#                       /anticodon=(pos:&lt;location&gt;,aa:&lt;amino_acid&gt;,seq:&lt;text&gt;)\n#                       /circular_RNA\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /operon=\"text\"\n#                       /product=\"text\"\n#                       /pseudo\n#                       /pseudogene=\"TYPE\"\n#                       /standard_name=\"text\"\n#                       /trans_splicing\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\" }\n    single_valued = {\"allele\",  \"map\",    \"old_locus_tag\", \"standard_name\"}\n\n    qualifiers = feature.qualifiers\n\n    add_optional_qualifiers(bakta_trna_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"circular_RNA\", \"pseudo\", \"trans_splicing\"]:\n        if flag in qualifiers:\n            bakta_trna_entry[flag] = flag in qualifiers\n\n    return bakta_trna_entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.convert_utr_region_feature","title":"<code>convert_utr_region_feature(feature, rec, id, three)</code>","text":"<p>Convert a UTR GenBank feature to a simplified Bakta-style feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The UTR feature from the GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The full GenBank record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Simplified Bakta-style feature.</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def convert_utr_region_feature(feature, rec, id, three):\n    \"\"\"\n    Convert a UTR GenBank feature to a simplified Bakta-style feature.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The UTR feature from the GBK.\n        rec: Bio.SeqRecord\n            The full GenBank record containing the sequence.\n\n    Returns:\n        dict: Simplified Bakta-style feature.\n    \"\"\"\n\n    if three:\n        type = \"3'UTR\"\n        so_code =  so.SO_3UTR.id\n    else:\n        type = \"5'UTR\"\n        so_code =  so.SO_5UTR.id\n\n    # Extract location\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n    start = int(feature.location.start) + 1   # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)         # already inclusive after conversion\n\n\n    qualifiers = feature.qualifiers\n    note = qualifiers.get(\"note\", [None])[0]\n\n\n    # fall back to start_stop_strand if there is no locus tag\n    if 'locus_tag' in qualifiers and qualifiers['locus_tag']:\n        locus_tag = qualifiers['locus_tag'][0]\n    else:\n        logger.warning(f\"No locus_tag found for feature {id}\")\n        locus_tag = f\"{GENOME_RANDOM_BACKUP_LOCUSTAG_STR}_{start}_{stop}\"\n        logger.warning(f\"Generating a locus_tag: {locus_tag}\")\n\n    # always just take the positive strand to get the NT seq (UTR region)\n    seq =  str(rec.seq)\n    nt_seq = seq[start-1:stop]\n\n\n# Feature Key           3'UTR\n\n\n# Definition            1) region at the 3' end of a mature transcript (following \n#                       the stop codon) that is not translated into a protein;\n#                       2) region at the 3' end of an RNA virus (following the last stop\n#                       codon) that is not translated into a protein;\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /standard_name=\"text\"\n#                       /trans_splicing\n\n\n\n# Feature Key           5'UTR\n\n\n# Definition            1) region at the 5' end of a mature transcript (preceding \n#                       the initiation codon) that is not translated into a protein;\n#                       2) region at the 5' end of an RNA virus genome (preceding the first \n#                       initiation codon) that is not translated into a protein;\n\n# Optional qualifiers   /allele=\"text\"\n#                       /db_xref=\"&lt;database&gt;:&lt;identifier&gt;\"\n#                       /experiment=\"[CATEGORY:]text\"\n#                       /function=\"text\"\n#                       /gene=\"text\"\n#                       /gene_synonym=\"text\"\n#                       /inference=\"[CATEGORY:]TYPE[ (same species)][:EVIDENCE_BASIS]\"\n#                       /locus_tag=\"text\" (single token)\n#                       /map=\"text\"\n#                       /note=\"text\"\n#                       /old_locus_tag=\"text\" (single token)\n#                       /standard_name=\"text\"\n#                       /trans_splicing\n\n    so_code =  so.SO_REPEAT.id\n\n    # Get existing db_xref list or default to [so.SO_CDS.id]\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [so_code])\n\n    # Append only if it\u2019s not already present\n    if so_code not in db_xrefs:\n        db_xrefs.append(so_code)\n\n\n    # Minimal Bakta-like structure\n    utr_entry = {\n        \"type\": type,\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand, # matches Bakta and is required\n        \"product\": note, \n        \"nt\": nt_seq, # needed for batka .ffn writeout\n        \"id\": id, # bakta_id needed \n        \"db_xrefs\": db_xrefs,\n        \"locus\": locus_tag\n    }\n\n    multi_valued = {\"experiment\", \"function\",  \"gene_synonym\",  \"inference\", \"note\" }\n    single_valued = {\"allele\", \"gene\",   \"map\",  \"old_locus_tag\", \"operon\", \"phenotype\", \"standard_name\"}\n\n    qualifiers = feature.qualifiers\n\n    add_optional_qualifiers(utr_entry, qualifiers, single_valued, multi_valued)\n\n    # Flags (boolean-like qualifiers)\n    for flag in [\"pseudo\", \"trans_splicing\"]:\n        if flag in qualifiers:\n            utr_entry[flag] = flag in qualifiers\n\n    return utr_entry\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.get_bakta_style_id_from_locus_tag","title":"<code>get_bakta_style_id_from_locus_tag(records)</code>","text":"<p>Gets 10 char bakta-style ID tag based off the 8 char locus tag in first CDS on the first  record + 2 random chars</p> <p>Assumes all records will have the same locus tag prefix</p> <p>Will always add 2 chars to make ID unique vs locus tag</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def get_bakta_style_id_from_locus_tag(records):\n    \"\"\"\n    Gets 10 char bakta-style ID tag based off the 8 char locus tag in first CDS on the first  record + 2 random chars\n\n    Assumes all records will have the same locus tag prefix\n\n    Will always add 2 chars to make ID unique vs locus tag\n    \"\"\"\n\n    if not records:\n        raise ValueError(\"No GenBank records found.\")\n\n    for record in records:\n\n        for feat in record.features:\n            if feat.type == \"CDS\":\n                locus_tag_list = feat.qualifiers.get(\"locus_tag\") # returns None if doesn't exist\n\n                if locus_tag_list:\n                    locus_tag = locus_tag_list[0]\n\n                    if len(locus_tag) &gt; 7:\n\n                        locus_tag_prefix = locus_tag[:-7] # trims off _000001 from CDS\n\n                        rand_two_chars = random_n_letter_id(2)\n\n                        # by default  locus tag is 8 chars. So this returns a 10 char string (same as bakta defaults)\n\n                        id_tag = f\"{locus_tag_prefix}{rand_two_chars}\"\n\n                        return id_tag\n\n\n                    else:\n                        return random_n_letter_id(10)\n\n                # fallback if locus_tag missing or too short\n                return random_n_letter_id(10)\n\n    # No CDS feature found at all (shouldn't happen)\n    return random_n_letter_id(10)\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.eukaryotic_to_json.random_n_letter_id","title":"<code>random_n_letter_id(n=4)</code>","text":"<p>generates a n letter id prefix </p> <p>n=2 to append to   locus tag  for bakta id to make it different n=10 if the locus tag is somehow missing (should never happen)</p> Source code in <code>src/baktfold/io/eukaryotic_to_json.py</code> <pre><code>def random_n_letter_id(n=4):\n    \"\"\"\n    generates a n letter id prefix \n\n    n=2 to append to   locus tag  for bakta id to make it different\n    n=10 if the locus tag is somehow missing (should never happen) \n    \"\"\"\n    return ''.join(random.choices(string.ascii_uppercase, k=n))\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.io.write_bakta_outputs","title":"<code>write_bakta_outputs(data, features, features_by_sequence, output, prefix, custom_db, euk, has_duplicate_locus, fast, translation_table)</code>","text":"<p>Writes the bakta outputs to a given path.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>The dictionary containing the bakta outputs.</p> required <code>features</code> <code>Sequence[dict]</code> <p>The sequence of dictionaries containing the features.</p> required <code>features_by_sequence</code> <code>Sequence[dict]</code> <p>The sequence of dictionaries containing the features by sequence.</p> required <code>output</code> <code>Path</code> <p>The path to save the bakta outputs to.</p> required <code>prefix</code> <code>str</code> <p>The prefix to use for the bakta outputs.</p> required <code>custom_db</code> <code>bool</code> <p>A boolean indicating whether a custom database is used.</p> required <code>euk</code> <code>bool</code> <p>A boolean indicating whether the sequences are eukaryotic.</p> required <code>has_duplicate_locus</code> <code>bool</code> <p>A boolean indicating whether there are duplicate loci.</p> required <code>fast</code> <code>bool</code> <p>If True, skips AFDB step</p> required <code>translation_table</code> <code>str</code> <p>Translation table inferred from input JSON</p> required <p>Returns:</p> Type Description <p>None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; write_bakta_outputs(data, features, features_by_sequence, output, prefix, custom_db, euk, has_duplicate_locus)\n</code></pre> Source code in <code>src/baktfold/io/io.py</code> <pre><code>def write_bakta_outputs(data: dict, features: Sequence[dict], features_by_sequence: Sequence[dict] , \n                        output: Path, prefix: str, custom_db: bool, euk: bool, has_duplicate_locus: bool,\n                        fast: bool, translation_table: int):\n    \"\"\"\n    Writes the bakta outputs to a given path.\n\n    Args:\n      data (dict): The dictionary containing the bakta outputs.\n      features (Sequence[dict]): The sequence of dictionaries containing the features.\n      features_by_sequence (Sequence[dict]): The sequence of dictionaries containing the features by sequence.\n      output (Path): The path to save the bakta outputs to.\n      prefix (str): The prefix to use for the bakta outputs.\n      custom_db (bool): A boolean indicating whether a custom database is used.\n      euk (bool): A boolean indicating whether the sequences are eukaryotic.\n      has_duplicate_locus (bool): A boolean indicating whether there are duplicate loci.\n      fast (bool): If True, skips AFDB step\n      translation_table (str): Translation table inferred from input JSON\n\n    Returns:\n      None.\n\n    Examples:\n      &gt;&gt;&gt; write_bakta_outputs(data, features, features_by_sequence, output, prefix, custom_db, euk, has_duplicate_locus)\n    \"\"\"\n\n    #logger.info(f'selected features={len(features)}')\n\n    logger.info('writing human readable TSV...')\n    tsv_path: Path = Path(output) / f\"{prefix}.tsv\"\n    tsv.write_features(data['sequences'], features_by_sequence, tsv_path)\n\n    logger.info('writing GFF3...')\n    gff3_path: Path = Path(output) / f\"{prefix}.gff3\"\n    # fix later prokka\n    prokka = False\n    gff.write_features(data, features_by_sequence, gff3_path, prokka, euk)\n\n    logger.info('writing INSDC GenBank &amp; EMBL...')\n    genbank_path: Path = Path(output) / f\"{prefix}.gbff\"\n    embl_path: Path = Path(output) / f\"{prefix}.embl\"\n    insdc.write_features(data, features, genbank_path, embl_path, prokka, euk, translation_table)\n\n    logger.info('writing genome sequences...')\n    fna_path: Path = Path(output) / f\"{prefix}.fna\"\n    fasta.export_sequences(data['sequences'], fna_path, description=True, wrap=True)\n\n    logger.info('writing feature nucleotide sequences...')\n    ffn_path: Path = Path(output) / f\"{prefix}.ffn\"\n    fasta.write_ffn(features, ffn_path)\n\n    logger.info('writing translated CDS sequences...')\n    faa_path: Path = Path(output) / f\"{prefix}.faa\"\n    fasta.write_faa(features, faa_path)\n\n    # inference here is the different databases?\n    annotations_path: Path = Path(output) / f\"{prefix}.inference.tsv\"\n    if custom_db:\n        header_columns = ['Locus', 'Length', 'Product', 'Swissprot', 'AFDBClusters', 'PDB', 'CATH', 'Custom_DB']\n        if has_duplicate_locus:\n            header_columns = ['Locus', 'ID', 'Product', 'Swissprot', 'AFDBClusters', 'PDB', 'CATH', 'Custom_DB']\n    else:\n        header_columns = ['Locus', 'Length', 'Product', 'Swissprot', 'AFDBClusters', 'PDB', 'CATH']\n        if has_duplicate_locus:\n            header_columns = ['Locus', 'ID', 'Product', 'Swissprot', 'AFDBClusters', 'PDB', 'CATH']\n\n    # Remove 'AFDBClusters' if fast is True\n    if fast:\n        header_columns = [col for col in header_columns if col != 'AFDBClusters']\n\n    logger.info(f'Exporting annotations (TSV) to: {annotations_path}')\n\n    selected_features = []\n\n    for seq_id, features in features_by_sequence.items():\n        for feat in features:\n            # get() ensures we don't crash if the key doesn't exist\n            if 'hypothetical' in feat or 'baktfold' in feat:\n                selected_features.append(feat)\n\n\n    tsv.write_protein_features(selected_features, header_columns, annotations_path, custom_db, has_duplicate_locus, fast=fast)\n\n\n\n    cfg.skip_cds = False\n    if(cfg.skip_cds is False):\n\n        # no need to write the hypotheticals I think\n\n        # hypotheticals = [feat for feat in features if feat['type'] == bc.FEATURE_CDS and 'hypothetical' in feat]\n\n\n        # print('writing hypothetical TSV...')\n        # tsv_path: Path = Path(output) / f\"{prefix}.hypotheticals.tsv\"\n        # tsv.write_hypotheticals(hypotheticals, tsv_path)\n\n        # print('writing translated hypothetical CDS sequences...')\n        # print('writing translated CDS sequences...')\n        # faa_path: Path = Path(output) / f\"{prefix}.hypotheticals.faa\"\n        # fasta.write_faa(hypotheticals, faa_path)\n\n        # calc &amp; store runtime\n\n        # run_duration = (cfg.run_end - cfg.run_start).total_seconds()\n        # data['run'] = {\n        #     'start': cfg.run_start.strftime('%Y-%m-%d %H:%M:%S'),\n        #     'end': cfg.run_end.strftime('%Y-%m-%d %H:%M:%S'),\n        #     'duration': f'{(run_duration / 60):.2f} min'\n        # }\n\n        logger.info('write machine readable JSON...')\n        json_path: Path = Path(output) / f\"{prefix}.json\"\n        json.write_json(data, features, json_path)\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.io.write_bakta_proteins_outputs","title":"<code>write_bakta_proteins_outputs(aas, output, prefix, custom_db, fast)</code>","text":"<p>Writes the bakta protein outputs to a given path.</p> <p>Parameters:</p> Name Type Description Default <code>aas</code> <code>Sequence[dict]</code> <p>The sequence of dictionaries containing the amino acids.</p> required <code>output</code> <code>Path</code> <p>The path to save the bakta protein outputs to.</p> required <code>prefix</code> <code>str</code> <p>The prefix to use for the bakta protein outputs.</p> required <code>custom_db</code> <code>bool</code> <p>A boolean indicating whether a custom database is used.</p> required <code>fast</code> <code>bool</code> <p>If True, skips AFDB step</p> required <p>Returns:</p> Type Description <p>None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; write_bakta_proteins_outputs(aas, output, prefix, custom_db)\n</code></pre> Source code in <code>src/baktfold/io/io.py</code> <pre><code>def write_bakta_proteins_outputs(aas: Sequence[dict], output: Path, prefix: str, custom_db: bool, fast: bool):\n    \"\"\"\n    Writes the bakta protein outputs to a given path.\n\n    Args:\n      aas (Sequence[dict]): The sequence of dictionaries containing the amino acids.\n      output (Path): The path to save the bakta protein outputs to.\n      prefix (str): The prefix to use for the bakta protein outputs.\n      custom_db (bool): A boolean indicating whether a custom database is used.\n      fast (bool): If True, skips AFDB step\n\n    Returns:\n      None.\n\n    Examples:\n      &gt;&gt;&gt; write_bakta_proteins_outputs(aas, output, prefix, custom_db)\n    \"\"\"\n\n\n    annotations_path: Path = Path(output) / f\"{prefix}.tsv\"\n    if custom_db:\n        header_columns = ['ID', 'Length', 'Product', 'Swissprot', 'AFDBClusters', 'PDB', 'CATH', 'Custom_DB']\n    else:\n        header_columns = ['ID', 'Length', 'Product', 'Swissprot', 'AFDBClusters', 'PDB', 'CATH']\n\n    if fast:\n        header_columns = [col for col in header_columns if col != 'AFDBClusters']\n\n\n    logger.info(f'Exporting annotations (TSV) to: {annotations_path}')\n    tsv.write_protein_features(aas, header_columns, annotations_path, custom_db, has_duplicate_locus=False, fast=fast)\n\n\n    # do i combine the tophits tsvs, sort by column, add a column for db and put out as one tsv\n\n    full_annotations_path: Path = Path(output) / f\"{prefix}.json\"\n    logger.info(f'Full annotations (JSON): {full_annotations_path}')\n    json.write_json({'features': aas}, aas, full_annotations_path)\n\n\n    #### don't write hyps I think\n\n    # hypotheticals_path = output_path.joinpath(f'{cfg.prefix}.hypotheticals.tsv')\n    # header_columns = ['ID', 'Length', 'Mol Weight [kDa]', 'Iso El. Point', 'Pfam hits']\n    # hypotheticals = hypotheticals = [aa for aa in aas if 'hypothetical' in aa]\n    # print(f'\\tinformation on hypotheticals (TSV): {hypotheticals_path}')\n    # tsv.write_protein_features(hypotheticals, header_columns, map_hypothetical_columns, hypotheticals_path)\n\n    aa_output_path: Path = Path(output) / f\"{prefix}.faa\"\n    logger.info(f'Annotated sequences (Fasta): {aa_output_path}')\n    fasta.write_faa(aas, aa_output_path)\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.io.write_foldseek_tophit","title":"<code>write_foldseek_tophit(tophit_df, pdb_tophit_path)</code>","text":"<p>Writes the foldseek tophits to a given path.</p> <p>Parameters:</p> Name Type Description Default <code>tophit_df</code> <code>pd.DataFrame</code> <p>The dataframe containing the foldseek tophits.</p> required <code>pdb_tophit_path</code> <code>Path</code> <p>The path to save the foldseek tophits to.</p> required <p>Returns:</p> Type Description <p>None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; write_foldseek_tophit(tophit_df, pdb_tophit_path)\n</code></pre> Source code in <code>src/baktfold/io/io.py</code> <pre><code>def write_foldseek_tophit(tophit_df: pd.DataFrame, pdb_tophit_path: Path):\n    \"\"\"\n    Writes the foldseek tophits to a given path.\n\n    Args:\n      tophit_df (pd.DataFrame): The dataframe containing the foldseek tophits.\n      pdb_tophit_path (Path): The path to save the foldseek tophits to.\n\n    Returns:\n      None.\n\n    Examples:\n      &gt;&gt;&gt; write_foldseek_tophit(tophit_df, pdb_tophit_path)\n    \"\"\"\n    logger.info(f\"Saving foldseek tophits to {pdb_tophit_path}\")\n    tophit_df.to_csv(pdb_tophit_path, sep=\"\\t\", index=False)\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.tsv.map_aa_columns","title":"<code>map_aa_columns(feat, custom_db, has_duplicate_locus, fast)</code>","text":"<p>Maps amino acid columns.</p> <p>Parameters:</p> Name Type Description Default <code>feat</code> <code>dict</code> <p>The dictionary containing the features.</p> required <code>custom_db</code> <code>bool</code> <p>A boolean indicating whether a custom database is used.</p> required <code>has_duplicate_locus</code> <code>bool</code> <p>A boolean indicating whether there are duplicate loci.</p> required <code>fast</code> <code>bool</code> <p>A boolean indicating whether AFDBclusters Foldseek search should be skipped</p> required <p>Returns:</p> Type Description <code>Sequence[str]</code> <p>Sequence[str]: A sequence of strings containing the mapped amino acid columns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; map_aa_columns({'locus': 'ABC', 'length': 100, 'product': 'protein'}, False, False)\n['ABC', '100', 'protein', '', '', '', '']\n</code></pre> Source code in <code>src/baktfold/io/tsv.py</code> <pre><code>def map_aa_columns(feat: dict, custom_db: bool, has_duplicate_locus: bool, fast: bool) -&gt; Sequence[str]:\n    \"\"\"\n    Maps amino acid columns.\n\n    Args:\n      feat (dict): The dictionary containing the features.\n      custom_db (bool): A boolean indicating whether a custom database is used.\n      has_duplicate_locus (bool): A boolean indicating whether there are duplicate loci.\n      fast (bool): A boolean indicating whether AFDBclusters Foldseek search should be skipped\n\n    Returns:\n      Sequence[str]: A sequence of strings containing the mapped amino acid columns.\n\n    Examples:\n      &gt;&gt;&gt; map_aa_columns({'locus': 'ABC', 'length': 100, 'product': 'protein'}, False, False)\n      ['ABC', '100', 'protein', '', '', '', '']\n    \"\"\"\n    # Ensure length exists\n    if 'length' not in feat:\n        feat['length'] = int(len(feat['nt']) / 3)\n\n    xrefs = feat.get('db_xrefs', [])\n\n    # Extract dbxref groups once\n    def join_filtered(prefix: str, replacement: str = None):\n        \"\"\"\n    Joins filtered database cross-references.\n\n    Args:\n      prefix (str): The prefix to filter by.\n      replacement (str): The string to replace the prefix with. Defaults to None.\n\n    Returns:\n      str: The joined filtered database cross-references.\n\n    Examples:\n      &gt;&gt;&gt; join_filtered('swissprot', 'afdb_v6:')\n      'afdb_v6:'\n    \"\"\"\n        if replacement is None:\n            replacement = prefix\n        return ','.join(\n            db.replace(replacement, '') for db in xrefs\n            if prefix in db\n        )\n\n    swissprot   = join_filtered('swissprot', 'afdb_v6:')\n    afdbclust   = join_filtered('afdbclusters_', 'afdb_v6:')\n    pdb         = join_filtered('pdb:')\n    cath        = join_filtered('cath:')\n    custom_refs = join_filtered('custom:', 'custom:custom_')\n\n    # Build the output row\n    row = [feat['locus']]\n\n    # add id if multiple CDS per Locus in that record (euks)\n    if has_duplicate_locus:\n        row.append(feat['id'])\n\n    row.extend([\n        str(feat['length']),\n        feat['product'],\n        swissprot,\n    ])\n\n    # Only add AFDBClusters if not in fast mode\n    if not fast:\n        row.append(afdbclust)\n\n    # Always add these\n    row.extend([\n        pdb,\n        cath,\n    ])\n\n    if custom_db:\n        row.append(custom_refs)\n\n    return row\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.tsv.write_feature_inferences","title":"<code>write_feature_inferences(sequences, features_by_sequence, tsv_path)</code>","text":"<p>Export feature inference statistics in TSV format.</p> Source code in <code>src/baktfold/io/tsv.py</code> <pre><code>def write_feature_inferences(sequences: Sequence[dict], features_by_sequence: Dict[str, dict], tsv_path: Path):\n    \"\"\"Export feature inference statistics in TSV format.\"\"\"\n    logger.info('write tsv: path=%s', tsv_path)\n\n    with tsv_path.open('wt') as fh:\n        fh.write('# Annotated with Bakta\\n')\n        fh.write(f'# Software: v{cfg.version}\\n')\n        fh.write(f\"# Database: v{cfg.version}\\n\") # fix later\n        #fh.write(f\"# Database: v{cfg.db_info['major']}.{cfg.db_info['minor']}, {cfg.db_info['type']}\\n\")\n        fh.write(f'# DOI: {bc.BAKTFOLD_DOI}\\n')\n        fh.write(f'# URL: {bc.BAKTFOLD_URL}\\n')\n        fh.write('#Sequence Id\\tType\\tStart\\tStop\\tStrand\\tLocus Tag\\tScore\\tEvalue\\tQuery Cov\\tSubject Cov\\tId\\tAccession\\n')\n\n        for seq in sequences:\n            for feat in features_by_sequence[seq['id']]:\n                if(feat['type'] in [bc.FEATURE_CDS, bc.FEATURE_SORF]):\n                    score, evalue, query_cov, subject_cov, identity, accession = None, None, None, None, None, '-'\n                    if('ups' in feat or 'ips' in feat):\n                        query_cov = 1\n                        subject_cov = 1\n                        identity = 1\n                        evalue = 0\n                        accession = f\"{bc.DB_XREF_UNIREF}:{feat['ips'][DB_IPS_COL_UNIREF100]}\" if 'ips' in feat else f\"{bc.DB_XREF_UNIPARC}:{feat['ups'][DB_UPS_COL_UNIPARC]}\"\n                    elif('psc' in feat or 'pscc' in feat):\n                        psc_type = 'psc' if 'psc' in feat else 'pscc'\n                        query_cov = feat[psc_type]['query_cov']\n                        subject_cov = feat[psc_type].get('subject_cov', -1)\n                        identity = feat[psc_type]['identity']\n                        score = feat[psc_type].get('score', -1)\n                        evalue = feat[psc_type].get('evalue', -1)\n                        accession = f\"{bc.DB_XREF_UNIREF}:{feat['psc'][DB_PSC_COL_UNIREF90]}\" if 'psc' in feat else f\"{bc.DB_XREF_UNIREF}:{feat['pscc'][DB_PSCC_COL_UNIREF50]}\"\n                    fh.write('\\t'.join(\n                        [\n                            feat['sequence'] if 'sequence' in feat else feat['contig'],  # &lt;1.10.0 compatibility\n                            feat['type'],\n                            str(feat['start']),\n                            str(feat['stop']),\n                            feat['strand'],\n                            feat['locus'],\n                            f\"{score:0.1f}\" if score != None else '-',\n                            ('0.0' if evalue == 0 else f\"{evalue:1.1e}\") if evalue != None else '-',\n                            ('1.0' if query_cov == 1 else f\"{query_cov:0.3f}\") if query_cov != None else '-',\n                            ('1.0' if subject_cov == 1 else f\"{subject_cov:0.3f}\") if subject_cov != None else '-',\n                            ('1.0' if identity == 1 else f\"{identity:0.3f}\") if identity != None else '-',\n                            accession\n                        ])\n                    )\n                    fh.write('\\n')\n                elif(feat['type'] in [bc.FEATURE_T_RNA, bc.FEATURE_R_RNA, bc.FEATURE_NC_RNA, bc.FEATURE_NC_RNA_REGION]):\n                    accession = '-' if feat['type'] == bc.FEATURE_T_RNA else [xref for xref in feat['db_xrefs'] if bc.DB_XREF_RFAM in xref][0]\n                    fh.write('\\t'.join(\n                        [\n                            feat['sequence'] if 'sequence' in feat else feat['contig'],  # &lt;1.10.0 compatibility\n                            feat['type'],\n                            str(feat['start']),\n                            str(feat['stop']),\n                            feat['strand'],\n                            feat['locus'] if 'locus' in feat else '-',\n                            f\"{feat['score']:0.1f}\",\n                            ('0.0' if feat['evalue'] == 0 else f\"{feat['evalue']:1.1e}\") if 'evalue' in feat else '-',\n                            ('1.0' if feat['query_cov'] == 1 else f\"{feat['query_cov']:0.3f}\") if 'query_cov' in feat else '-',\n                            ('1.0' if feat['subject_cov'] == 1 else f\"{feat['subject_cov']:0.3f}\") if 'subject_cov' in feat else '-',\n                            ('1.0' if feat['identity'] == 1 else f\"{feat['identity']:0.3f}\") if 'identity' in feat else '-',\n                            accession\n                        ])\n                    )\n                    fh.write('\\n')\n    return\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.tsv.write_features","title":"<code>write_features(sequences, features_by_sequence, tsv_path)</code>","text":"<p>Export features in TSV format.</p> Source code in <code>src/baktfold/io/tsv.py</code> <pre><code>def write_features(sequences: Sequence[dict], features_by_sequence: Dict[str, dict], tsv_path: Path):\n    \"\"\"Export features in TSV format.\"\"\"\n    logger.info(f'write feature tsv: path={tsv_path}')\n\n    with tsv_path.open('wt') as fh:\n        fh.write('# Annotated with Baktfold\\n')\n        fh.write(f'# Software: v{cfg.version}\\n')\n        fh.write(f\"# Database: v{cfg.version}\\n\") # fix later\n        #fh.write(f\"# Database: v{cfg.db_info['major']}.{cfg.db_info['minor']}, {cfg.db_info['type']}\\n\")\n        fh.write(f'# DOI: {bc.BAKTFOLD_DOI}\\n')\n        fh.write(f'# URL: {bc.BAKTFOLD_URL}\\n')\n        fh.write('#Sequence Id\\tType\\tStart\\tStop\\tStrand\\tLocus Tag\\tGene\\tProduct\\tDbXrefs\\n')\n\n        for seq in sequences:\n            for feat in features_by_sequence[seq['id']]:\n                seq_id = feat['sequence'] if 'sequence' in feat else feat['contig']  # &lt;1.10.0 compatibility\n                feat_type = feat['type']\n                if(feat_type == bc.FEATURE_GAP):\n                    feat_type = bc.INSDC_FEATURE_ASSEMBLY_GAP if feat['length'] &gt;= 100 else bc.INSDC_FEATURE_GAP\n\n                gene = feat['gene'] if feat.get('gene', None) else ''\n                product = feat.get('product', '')\n                if(bc.PSEUDOGENE in feat):\n                    product = f\"(pseudo) {product}\"\n                elif(feat.get('truncated', '') == bc.FEATURE_END_5_PRIME):\n                    product = f\"(5' truncated) {product}\"\n                elif(feat.get('truncated', '') == bc.FEATURE_END_3_PRIME):\n                    product = f\"(3' truncated) {product}\"\n                elif(feat.get('truncated', '') == bc.FEATURE_END_BOTH):\n                    product = f\"(partial) {product}\"\n\n                def s(x):\n                    return '' if x is None else str(x)\n\n                fh.write('\\t'.join(\n                    [\n                        seq_id,\n                        feat_type,\n                        str(feat['start']),\n                        str(feat['stop']),\n                        str(feat['strand']),\n                        s(feat.get('locus')), # handles None \u2192 ''\n                        s(gene),        # handles None \u2192 ''\n                        s(product),     # handles None \u2192 ''\n                        ', '.join(sorted(feat.get('db_xrefs', [])))\n                    ])\n                )\n                fh.write('\\n')\n                if(feat_type == bc.FEATURE_CRISPR):\n                    i = 0\n                    # spacers and repeats wont exist if Prokka input\n                    spacers = feat.get('spacers', [])\n                    repeat = feat.get('repeat', [])\n\n                    if len(spacers) &gt; 0 and len(repeat) &gt; 0: \n                    # if not - will just skip\n                        while i &lt; len(feat['spacers']):\n                            repeat = feat['repeats'][i]\n                            fh.write('\\t'.join([seq_id, bc.FEATURE_CRISPR_REPEAT, str(repeat['start']), str(repeat['stop']), repeat['strand'], '', '', f\"CRISPR repeat\", '']))\n                            fh.write('\\n')\n                            spacer = feat['spacers'][i]\n                            fh.write('\\t'.join([seq_id, bc.FEATURE_CRISPR_SPACER, str(spacer['start']), str(spacer['stop']), spacer['strand'], '', '', f\"CRISPR spacer, sequence {spacer['sequence']}\", '']))\n                            fh.write('\\n')\n                            i += 1\n                        if(len(feat['repeats']) - 1 == i):\n                            repeat = feat['repeats'][i]\n                            fh.write('\\t'.join([seq_id, bc.FEATURE_CRISPR_REPEAT, str(repeat['start']), str(repeat['stop']), repeat['strand'], '', '', f\"CRISPR repeat\", '']))\n                            fh.write('\\n')\n    return\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.tsv.write_hypotheticals","title":"<code>write_hypotheticals(hypotheticals, tsv_path)</code>","text":"<p>Export hypothetical information in TSV format.</p> Source code in <code>src/baktfold/io/tsv.py</code> <pre><code>def write_hypotheticals(hypotheticals: Sequence[dict], tsv_path: Path):\n    \"\"\"Export hypothetical information in TSV format.\"\"\"\n    logger.info('write hypothetical tsv: path=%s', tsv_path)\n\n    with tsv_path.open('wt') as fh:\n        fh.write(f'#Annotated with Baktfold v{cfg.version}, https://github.com/oschwengers/bakta\\n')\n        #fh.write(f\"#Database v{cfg.db_info['major']}.{cfg.db_info['minor']}, https://doi.org/10.5281/zenodo.4247252\\n\")\n        fh.write('#Sequence Id\\tStart\\tStop\\tStrand\\tLocus Tag\\tMol Weight [kDa]\\tIso El. Point\\tPfam hits\\tDbxrefs\\n')\n        for hypo in hypotheticals:\n            pfams = [f\"{pfam['id']}|{pfam['name']}\" for pfam in hypo.get('pfams', [])]\n            seq_stats = hypo['seq_stats']\n            mol_weight = f\"{(seq_stats['molecular_weight']/1000):.1f}\" if seq_stats['molecular_weight'] else 'NA'\n            iso_point = f\"{seq_stats['isoelectric_point']:.1f}\" if seq_stats['isoelectric_point'] else 'NA'\n            seq_id = hypo['sequence'] if 'sequence' in hypo else hypo['contig']  # &lt;1.10.0 compatibility\n            fh.write(f\"{seq_id}\\t{hypo['start']}\\t{hypo['stop']}\\t{hypo['strand']}\\t{hypo.get('locus', '')}\\t{mol_weight}\\t{iso_point}\\t{', '.join(sorted(pfams))}\\t{', '.join(sorted(hypo.get('db_xrefs', [])))}\\n\")\n    return\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.tsv.write_protein_features","title":"<code>write_protein_features(features, header_columns, tsv_path, custom_db, has_duplicate_locus, fast)</code>","text":"<p>Export protein features in TSV format.</p> Source code in <code>src/baktfold/io/tsv.py</code> <pre><code>def write_protein_features(features: Sequence[dict], header_columns: Sequence[str], tsv_path: Path, custom_db: bool, has_duplicate_locus: bool, fast: bool):\n    \"\"\"Export protein features in TSV format.\"\"\"\n    logger.info(f'write protein feature tsv: path={tsv_path}')\n\n    with tsv_path.open('wt') as fh:\n        fh.write(f'#Annotated with Baktfold (v{cfg.version}): https://github.com/gbouras13/baktfold\\n')\n        #fh.write(f\"#Database (v{cfg.db_info['major']}.{cfg.db_info['minor']}): https://doi.org/10.5281/zenodo.4247252\\n\")\n        fh.write('\\t'.join(header_columns))\n        fh.write('\\n')\n        for feat in features:\n            columns = map_aa_columns(feat, custom_db, has_duplicate_locus, fast)\n            fh.write('\\t'.join(columns))\n            fh.write('\\n')\n    return\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.fasta_in.parse_protein_input","title":"<code>parse_protein_input(input_path, faa_path)</code>","text":"<p>handles regular FASTA and gzipped  returns cds_dict</p> Source code in <code>src/baktfold/io/fasta_in.py</code> <pre><code>def parse_protein_input(input_path, faa_path):\n    \"\"\"\n    handles regular FASTA and gzipped \n    returns cds_dict\n    \"\"\"\n\n    # handles regular FASTA and gzipped \n\n    try:\n        if input_path == '':\n            raise ValueError('File path argument must be non-empty')\n        input_path = Path(input_path).resolve()\n    except:\n        logger.error(f'ERROR: annotation file {input_path} not valid!')\n\n\n    try:\n        logger.info('Parsing input protein sequences...')\n        aas = fasta.import_sequences(input_path, False, False)\n        logger.info(f'Imported sequences={len(aas)}')\n    except:\n        logger.error('ERROR: wrong file format or unallowed characters in amino acid sequences!')\n\n    mock_start = 1\n    for aa in aas:  # rename and mock feature attributes to reuse existing functions\n        aa['type'] = bc.FEATURE_CDS\n        aa['locus'] = aa['id']\n        aa['sequence'] = '-'\n        aa['start'] = mock_start\n        aa['stop'] = mock_start + aa['length'] - 1\n        aa['strand'] = bc.STRAND_UNKNOWN\n        aa['frame'] = 1\n        mock_start += 100\n\n    # write hypothetical proteins to file\n    with faa_path.open('wt') as fh:\n        for aa in aas:\n            fh.write(f\"&gt;{aa['locus']}\\n{aa['aa']}\\n\")\n\n    logger.info('Parsing complete')\n\n    return aas\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.gff.encode_annotations","title":"<code>encode_annotations(annotations)</code>","text":"<p>Encodes annotations into a string.</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>dict</code> <p>A dictionary containing the annotations.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The encoded annotations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; encode_annotations({\n    'ID': 'EHICP_3230_sigpep',\n    'Name': 'signal peptide',\n    'product': 'signal peptide',\n    'score': 0.5,\n    'Parent': 'EHICP_3230'\n})\n'ID=EHICP_3230_sigpep;Name=signal peptide;product=signal peptide;score=0.5;Parent=EHICP_3230'\n</code></pre> Source code in <code>src/baktfold/io/gff.py</code> <pre><code>def encode_annotations(annotations: Dict[str, Union[str, Sequence[str]]]) -&gt; str:\n    \"\"\"\n    Encodes annotations into a string.\n\n    Args:\n      annotations (dict): A dictionary containing the annotations.\n\n    Returns:\n      str: The encoded annotations.\n\n    Examples:\n      &gt;&gt;&gt; encode_annotations({\n          'ID': 'EHICP_3230_sigpep',\n          'Name': 'signal peptide',\n          'product': 'signal peptide',\n          'score': 0.5,\n          'Parent': 'EHICP_3230'\n      })\n      'ID=EHICP_3230_sigpep;Name=signal peptide;product=signal peptide;score=0.5;Parent=EHICP_3230'\n    \"\"\"\n    annotation_strings = []\n    for key, val in annotations.items():\n        if(type(val) is list):\n            if(len(val) &gt;= 1):\n                val = [encode_attribute(k) for k in val]\n                annotation = f\"{key}={','.join(val)}\"\n                annotation_strings.append(annotation)\n        else:\n            annotation_strings.append(f'{key}={encode_attribute(val)}')\n    return ';'.join(annotation_strings)\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.gff.encode_attribute","title":"<code>encode_attribute(product)</code>","text":"<p>Replace special characters forbidden in column 9 of the GFF3 format: https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md</p> Source code in <code>src/baktfold/io/gff.py</code> <pre><code>def encode_attribute(product: str) -&gt; str:\n    \"\"\"Replace special characters forbidden in column 9 of the GFF3 format: https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md\"\"\"\n    product = str(product)\n    product = product.replace('%', '%25')\n    product = product.replace(';', '%3B')\n    product = product.replace('=', '%3D')\n    product = product.replace('&amp;', '%26')\n    product = product.replace(',', '%2C')\n    return product\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.gff.write_euk_cds_feature","title":"<code>write_euk_cds_feature(fh, seq_id, feat)</code>","text":"<p>Write a eukaryotic CDS feature to GFF3 with multiple CDS parts.</p>"},{"location":"reference/io/#src.baktfold.io.gff.write_euk_cds_feature--parameters","title":"Parameters","text":"<p>fh : file-handle seq_id : str</p> dict-like feature with keys: <p>\"start\", \"stop\", \"strand\", \"locus\", \"starts\", \"stops\"</p> Source code in <code>src/baktfold/io/gff.py</code> <pre><code>def write_euk_cds_feature(fh, seq_id, feat):\n    \"\"\"\n    Write a eukaryotic CDS feature to GFF3 with multiple CDS parts.\n\n    Parameters\n    ----------\n    fh : file-handle\n    seq_id : str\n    feat : dict-like feature with keys:\n            \"start\", \"stop\", \"strand\", \"locus\", \"starts\", \"stops\"\n    \"\"\"\n\n    strand = feat.get(\"strand\", \"+\")\n    locus = feat.get(\"locus\", \"unknown\")\n\n    transcript_id = f\"{locus}-T1\"\n    cds_id = f\"{transcript_id}.cds\"\n\n    starts = feat.get(\"starts\")\n    stops = feat.get(\"stops\")\n\n    # -------------------------------\n    # 1. Determine CDS sub-coordinates\n    # -------------------------------\n    if (\n        isinstance(starts, list)\n        and isinstance(stops, list)\n        and len(starts) == len(stops)\n        and len(starts) &gt; 0\n    ):\n        cds_coords = list(zip(starts, stops))\n    else:\n        cds_coords = [(feat[\"start\"], feat[\"stop\"])]\n\n    # -------------------------------\n    # 2. Reverse order for negative strand\n    # -------------------------------\n    if strand == \"-\":\n        cds_coords.reverse()\n\n    # -------------------------------\n    # 3. Emit CDS lines with correct phase\n    # -------------------------------\n    offset = 0\n\n    for i, (cds_start, cds_stop) in enumerate(cds_coords, start=1):\n\n        length = cds_stop - cds_start + 1\n        phase = offset % 3\n        offset += length\n\n        attr = f\"ID={cds_id}-{i};Parent={transcript_id}\"\n\n        fh.write(\n            f\"{seq_id}\\tbaktfold\\tCDS\\t{cds_start}\\t{cds_stop}\"\n            f\"\\t.\\t{strand}\\t{phase}\\t{attr}\\n\"\n        )\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.gff.write_euk_repeat_region_feature","title":"<code>write_euk_repeat_region_feature(fh, seq_id, feat)</code>","text":"<p>Writes a repeat region feature to a file.</p> <p>Parameters:</p> Name Type Description Default <code>fh</code> <code>file</code> <p>The file handle to write to.</p> required <code>seq_id</code> <code>str</code> <p>The sequence ID.</p> required <code>feat</code> <code>dict</code> <p>A dictionary containing the feature information.</p> required <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; write_euk_repeat_region_feature(fh, 'DS572673.1', {\n    \"type\": \"repeat_region\",\n    \"sequence\": \"DS571531.1\",\n    \"start\": 1470,\n    \"stop\": 1716,\n    \"strand\": \"?\",\n    \"family\": \"LINE2\",\n    \"rpt_type\": null,\n    \"repeat_unit\": null,\n    \"product\": null,\n    \"nt\": \"AATAAAATCATATCAGAAATAAAAAGAATGAAAATAAACAAATTAAAGAAAATAATTATAAAATTAATAAACGATATTTAAATGAAAGAAAATAGAGAATATGTAATAAGTACAAATGGTTCATTCATTAATAAGAAATTAACAATAATAAAATAGAGAATATTGATTATAAAAAGAAATATATTTCTCAAAACAGTAGAGATACAAAAAGAATAGATATGAAATAAATATTAATTCTAAAATACTC\",\n    \"id\": \"EHICP_3230\",\n    \"db_xrefs\": [\n        \"SO:0000657\"\n    ]\n})\n</code></pre> Source code in <code>src/baktfold/io/gff.py</code> <pre><code>def write_euk_repeat_region_feature(fh, seq_id, feat):\n    \"\"\"\n    Writes a repeat region feature to a file.\n\n    Args:\n      fh (file): The file handle to write to.\n      seq_id (str): The sequence ID.\n      feat (dict): A dictionary containing the feature information.\n\n    Returns:\n      None\n\n    Examples:\n      &gt;&gt;&gt; write_euk_repeat_region_feature(fh, 'DS572673.1', {\n          \"type\": \"repeat_region\",\n          \"sequence\": \"DS571531.1\",\n          \"start\": 1470,\n          \"stop\": 1716,\n          \"strand\": \"?\",\n          \"family\": \"LINE2\",\n          \"rpt_type\": null,\n          \"repeat_unit\": null,\n          \"product\": null,\n          \"nt\": \"AATAAAATCATATCAGAAATAAAAAGAATGAAAATAAACAAATTAAAGAAAATAATTATAAAATTAATAAACGATATTTAAATGAAAGAAAATAGAGAATATGTAATAAGTACAAATGGTTCATTCATTAATAAGAAATTAACAATAATAAAATAGAGAATATTGATTATAAAAAGAAATATATTTCTCAAAACAGTAGAGATACAAAAAGAATAGATATGAAATAAATATTAATTCTAAAATACTC\",\n          \"id\": \"EHICP_3230\",\n          \"db_xrefs\": [\n              \"SO:0000657\"\n          ]\n      })\n    \"\"\"\n\n    start = int(feat['start'])\n    stop  = int(feat['stop'])\n    strand = feat['strand']\n\n    id = feat['sequence']\n\n    attrs = {\n        \"ID\": f\"{id}:{start}..{stop}\",\n        \"gbkey\": \"repeat_region\"\n    }\n\n    if feat.get('family') is not None:\n        attrs[\"rpt_family\"] = feat.get('family')\n\n    attr_str = \";\".join(f\"{k}={v}\" for k, v in attrs.items())\n\n    fh.write(f\"{seq_id}\\tbaktfold\\trepeat_region\\t{start}\\t{stop}\\t.\\t{strand}\\t.\\t{attr_str}\\n\")\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.gff.write_euk_trna_feature","title":"<code>write_euk_trna_feature(fh, seq_id, feat)</code>","text":"<p>Write a tRNA feature to GFF3 with a top-level line and single exon.</p>"},{"location":"reference/io/#src.baktfold.io.gff.write_euk_trna_feature--parameters","title":"Parameters","text":"file-like <p>Open file handle to write GFF lines.</p> str <p>Sequence/contig ID.</p> SeqFeature <p>Biopython SeqFeature object of type 'tRNA'.</p>"},{"location":"reference/io/#src.baktfold.io.gff.write_euk_trna_feature--notes","title":"Notes","text":"<ul> <li>Generates one tRNA line and one exon line.</li> <li>Includes optional 'product' qualifier.</li> </ul> Source code in <code>src/baktfold/io/gff.py</code> <pre><code>def write_euk_trna_feature(fh, seq_id, feat):\n    \"\"\"\n    Write a tRNA feature to GFF3 with a top-level line and single exon.\n\n    Parameters\n    ----------\n    fh : file-like\n        Open file handle to write GFF lines.\n    seq_id : str\n        Sequence/contig ID.\n    feat : SeqFeature\n        Biopython SeqFeature object of type 'tRNA'.\n\n    Notes\n    -----\n    - Generates one tRNA line and one exon line.\n    - Includes optional 'product' qualifier.\n    \"\"\"\n    start = int(feat['start'])\n    stop  = int(feat['stop'])\n\n    strand = feat['strand']\n\n    locus = feat['locus']\n\n    trna_id = f\"{locus}-T1\"\n\n    # Top-level tRNA attributes\n    attrs = {\n        \"ID\": trna_id,\n        \"Parent\": locus\n    }\n\n    attrs = {}\n\n    product = feat.get(\"product\", [])\n\n    if product:\n\n        key = \"product\"         \n        if isinstance(product, list):\n            if len(product) == 1:\n                attrs[key] = str(product[0])\n            else:\n                attrs[key] = \",\".join(str(v) for v in product)\n        else:\n            attrs[key] = str(product)\n\n\n    attr_str = \";\".join(f\"{k}={v}\" for k, v in attrs.items())\n\n    # Write top-level tRNA line\n    fh.write(f\"{seq_id}\\tbaktfold\\ttRNA\\t{start}\\t{stop}\\t.\\t{strand}\\t.\\t{attr_str}\\n\")\n\n    # Write exon line (tRNA single-exon)\n    exon_id = f\"{trna_id}.ex\u00dfon1\"\n    exon_attrs = f\"ID={exon_id};Parent={trna_id}\"\n    fh.write(f\"{seq_id}\\tbaktfold\\texon\\t{start}\\t{stop}\\t.\\t{strand}\\t.\\t{exon_attrs}\\n\")\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.gff.write_euk_utr_feature","title":"<code>write_euk_utr_feature(fh, seq_id, feat, locus_counter, three=False)</code>","text":"<p>Write a 'utr' feature.</p> Source code in <code>src/baktfold/io/gff.py</code> <pre><code>def write_euk_utr_feature(fh, seq_id, feat, locus_counter, three=False):\n    \"\"\"Write a 'utr' feature.\"\"\"\n    start = int(feat['start'])\n    stop  = int(feat['stop'])\n    strand = feat['strand']\n\n    locus = feat['locus']\n\n    # Count occurrences for this locus\n    count = locus_counter.get(locus, 0) + 1\n    locus_counter[locus] = count\n\n    # Construct ID with suffix -2, -3, etc.\n    # For first entry we keep ID=locus (no -1)\n    if count == 1:\n        utr_id = locus\n    else:\n        utr_id = f\"{locus}-{count}\"\n\n    # Top-level mRNA line\n    attrs = {\n        \"ID\": f\"{utr_id}\",\n        \"Parent\": f\"{locus}\",\n    }\n\n# CAMXCT020000566.1\tEMBL\tthree_prime_UTR\t84568\t84617\t.\t-\t.\tID=id-C1SCF055_LOCUS8420;Parent=gene-C1SCF055_LOCUS8420;Note=ID:SCF055_s1507_g28601.utr3p1%3B~source:feature;gbkey=3'UTR;locus_tag=C1SCF055_LOCUS8420\n# CAMXCT020000566.1\tEMBL\tfive_prime_UTR\t136251\t136259\t.\t-\t.\tID=id-C1SCF055_LOCUS8420-2;Parent=gene-C1SCF055_LOCUS8420;Note=ID:SCF055_s1507_g28601.utr5p1%3B~source:feature;gbkey=5'UTR;locus_tag=C1SCF055_LOCUS8420\n\n    if feat.get('Note') is not None:\n        attrs[\"Note\"] = feat.get('note')\n\n\n    attrs[\"gbkey\"] = \"3'UTR\" if three else \"5'UTR\"\n\n    if feat.get('Note') is not None:\n        attrs[\"locus_tag\"] = feat.get('locus')\n\n    attr_str = \";\".join(f\"{k}={v}\" for k, v in attrs.items())\n\n    if three:\n        gene_tag = 'three_prime_UTR'\n    else:\n        gene_tag = 'five_prime_UTR'\n\n    fh.write(f\"{seq_id}\\tbaktfold\\t{gene_tag}\\t{start}\\t{stop}\\t.\\t{strand}\\t.\\t{attr_str}\\n\")\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.gff.write_features","title":"<code>write_features(data, features_by_sequence, gff3_path, prokka=False, euk=False)</code>","text":"<p>Export features in GFF3 format.</p> Source code in <code>src/baktfold/io/gff.py</code> <pre><code>def write_features(data: dict, features_by_sequence: Dict[str, dict], gff3_path: Path, prokka: bool = False, euk: bool = False):\n    \"\"\"Export features in GFF3 format.\"\"\"\n    logger.info(f'write features: path={gff3_path}')\n\n    with gff3_path.open('wt') as fh:\n        fh.write('##gff-version 3\\n')  # GFF version\n        fh.write('##feature-ontology https://github.com/The-Sequence-Ontology/SO-Ontologies/blob/v3.1/so.obo\\n')  # SO feature version\n\n        if(data['genome'].get('taxon', None)):  # write organism info\n            fh.write(f\"# organism {data['genome']['taxon']}\\n\")\n\n        fh.write('# Annotated with Baktfold\\n')\n        fh.write(f'# Software: v{cfg.version}\\n')\n        fh.write(f\"# Database: v{cfg.version}\\n\") # fix later\n        #fh.write(f\"# Database: v{cfg.db_info['major']}.{cfg.db_info['minor']}, {cfg.db_info['type']}\\n\")\n        fh.write(f'# DOI: {bc.BAKTFOLD_DOI}\\n')\n        fh.write(f'# URL: {bc.BAKTFOLD_URL}\\n')\n\n        for seq in data['sequences']:  # write features\n            if euk:\n                locus_counter = {} # for UTRs\n\n            fh.write(f\"##sequence-region {seq['id']} 1 {seq['length']}\\n\")  # sequence region\n\n            # write landmark region\n            annotations = {\n                'ID': seq['id'],\n                'Name': seq['id']\n            }\n            if(seq['topology'] == bc.TOPOLOGY_CIRCULAR):\n                annotations['Is_circular'] = 'true'\n            annotations = encode_annotations(annotations)\n            fh.write(f\"{seq['id']}\\tBaktfold\\tregion\\t1\\t{str(seq['length'])}\\t.\\t+\\t.\\t{annotations}\\n\")\n\n            for feat in features_by_sequence[seq['id']]:\n                seq_id = feat['sequence'] if 'sequence' in feat else feat['contig']  # &lt;1.10.0 compatibility\n                start = feat['start']\n                stop = feat['stop']\n                if('edge' in feat):\n                    stop += seq['length']\n\n                # euks\n                if euk:\n                    if(feat['type'] == bc.FEATURE_REPEAT):\n                        write_euk_repeat_region_feature(fh, seq_id, feat)\n\n                    if(feat['type'] == bc.FEATURE_5UTR or feat['type'] == bc.FEATURE_3UTR):\n                        if feat['type'] == bc.FEATURE_3UTR:\n                            write_euk_utr_feature(fh, seq_id, feat, locus_counter, three=True)\n                        elif feat['type'] == bc.FEATURE_5UTR:\n                            write_euk_utr_feature(fh, seq_id, feat, locus_counter, three=False)\n\n                if(feat['type'] == bc.FEATURE_T_RNA):\n\n                    if euk:\n                        write_euk_trna_feature(fh, seq_id, feat)\n                    else:\n\n                        trna_tool = \"tRNAscan-SE\"\n                        if prokka:\n                            trna_tool = \"Aragorn\"\n                        annotations = {\n                            'ID': feat['locus'],\n                            'Name': feat['product'],\n                            'locus_tag': feat['locus'],\n                            'product': feat['product'],\n                            'Dbxref': feat['db_xrefs']\n                        }\n                        if(feat.get('gene', None)):  # add gene annotation if available\n                            annotations['gene'] = feat['gene']\n                        if(bc.PSEUDOGENE in feat):\n                            annotations[bc.INSDC_FEATURE_PSEUDOGENE] = bc.INSDC_FEATURE_PSEUDOGENE_TYPE_UNKNOWN\n                        elif('truncated' in feat):\n                            annotations[bc.INSDC_FEATURE_PSEUDO] = True\n                        if(feat.get('anti_codon', False)):\n                            annotations['anti_codon'] = feat['anti_codon']\n                        if(feat.get('amino_acid', False)):\n                            annotations['amino_acid'] = feat['amino_acid']\n                        if(cfg.compliant):\n                            gene_id = f\"{feat['locus']}_gene\"\n                            annotations['Parent'] = gene_id\n                            annotations['inference'] = 'profile:tRNAscan:2.0'\n                            annotations['Dbxref'], annotations['Note'] = insdc.revise_dbxref_insdc(feat['db_xrefs'])  # remove INSDC invalid DbXrefs\n                            gene_annotations = {\n                                'ID': gene_id,\n                                'locus_tag': feat['locus']\n                            }\n                            if(feat.get('gene', None)):\n                                gene_annotations['gene'] = feat['gene']\n                            if(bc.PSEUDOGENE in feat):\n                                gene_annotations[bc.INSDC_FEATURE_PSEUDOGENE] = bc.INSDC_FEATURE_PSEUDOGENE_TYPE_UNKNOWN\n                            gene_annotations = encode_annotations(gene_annotations)\n                            fh.write(f\"{seq_id}\\t{trna_tool}\\tgene\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{gene_annotations}\\n\")\n                        annotations = encode_annotations(annotations)\n                        fh.write(f\"{seq_id}\\t{trna_tool}\\t{so.SO_TRNA.name}\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{annotations}\\n\")\n                elif(feat['type'] == bc.FEATURE_TM_RNA):\n                    # both prokka and bakta use Aragorn\n                    annotations = {\n                        'ID': feat['locus'],\n                        'Name': feat['product'],\n                        'locus_tag': feat['locus'],\n                        'gene': feat['gene'],\n                        'product': feat['product'],\n                        'Dbxref': feat['db_xrefs']\n                    }\n                    if('tag' in feat):\n                        annotations['tag_peptide'] = feat['tag']['aa']\n                    if('truncated' in feat):\n                        annotations[bc.INSDC_FEATURE_PSEUDO] = True\n                    if(cfg.compliant):\n                        gene_id = f\"{feat['locus']}_gene\"\n                        annotations['Parent'] = gene_id\n                        annotations['inference'] = 'profile:aragorn:1.2'\n                        annotations['Dbxref'], annotations['Note'] = insdc.revise_dbxref_insdc(feat['db_xrefs'])  # remove INSDC invalid DbXrefs\n                        if('tag' in feat):\n                            annotations['tag_peptide'] = f\"{feat['tag']['start']}..{feat['tag']['stop']}\" if feat['strand'] == bc.STRAND_FORWARD else f\"complement({feat['tag']['start']}..{feat['tag']['stop']})\"\n                        gene_annotations = {\n                            'ID': gene_id,\n                            'locus_tag': feat['locus'],\n                            'gene': feat['gene']\n                        }\n                        if('truncated' in feat):\n                            gene_annotations[bc.INSDC_FEATURE_PSEUDO] = True\n                        gene_annotations = encode_annotations(gene_annotations)\n                        fh.write(f\"{seq_id}\\tAragorn\\tgene\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{gene_annotations}\\n\")\n                    annotations = encode_annotations(annotations)\n                    fh.write(f\"{seq_id}\\tAragorn\\t{so.SO_TMRNA.name}\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{annotations}\\n\")\n                elif(feat['type'] == bc.FEATURE_R_RNA):\n                    rrna_tool = \"Infernal\"\n                    if prokka:\n                        rrna_tool = \"barrnap\"\n                    annotations = {\n                        'ID': feat['locus'],\n                        'Name': feat['product'],\n                        'locus_tag': feat['locus'],\n                        'gene': feat['gene'],\n                        'product': feat['product'],\n                        'Dbxref': feat['db_xrefs']\n                    }\n                    if('truncated' in feat):\n                        annotations[bc.INSDC_FEATURE_PSEUDO] = True\n                    if(cfg.compliant):\n                        gene_id = f\"{feat['locus']}_gene\"\n                        annotations['Parent'] = gene_id\n                        annotations['Dbxref'], annotations['Note'] = insdc.revise_dbxref_insdc(feat['db_xrefs'])  # remove INSDC invalid DbXrefs\n                        for rfam_id in [dbxref.split(':')[1] for dbxref in feat['db_xrefs'] if dbxref.split(':')[0] == bc.DB_XREF_RFAM]:\n                            annotations['inference'] = f'profile:Rfam:{rfam_id}'\n                        gene_annotations = {\n                            'ID': gene_id,\n                            'locus_tag': feat['locus'],\n                            'gene': feat['gene']\n                        }\n                        if('truncated' in feat):\n                            gene_annotations[bc.INSDC_FEATURE_PSEUDO] = True\n                        gene_annotations = encode_annotations(gene_annotations)\n                        fh.write(f\"{seq_id}\\t{rrna_tool}\\tgene\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{gene_annotations}\\n\")\n                    annotations = encode_annotations(annotations)\n                    fh.write(f\"{seq_id}\\t{rrna_tool}\\t{so.SO_RRNA.name}\\t{start}\\t{stop}\\t{feat['evalue']}\\t{feat['strand']}\\t.\\t{annotations}\\n\")\n                elif(feat['type'] == bc.FEATURE_NC_RNA):\n                    # both prokka and bakta use infernal for ncrna\n                    annotations = {\n                        'ID': feat['locus'],\n                        'Name': feat['product'],\n                        'locus_tag': feat['locus'],\n                        'gene': feat['gene'],\n                        'product': feat['product'],\n                        'Dbxref': feat['db_xrefs']\n                    }\n                    if('truncated' in feat):\n                        annotations[bc.INSDC_FEATURE_PSEUDO] = True\n                    if(cfg.compliant):\n                        gene_id = f\"{feat['locus']}_gene\"\n                        annotations['Parent'] = gene_id\n                        annotations['Dbxref'], annotations['Note'] = insdc.revise_dbxref_insdc(feat['db_xrefs'])  # remove INSDC invalid DbXrefs\n                        annotations[bc.INSDC_FEATURE_NC_RNA_CLASS] = insdc.select_ncrna_class(feat)\n                        for rfam_id in [dbxref.split(':')[1] for dbxref in feat['db_xrefs'] if dbxref.split(':')[0] == bc.DB_XREF_RFAM]:\n                            annotations['inference'] = f'profile:Rfam:{rfam_id}'\n                        gene_annotations = {\n                            'ID': gene_id,\n                            'locus_tag': feat['locus'],\n                            'gene': feat['gene']\n                        }\n                        if(ba.RE_GENE_SYMBOL.fullmatch(feat['gene'])):  # discard non-standard ncRNA gene symbols\n                            gene_annotations['gene'] = feat['gene']\n                        else:\n                            annotations.pop('gene', None)\n                        if('truncated' in feat):\n                            gene_annotations[bc.INSDC_FEATURE_PSEUDO] = True\n                        gene_annotations = encode_annotations(gene_annotations)\n                        fh.write(f\"{seq_id}\\tInfernal\\tgene\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{gene_annotations}\\n\")\n                    annotations = encode_annotations(annotations)\n                    fh.write(f\"{seq_id}\\tInfernal\\t{so.SO_NCRNA_GENE.name}\\t{start}\\t{stop}\\t{feat['evalue']}\\t{feat['strand']}\\t.\\t{annotations}\\n\")\n                elif(feat['type'] == bc.FEATURE_NC_RNA_REGION):\n                    annotations = {\n                        'ID': feat['id'],\n                        'Name': feat['product'],\n                        'product': feat['product'],\n                        'Dbxref': feat['db_xrefs']\n                    }\n                    if('truncated' in feat):\n                        annotations[bc.INSDC_FEATURE_PSEUDO] = True\n                    if(cfg.compliant):\n                        for rfam_id in [dbxref.split(':')[1] for dbxref in feat['db_xrefs'] if dbxref.split(':')[0] == bc.DB_XREF_RFAM]:\n                            annotations['inference'] = f'profile:Rfam:{rfam_id}'\n                        annotations['Dbxref'], annotations['Note'] = insdc.revise_dbxref_insdc(feat['db_xrefs'])  # remove INSDC invalid DbXrefs\n                        annotations[bc.INSDC_FEATURE_REGULATORY_CLASS] = insdc.select_regulatory_class(feat)\n                    annotations = encode_annotations(annotations)\n                    fh.write(f\"{seq_id}\\tInfernal\\t{so.SO_REGULATORY_REGION.name}\\t{start}\\t{stop}\\t{feat['evalue']}\\t{feat['strand']}\\t.\\t{annotations}\\n\")\n                elif(feat['type'] == bc.FEATURE_CRISPR):\n                    crispr_tool = \"PILER-CR\"\n                    if prokka:\n                        crispr_tool = \"MinCED\"\n                    annotations = {\n                        'ID': feat['id'],\n                        'Name': feat['product'],\n                        'product': feat['product']\n                    }\n                    feat_type = so.SO_CRISPR.name\n                    if(cfg.compliant):\n                        feat_type = bc.INSDC_FEATURE_REPEAT_REGION\n                        annotations['inference'] = 'COORDINATES:alignment:pilercr:1.02'\n                        annotations['Dbxref'], annotations['Note'] = insdc.revise_dbxref_insdc(feat['db_xrefs'])  # remove INSDC invalid DbXrefs\n                        annotations[bc.INSDC_FEATURE_REPEAT_FAMILY] = 'CRISPR'\n                        annotations[bc.INSDC_FEATURE_REPEAT_TYPE] = 'direct'\n                        annotations[bc.INSDC_FEATURE_REPEAT_UNIT_SEQ] = feat['repeat_consensus']\n                    annotations = encode_annotations(annotations)\n                    fh.write(f\"{seq_id}\\t{crispr_tool}\\t{feat_type}\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{annotations}\\n\")\n                    if(not cfg.compliant):\n                        i = 0\n                        # spacers and repeats wont exist if Prokka input\n                        spacers = feat.get('spacers', [])\n                        repeat = feat.get('repeat', [])\n                        if len(spacers) &gt; 0 and len(repeat) &gt; 0: \n                            while i &lt; len(feat['spacers']):\n                                repeat = feat['repeats'][i]\n                                annotations = {\n                                    'ID': f\"{feat['id']}_repeat_{i+1}\",\n                                    'Parent': feat['id']\n                                }\n                                annotations = encode_annotations(annotations)\n                                # will always be PILER here as prokka won't have any\n                                fh.write(f\"{seq_id}\\tPILER-CR\\t{bc.FEATURE_CRISPR_REPEAT}\\t{repeat['start']}\\t{repeat['stop']}\\t.\\t{repeat['strand']}\\t.\\t{annotations}\\n\")\n                                spacer = feat['spacers'][i]\n                                annotations = {\n                                    'ID': f\"{feat['id']}_spacer_{i+1}\",\n                                    'Parent': feat['id'],\n                                    'sequence': spacer['sequence']\n                                }\n                                annotations = encode_annotations(annotations)\n                                fh.write(f\"{seq_id}\\tPILER-CR\\t{bc.FEATURE_CRISPR_SPACER}\\t{spacer['start']}\\t{spacer['stop']}\\t.\\t{spacer['strand']}\\t.\\t{annotations}\\n\")\n                                i += 1\n                            if(len(feat['repeats']) - 1 == i):\n                                repeat = feat['repeats'][i]\n                                annotations = { 'ID': f\"{feat['id']}_repeat_{i+1}\" }\n                                annotations = encode_annotations(annotations)\n                                fh.write(f\"{seq_id}\\tPILER-CR\\t{bc.FEATURE_CRISPR_REPEAT}\\t{repeat['start']}\\t{repeat['stop']}\\t.\\t{repeat['strand']}\\t.\\t{annotations}\\n\")\n                elif feat['type'] == bc.FEATURE_CDS:\n                    if euk:\n                        write_euk_cds_feature(fh, seq_id, feat)\n                    else:\n                        annotations = {\n                            'ID': feat['locus'],\n                            'Name': feat['product'],\n                            'locus_tag': feat['locus'],\n                            'product': feat['product'],\n                            'Dbxref': feat['db_xrefs']\n                        }\n                        if(bc.PSEUDOGENE in feat):\n                            annotations[bc.INSDC_FEATURE_PSEUDOGENE] = bc.INSDC_FEATURE_PSEUDOGENE_TYPE_UNPROCESSED if feat[bc.PSEUDOGENE]['paralog'] else bc.INSDC_FEATURE_PSEUDOGENE_TYPE_UNITARY\n                        elif('truncated' in feat):\n                            annotations[bc.INSDC_FEATURE_PSEUDO] = True\n                        if(feat.get('gene', None)):  # add gene annotation if available\n                            annotations['gene'] = feat['gene']\n                        source = '?' if feat.get('source', None) == bc.CDS_SOURCE_USER else 'Pyrodigal'\n                        if prokka: \n                            source = 'Prodigal'\n                        if(cfg.compliant):\n                            gene_id = f\"{feat['locus']}_gene\"\n                            annotations['Parent'] = gene_id\n                            annotations['inference'] = 'EXISTENCE:non-experimental evidence, no additional details recorded' if feat.get('source', None) == bc.CDS_SOURCE_USER else 'ab initio prediction:Pyrodigal:3.5'\n                            annotations['Dbxref'], annotations['Note'] = insdc.revise_dbxref_insdc(feat['db_xrefs'])  # remove INSDC invalid DbXrefs\n                            annotations['Note'], ec_number = insdc.extract_ec_from_notes_insdc(annotations, 'Note')\n                            if(ec_number is not None):\n                                annotations['ec_number'] = ec_number\n                            gene_annotations = {\n                                'ID': gene_id,\n                                'locus_tag': feat['locus']\n                            }\n                            if(feat.get('gene', None)):\n                                gene_annotations['gene'] = feat['gene']\n                            if(bc.PSEUDOGENE in feat):\n                                gene_annotations[bc.INSDC_FEATURE_PSEUDOGENE] = bc.INSDC_FEATURE_PSEUDOGENE_TYPE_UNPROCESSED if feat[bc.PSEUDOGENE]['paralog'] else bc.INSDC_FEATURE_PSEUDOGENE_TYPE_UNITARY\n                            gene_annotations = encode_annotations(gene_annotations)\n                            fh.write(f\"{seq_id}\\t{source}\\tgene\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{gene_annotations}\\n\")\n                        if('exception' in feat):\n                            ex = feat['exception']\n                            pos = f\"{ex['start']}..{ex['stop']}\"\n                            if(feat['strand'] == bc.STRAND_REVERSE):\n                                pos = f\"complement({pos})\"\n                            annotations['transl_except']=f\"(pos:{pos},aa:{ex['aa']})\"\n                            notes = annotations.get('Note', [])\n                            notes.append(f\"codon on position {ex['codon_position']} is a {ex['type']} codon\")\n                            if('Notes' not in annotations):\n                                annotations['Note'] = notes\n                        annotations = encode_annotations(annotations)\n                        fh.write(f\"{seq_id}\\t{source}\\t{so.SO_CDS.name}\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t0\\t{annotations}\\n\")\n                        if(bc.FEATURE_SIGNAL_PEPTIDE in feat):\n                            write_signal_peptide(fh, feat)\n                elif(feat['type'] == bc.FEATURE_SORF):\n                    annotations = {\n                        'ID': feat['locus'],\n                        'Name': feat['product'],\n                        'locus_tag': feat['locus'],\n                        'product': feat['product'],\n                        'Dbxref': feat['db_xrefs']\n                    }\n                    if(feat.get('gene', None)):  # add gene annotation if available\n                        annotations['gene'] = feat['gene']\n                    if(cfg.compliant):\n                        gene_id = f\"{feat['locus']}_gene\"\n                        annotations['Parent'] = gene_id\n                        annotations['Dbxref'], annotations['Note'] = insdc.revise_dbxref_insdc(feat['db_xrefs'])  # remove INSDC invalid DbXrefs\n                        annotations['Note'], ec_number = insdc.extract_ec_from_notes_insdc(annotations, 'Note')\n                        if(ec_number is not None):\n                            annotations['ec_number'] = ec_number\n                        gene_annotations = {\n                            'ID': gene_id,\n                            'locus_tag': feat['locus'],\n                            'inference': 'ab initio prediction:Bakta'\n                        }\n                        if(feat.get('gene', None)):\n                            gene_annotations['gene'] = feat['gene']\n                        gene_annotations = encode_annotations(gene_annotations)\n                        fh.write(f\"{seq_id}\\tBakta\\tgene\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{gene_annotations}\\n\")\n                    annotations = encode_annotations(annotations)\n                    fh.write(f\"{seq_id}\\tBakta\\t{so.SO_CDS.name}\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t0\\t{annotations}\\n\")\n                    if(bc.FEATURE_SIGNAL_PEPTIDE in feat):\n                        write_signal_peptide(fh, feat)\n                elif(feat['type'] == bc.FEATURE_GAP):\n                    gap_tool=\"Bakta\"\n                    if prokka:\n                        gap_tool=\"Prokka\"\n                    annotations = {\n                        'ID': feat['id'],\n                        'Name': f\"gap ({feat['length']} bp)\",\n                        'product': f\"gap ({feat['length']} bp)\"\n                    }\n                    annotations = encode_annotations(annotations)\n                    fh.write(f\"{seq_id}\\t{gap_tool}\\t{so.SO_GAP.name}\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{annotations}\\n\")\n                elif(feat['type'] == bc.FEATURE_ORIC):\n                    annotations = {\n                        'ID': feat['id'],\n                        'Name': feat['product']\n                    }\n                    if(cfg.compliant):\n                        annotations['Note'] = feat['product']\n                    else:\n                        annotations['product'] = feat['product']\n                        annotations['inference'] = 'similar to DNA sequence'\n                    annotations = encode_annotations(annotations)\n                    feat_type = bc.INSDC_FEATURE_ORIGIN_REPLICATION if cfg.compliant else so.SO_ORIC.name\n                    fh.write(f\"{seq_id}\\tBLAST+\\t{feat_type}\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{annotations}\\n\")\n                elif(feat['type'] == bc.FEATURE_ORIV):\n                    annotations = {\n                        'ID': feat['id'],\n                        'Name': feat['product']\n                    }\n                    if(cfg.compliant):\n                        annotations['Note'] = feat['product']\n                    else:\n                        annotations['product'] = feat['product']\n                        annotations['inference'] = 'similar to DNA sequence'\n                    annotations = encode_annotations(annotations)\n                    feat_type = bc.INSDC_FEATURE_ORIGIN_REPLICATION if cfg.compliant else so.SO_ORIC.name\n                    fh.write(f\"{seq_id}\\tBLAST+\\t{feat_type}\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{annotations}\\n\")\n                elif(feat['type'] == bc.FEATURE_ORIT):\n                    annotations = {\n                        'ID': feat['id'],\n                        'Name': feat['product']\n                    }\n                    if(cfg.compliant):\n                        annotations['Note'] = feat['product']\n                    else:\n                        annotations['product'] = feat['product']\n                        annotations['inference'] = 'similar to DNA sequence'\n                    annotations = encode_annotations(annotations)\n                    feat_type = bc.INSDC_FEATURE_ORIGIN_TRANSFER if cfg.compliant else so.SO_ORIT.name\n                    fh.write(f\"{seq_id}\\tBLAST+\\t{feat_type}\\t{start}\\t{stop}\\t.\\t{feat['strand']}\\t.\\t{annotations}\\n\")\n                elif(feat['type'] == bc.FEATURE_GENE):\n                    write_gene_feature(fh, seq_id, feat)\n                elif(feat['type'] == bc.FEATURE_MRNA):\n                    write_mrna_feature(fh, seq_id, feat)\n\n        if(not cfg.compliant):\n            fh.write('##FASTA\\n')\n            for seq in data['sequences']:  # write sequences\n                fh.write(f\"&gt;{seq['id']}\\n\")\n                seq_nt = seq['nt'] if 'nt' in seq else seq['sequence']  # &lt;1.10.0 compatibility\n                fh.write(fasta.wrap_sequence(seq_nt))\n    return\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.gff.write_gene_feature","title":"<code>write_gene_feature(fh, seq_id, feat)</code>","text":"<p>Write a 'gene' feature including fuzzy boundaries.</p> Source code in <code>src/baktfold/io/gff.py</code> <pre><code>def write_gene_feature(fh, seq_id, feat):\n    \"\"\"Write a 'gene' feature including fuzzy boundaries.\"\"\"\n    start = int(feat['start'])\n    stop  = int(feat['stop'])\n    strand = feat['strand']\n\n    # fall back if there is no locus tag\n    locus = feat.get('locus') or f\"{seq_id}_{start}_{stop}_{strand}\"\n\n    attrs = {\n        \"ID\": f\"{locus}\"\n    }\n\n    if feat.get('gene') is not None:\n        attrs[\"Name\"] = feat.get('gene')\n\n    attr_str = \";\".join(f\"{k}={v}\" for k, v in attrs.items())\n\n    fh.write(f\"{seq_id}\\tbaktfold\\tgene\\t{start}\\t{stop}\\t.\\t{strand}\\t.\\t{attr_str}\\n\")\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.gff.write_mrna_feature","title":"<code>write_mrna_feature(fh, seq_id, feat)</code>","text":"<p>Write mRNA + implied exons based on join() structure.</p> Source code in <code>src/baktfold/io/gff.py</code> <pre><code>def write_mrna_feature(fh, seq_id, feat):\n    \"\"\"Write mRNA + implied exons based on join() structure.\"\"\"\n\n    start = int(feat['start'])\n    stop  = int(feat['stop'])\n    strand = feat['strand']\n\n    # fall back if there is no locus tag\n    locus = feat.get('locus') or f\"{seq_id}_{start}_{stop}_{strand}\"\n\n    mrna_id = f\"{locus}-T1\"\n\n    # Top-level mRNA line\n    attrs = {\n        \"ID\": mrna_id,\n        \"Parent\": f\"{locus}\",\n    }\n\n    product = feat.get(\"product\", [])\n\n    if product:\n\n        key = \"product\"         \n        if isinstance(product, list):\n            if len(product) == 1:\n                attrs[key] = str(product[0])\n            else:\n                attrs[key] = \",\".join(str(v) for v in product)\n        else:\n            attrs[key] = str(product)\n\n\n    # Ensure db_xrefs exists and is a list\n    db_xrefs = feat.get(\"db_xrefs\", [])\n\n    # Access note safely\n    note = feat.get(\"note\", None)\n\n\n    if db_xrefs:\n\n        key = \"Dbxref\"         \n        if isinstance(db_xrefs, list):\n            if len(db_xrefs) == 1:\n                attrs[key] = str(db_xrefs[0])\n            else:\n                attrs[key] = \",\".join(str(v) for v in db_xrefs)\n        else:\n            # if somehow not a list, just convert to string\n            attrs[key] = str(db_xrefs)\n\n    if note:\n\n        key = \"note\"         # &lt;-- you must define this\n        if isinstance(db_xrefs, list):\n            if len(db_xrefs) == 1:\n                attrs[key] = str(db_xrefs[0])\n            else:\n                attrs[key] = \",\".join(str(v) for v in db_xrefs)\n        else:\n            # if somehow not a list, just convert to string\n            attrs[key] = str(db_xrefs)\n\n\n    attr_str = \";\".join(f\"{k}={v}\" for k, v in attrs.items())\n\n    fh.write(f\"{seq_id}\\tbaktfold\\tmRNA\\t{start}\\t{stop}\\t.\\t{strand}\\t.\\t{attr_str}\\n\")\n\n    starts = feat.get(\"starts\")\n    stops  = feat.get(\"stops\")\n    strand = feat.get(\"strand\")\n    seq_id = feat.get(\"sequence\")\n\n    if (\n        isinstance(starts, list)\n        and isinstance(stops, list)\n        and len(starts) == len(stops)\n        and len(starts) &gt; 0\n    ):\n        # For minus strand, exons must be written in reverse order (5'\u21923')\n        if strand == \"-\":\n            exon_parts = list(zip(starts, stops))\n        else:\n            exon_parts = list(zip(starts, stops))\n\n        # Exons must be numbered in biological order (5' to 3')\n        if strand == \"-\":\n            exon_parts = exon_parts[::-1]   # reverse order\n\n        # Write each exon to GFF\n        for idx, (ex_start, ex_stop) in enumerate(exon_parts, start=1):\n            exon_id = f\"{mrna_id}.exon{idx}\"\n            exon_attrs = f\"ID={exon_id};Parent={mrna_id}\"\n            fh.write(\n                f\"{seq_id}\\tbaktfold\\texon\\t{ex_start}\\t{ex_stop}\\t.\\t{strand}\\t.\\t{exon_attrs}\\n\"\n            )\n    else:\n        # Single exon (no starts/stops provided)\n        exon_start = feat[\"start\"]\n        exon_stop = feat[\"stop\"]\n        exon_id = f\"{mrna_id}.exon1\"\n        exon_attrs = f\"ID={exon_id};Parent={mrna_id}\"\n\n        fh.write(\n            f\"{seq_id}\\tbaktfold\\texon\\t{exon_start}\\t{exon_stop}\"\n            f\"\\t.\\t{feat['strand']}\\t.\\t{exon_attrs}\\n\"\n        )\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.gff.write_signal_peptide","title":"<code>write_signal_peptide(fh, feat)</code>","text":"<p>Writes a signal peptide feature to a file.</p> <p>Parameters:</p> Name Type Description Default <code>fh</code> <code>file</code> <p>The file handle to write to.</p> required <code>feat</code> <code>dict</code> <p>A dictionary containing the feature information.</p> required <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; write_signal_peptide(fh, {\n    'locus': 'EHICP_3230',\n    'sequence': 'DS571531.1',\n    'strand': '+',\n    'signal_peptide': {\n        'start': 1,\n        'stop': 20,\n        'score': 0.5\n    }\n})\n</code></pre> Source code in <code>src/baktfold/io/gff.py</code> <pre><code>def write_signal_peptide(fh, feat: dict):  # &lt;1.10.0 compatibility\n    \"\"\"\n    Writes a signal peptide feature to a file.\n\n    Args:\n      fh (file): The file handle to write to.\n      feat (dict): A dictionary containing the feature information.\n\n    Returns:\n      None\n\n    Examples:\n      &gt;&gt;&gt; write_signal_peptide(fh, {\n          'locus': 'EHICP_3230',\n          'sequence': 'DS571531.1',\n          'strand': '+',\n          'signal_peptide': {\n              'start': 1,\n              'stop': 20,\n              'score': 0.5\n          }\n      })\n    \"\"\"\n    sig_peptide = feat[bc.FEATURE_SIGNAL_PEPTIDE]\n    annotations = {\n        'ID': f\"{feat['locus']}_sigpep\",\n        'Name': 'signal peptide',\n        'product': 'signal peptide',\n        'score': sig_peptide['score'],\n        'Parent': feat['locus']\n    }\n    annotations = encode_annotations(annotations)\n    seq_id = feat['sequence'] if 'sequence' in feat else feat['contig']  # &lt;1.10.0 compatibility\n    fh.write(f\"{seq_id}\\tDeepSig\\t{so.SO_SIGNAL_PEPTIDE.name}\\t{sig_peptide['start']}\\t{sig_peptide['stop']}\\t{sig_peptide['score']:.2f}\\t{feat['strand']}\\t.\\t{annotations}\\n\")\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.fasta.export_sequences","title":"<code>export_sequences(sequences, fasta_path, description=False, wrap=False)</code>","text":"<p>Write sequences to Fasta file.</p> Source code in <code>src/baktfold/io/fasta.py</code> <pre><code>def export_sequences(sequences: Sequence[dict], fasta_path: Path, description: bool=False, wrap: bool=False):\n    \"\"\"Write sequences to Fasta file.\"\"\"\n    logger.info(f'write genome sequences: path={fasta_path}, description={description}, wrap={wrap}')\n\n    with fasta_path.open('wt') as fh:\n        for seq in sequences:\n            if(description):\n                fh.write(f\"&gt;{seq['id']} {seq['description']}\\n\")\n            else:\n                fh.write(f\"&gt;{seq['id']}\\n\")\n            if(wrap):\n                fh.write(wrap_sequence(seq['nt'] if 'nt' in seq else seq['sequence']))  # &lt;1.10.0 compatibility\n            else:\n                fh.write(seq['nt'])\n                fh.write('\\n')\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.fasta.import_sequences","title":"<code>import_sequences(sequences_path, is_genomic=True, is_dna=True)</code>","text":"<p>Import raw sequences from Fasta file.</p> Source code in <code>src/baktfold/io/fasta.py</code> <pre><code>def import_sequences(sequences_path: Path, is_genomic: bool=True, is_dna: bool=True) -&gt; Sequence[dict]:\n    \"\"\"Import raw sequences from Fasta file.\"\"\"\n    sequences = []\n    with xopen(str(sequences_path), threads=0) as fh:\n        for record in SeqIO.parse(fh, 'fasta'):\n            sequence = {\n                'id': record.id,\n                'description': record.description.split(' ', maxsplit=1)[1] if ' ' in record.description else ''\n            }\n\n            raw_sequence = str(record.seq).upper()\n            if('-' in raw_sequence):\n                dash_count = raw_sequence.count('-')\n                raw_sequence = raw_sequence.replace('-', '')\n                logger.info('import: Discarded alignment gaps (dashes): id=%s, occurences=%i', record.id, dash_count)\n            if(is_dna):\n                if(FASTA_DNA_SEQUENCE_PATTERN.fullmatch(raw_sequence) is None):\n                    logger.error('import: Fasta sequence contains invalid DNA characters! id=%s', record.id)\n                    raise ValueError(f'Fasta sequence contains invalid DNA characters! id={record.id}')\n                sequence['nt'] = raw_sequence\n            else:\n                if(raw_sequence[-1] == '*'):  # remove trailing stop asterik\n                    raw_sequence = raw_sequence[:-1]\n                    logger.warning('import: Removed trailing asterik! id=%s, seq=%s', record.id, raw_sequence)\n                if(FASTA_AA_SEQUENCE_PATTERN.fullmatch(raw_sequence) is None):\n                    logger.error('import: Fasta sequence contains invalid AA characters! id=%s, seq=%s', record.id, raw_sequence)\n                    raise ValueError(f'Fasta sequence contains invalid AA characters! id={record.id}')\n                sequence['aa'] = raw_sequence\n            sequence['length'] = len(raw_sequence)\n            if(is_genomic):\n                sequence['complete'] = False\n                sequence['type'] = bc.REPLICON_CONTIG\n                sequence['topology'] = bc.TOPOLOGY_LINEAR\n            logger.info(\n                f\"imported: id={sequence['id']}, length={sequence['length']}, description={sequence['description']}, genomic={is_genomic}, dna={is_dna}\"\n            )   \n            sequences.append(sequence)\n    return sequences\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.fasta.wrap_sequence","title":"<code>wrap_sequence(sequence)</code>","text":"<p>Wraps a sequence into lines of 60 characters.</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <code>str</code> <p>The sequence to wrap.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The wrapped sequence.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; wrap_sequence('ARNDCQEGHILKMFPOSUTWYVBZXJ')\n'ARNDCQEGHILKMFPOSUTWYVBZXJ\\n'\n</code></pre> Notes <p>This function is used to format sequences in FASTA files.</p> Source code in <code>src/baktfold/io/fasta.py</code> <pre><code>def wrap_sequence(sequence: str):\n    \"\"\"\n    Wraps a sequence into lines of 60 characters.\n\n    Args:\n      sequence (str): The sequence to wrap.\n\n    Returns:\n      str: The wrapped sequence.\n\n    Examples:\n      &gt;&gt;&gt; wrap_sequence('ARNDCQEGHILKMFPOSUTWYVBZXJ')\n      'ARNDCQEGHILKMFPOSUTWYVBZXJ\\\\n'\n\n    Notes:\n      This function is used to format sequences in FASTA files.\n    \"\"\"\n    lines = []\n    for i in range(0, len(sequence), FASTA_LINE_WRAPPING):\n        lines.append(sequence[i:i + FASTA_LINE_WRAPPING])\n    return '\\n'.join(lines) + '\\n'\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.fasta.write_faa","title":"<code>write_faa(features, faa_path)</code>","text":"<p>Write translated CDS sequences to Fasta file.</p> Source code in <code>src/baktfold/io/fasta.py</code> <pre><code>def write_faa(features: Sequence[dict], faa_path: Path):\n    \"\"\"Write translated CDS sequences to Fasta file.\"\"\"\n    logger.info(f'write translated CDS/sORF: path={faa_path}')\n    with faa_path.open('wt') as fh:\n        for feat in features:\n            if(feat['type'] == bc.FEATURE_CDS or feat['type'] == bc.FEATURE_SORF):\n                fh.write(f\"&gt;{feat['locus']} {feat['product']}\\n{feat['aa']}\\n\")\n</code></pre>"},{"location":"reference/io/#src.baktfold.io.fasta.write_ffn","title":"<code>write_ffn(features, ffn_path)</code>","text":"<p>Write annotated nucleotide sequences to Fasta file.</p> Source code in <code>src/baktfold/io/fasta.py</code> <pre><code>def write_ffn(features: Sequence[dict], ffn_path: Path):\n    \"\"\"Write annotated nucleotide sequences to Fasta file.\"\"\"\n    logger.info(f'write feature nucleotide sequences: path={ffn_path}')\n    with ffn_path.open('wt') as fh:\n        for feat in features:\n            if(feat['type'] in [bc.FEATURE_T_RNA, bc.FEATURE_TM_RNA, bc.FEATURE_R_RNA, bc.FEATURE_NC_RNA, bc.FEATURE_NC_RNA_REGION, bc.FEATURE_CRISPR, bc.FEATURE_CDS, bc.FEATURE_SORF, bc.FEATURE_ORIC, bc.FEATURE_ORIV, bc.FEATURE_ORIT]):\n                identifier = feat['locus'] if 'locus' in feat else feat['id']\n                if(feat.get('product', '') != ''):\n                    fh.write(f\"&gt;{identifier} {feat['product']}\\n{feat['nt']}\\n\")\n                else:\n                    fh.write(f\"&gt;{identifier}\\n{feat['nt']}\\n\")\n</code></pre>"},{"location":"reference/prokka_gbk_to_json/","title":"Prokka gbk to json","text":""},{"location":"reference/prokka_gbk_to_json/#src.baktfold.io.prokka_gbk_to_json.build_bakta_sequence_entry","title":"<code>build_bakta_sequence_entry(rec)</code>","text":"<p>Convert a  SeqRecord into a Bakta-style sequence entry. Missing fields are filled with None.</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def build_bakta_sequence_entry(rec):\n    \"\"\"\n    Convert a  SeqRecord into a Bakta-style sequence entry.\n    Missing fields are filled with None.\n    \"\"\"\n\n    seq = str(rec.seq)\n\n    # -----------------------------------------\n    # Extract source feature qualifiers - genbank always has source field\n    # -----------------------------------------\n    source_feat = next((f for f in rec.features if f.type == \"source\"), None)\n\n    source_qualifiers = {}\n\n    # Defaults (None) for all fields\n    mol_type = None\n    organism = None\n    strain = None\n    db_xref = None\n    note = None\n\n    plasmid = None\n    chromosome = None\n    completeness_hint = None\n\n    if source_feat:\n        q = source_feat.qualifiers\n\n        mol_type = q.get(\"mol_type\", [None])[0]\n        organism = q.get(\"organism\", [None])[0]\n        strain = q.get(\"strain\", [None])[0]\n        note = q.get(\"note\", [None])[0]\n\n        if \"db_xref\" in q:\n            val = q[\"db_xref\"]\n            db_xref = val[0] if len(val) == 1 else val\n\n        plasmid = q.get(\"plasmid\", [None])[0]\n        chromosome = q.get(\"chromosome\", [None])[0]\n        completeness_hint = q.get(\"completeness\", [None])[0]\n\n    # -----------------------------------------\n    # Infer topology\n    # -----------------------------------------\n    topology = rec.annotations.get(\"topology\")\n    if topology not in {\"linear\", \"circular\"}:\n        topology = \"linear\"\n\n    # -----------------------------------------\n    # Infer type\n    # -----------------------------------------\n    if plasmid is not None or \"plasmid\" in rec.annotations:\n        seq_type = \"plasmid\"\n    elif chromosome is not None or \"chromosome\" in rec.annotations:\n        seq_type = \"chromosome\"\n    else:\n        seq_type = \"contig\"\n\n    # -----------------------------------------\n    # Infer completeness (conservative)\n    # -----------------------------------------\n    complete = False\n\n    if topology == \"circular\":\n        complete = True\n    elif completeness_hint is not None and completeness_hint.lower() == \"complete\":\n        complete = True\n    elif note and \"complete genome\" in note.lower():\n        complete = True\n\n    # -----------------------------------------\n    # Infer genetic codefor description\n    # -----------------------------------------\n    gcode = None\n\n    if \"genetic_code\" in rec.annotations:\n        gcode = rec.annotations[\"genetic_code\"]\n    elif \"gcode\" in rec.annotations:\n        gcode = rec.annotations[\"gcode\"]\n    elif source_feat and \"transl_table\" in source_feat.qualifiers:\n        gcode = source_feat.qualifiers[\"transl_table\"][0]\n\n    # Conservative fallback to 11 for prokka\n    if gcode is None:\n        gcode = 11 \n\n    description_parts = [\n        f\"[gcode={gcode}]\",\n        f\"[topology={topology}]\",\n    ]\n\n    description = \" \".join(description_parts)\n\n    # -----------------------------------------\n    # Build entry\n    # -----------------------------------------\n    entry = {\n        \"id\": rec.id,\n        \"description\": description,\n        \"nt\": seq,\n        \"length\": len(seq),\n        \"complete\": complete,\n        \"type\": seq_type,\n        \"topology\": topology,\n        \"simple_id\": rec.id,\n        \"orig_id\": rec.id,\n        \"orig_description\": None,\n    }\n\n    # -----------------------------------------\n    # Add source qualifiers if present\n    # -----------------------------------------\n    if organism is not None:\n        entry[\"organism\"] = organism\n    if mol_type is not None:\n        entry[\"mol_type\"] = mol_type\n    if strain is not None:\n        entry[\"strain\"] = strain\n    if db_xref is not None:\n        entry[\"db_xref\"] = db_xref\n    if note is not None:\n        entry[\"note\"] = note\n\n\n    # this is from bakta\n    # \"id\": \"contig_1\",\n    # \"description\": \"[gcode=11] [topology=linear]\",\n    # \"nt\": \"AT\"\n    # \"length\": 5165988,\n    # \"complete\": false,\n    # \"type\": \"contig\",\n    # \"topology\": \"linear\",\n    # \"simple_id\": \"contig_1\",\n    # \"orig_id\": \"GCF_002368115_000000000001\",\n    # \"orig_description\": \"\"\n\n    # Add source qualifiers only if they exist\n    if organism is not None:\n        entry[\"organism\"] = organism\n\n    if mol_type is not None:\n        entry[\"mol_type\"] = mol_type\n\n    if strain is not None:\n        entry[\"strain\"] = strain\n\n    if db_xref is not None:\n        entry[\"db_xref\"] = db_xref\n\n    if note is not None:\n        entry[\"note\"] = note\n\n    return entry\n</code></pre>"},{"location":"reference/prokka_gbk_to_json/#src.baktfold.io.prokka_gbk_to_json.calc_genome_stats","title":"<code>calc_genome_stats(records)</code>","text":"<p>Compute correct genome stats (size, GC, N-ratio, N50, N90) for records from a multi-contig Prokka GenBank file.</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def calc_genome_stats(records):\n    \"\"\"\n    Compute correct genome stats (size, GC, N-ratio, N50, N90) for records from a multi-contig\n    Prokka GenBank file.\n    \"\"\"\n\n    if not records:\n        raise ValueError(\"No GenBank records found.\")\n\n    # lengths of all contigs\n    contig_lengths = [len(r.seq) for r in records]\n    total_length = sum(contig_lengths)\n\n    # concatenate sequences for global GC + N calculation\n    full_seq = \"\".join(str(r.seq) for r in records)\n\n    # GC as fraction (Bakta wants 0\u20131)\n    gc_perc = gc_fraction(full_seq)\n\n    # N-ratio\n    n_ratio = full_seq.count(\"N\") / total_length\n\n    # ---------- N50 / N90 ----------\n    sorted_lengths = sorted(contig_lengths, reverse=True)\n\n    def nx_metric(sorted_lens, total, threshold):\n        \"\"\"\n        Generic N{threshold} function.\n        threshold: 0.5 for N50, 0.9 for N90\n        \"\"\"\n        cutoff = total * threshold\n        running = 0\n        for l in sorted_lens:\n            running += l\n            if running &gt;= cutoff:\n                return l\n        return sorted_lens[-1]  # fallback (should not happen)\n\n    n50 = nx_metric(sorted_lengths, total_length, 0.5)\n    n90 = nx_metric(sorted_lengths, total_length, 0.9)\n\n    return {\n        \"size\": total_length,\n        \"gc\": gc_perc,\n        \"n_ratio\": n_ratio,\n        \"n50\": n50,\n        \"n90\": n90,\n        \"coding_ratio\": None  \n    }\n</code></pre>"},{"location":"reference/prokka_gbk_to_json/#src.baktfold.io.prokka_gbk_to_json.convert_assembly_gap_feature","title":"<code>convert_assembly_gap_feature(feature, rec, id)</code>","text":"<p>Convert a Prokka GenBank assembly_gap feature to a simplified Bakta-style 'gap' feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The assembly_gap feature from the Prokka GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The full GenBank record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Simplified Bakta-style gap feature.</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def convert_assembly_gap_feature(feature, rec, id):\n    \"\"\"\n    Convert a Prokka GenBank assembly_gap feature to a simplified Bakta-style 'gap' feature.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The assembly_gap feature from the Prokka GBK.\n        rec: Bio.SeqRecord\n            The full GenBank record containing the sequence.\n\n    Returns:\n        dict: Simplified Bakta-style gap feature.\n    \"\"\"\n\n    # Coordinates (1-based)\n    start = int(feature.location.start) + 1\n    stop = int(feature.location.end)\n\n    qualifiers = feature.qualifiers\n\n    # Prokka may provide estimated_length but coordinates already give an exact span\n    est_len = qualifiers.get(\"estimated_length\", [None])[0]\n    if est_len is not None:\n        length = int(est_len)\n    else:\n        length = stop - start + 1  # fallback from coordinates\n\n    # Bakta always uses \".\" for strand on gaps\n    strand = \".\"\n\n    gap_entry = {\n        \"type\": \"gap\",\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"length\": length,\n        \"id\": id\n    }\n\n    return gap_entry\n</code></pre>"},{"location":"reference/prokka_gbk_to_json/#src.baktfold.io.prokka_gbk_to_json.convert_cds_feature","title":"<code>convert_cds_feature(feature, seq_record, translation_table, id)</code>","text":"<p>Convert a Prokka CDS Biopython SeqFeature to a Bakta CDS JSON entry.</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def convert_cds_feature(feature, seq_record, translation_table, id):\n    \"\"\"\n    Convert a Prokka CDS Biopython SeqFeature to a Bakta CDS JSON entry.\n    \"\"\"\n\n    # ----------- Location info -----------\n    start = int(feature.location.start) + 1     # Bakta uses 1-based inclusive\n    stop  = int(feature.location.end)           # already one past in BioPython\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n\n    # frame: Bakta uses 1/2/3; Prokka codon_start is [\"1\",\"2\",\"3\"]\n    codon_start = int(feature.qualifiers.get(\"codon_start\", [\"1\"])[0])\n    frame = codon_start\n\n    # ----------- Basic qualifiers -----------\n    gene = feature.qualifiers.get(\"gene\", [None])[0]\n    product = feature.qualifiers.get(\"product\", [None])[0]\n\n    locus_tag = feature.qualifiers.get(\"locus_tag\", [None])[0]\n    locus = locus_tag\n\n    # ----------- Extract nucleotides -----------\n    nt_seq = feature.extract(seq_record.seq)\n    nt = str(nt_seq)\n\n    # ----------- Extract amino acids -----------\n    aa = feature.qualifiers.get(\"translation\", [\"\"])[0]\n\n    # Compute translation if Prokka didn't provide it\n    if not aa:\n        try:\n            aa = str(nt_seq.translate(table=translation_table, cds=True))\n        except Exception:\n            aa = \"\"\n\n    # ----------- aa MD5 hexdigest -----------\n    aa_hexdigest = hashlib.md5(aa.encode()).hexdigest()\n\n    # ----------- Hypothetical? -----------\n    hypothetical = product is None or \"hypothetical protein\" in product.lower()\n\n    # ----------- Compute protein stats -----------\n    seq_stats = None\n    if aa:\n        try:\n            analysed = ProteinAnalysis(aa)\n            seq_stats = {\n                \"molecular_weight\": analysed.molecular_weight(),\n                \"isoelectric_point\": analysed.isoelectric_point()\n            }\n        except Exception:\n            seq_stats = None\n\n    # ----------- Make Bakta-format dict -----------\n    bakta_cds = {\n        \"type\": \"cds\",\n        \"sequence\": seq_record.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"frame\": frame,\n        \"gene\": gene,\n        \"product\": product,\n        \"db_xrefs\": feature.qualifiers.get(\"db_xref\", [so.SO_CDS.id]),  # there will be no other db_xref \n        \"nt\": nt,\n        \"aa\": aa,\n        \"aa_hexdigest\": aa_hexdigest,\n        \"start_type\": None,\n        \"rbs_motif\": None,\n        \"genes\": [],\n        \"seq_stats\": seq_stats,\n        \"id\": id,\n        \"locus\": locus,\n    }\n\n    if hypothetical:\n        bakta_cds[\"hypothetical\"] = True\n\n    return bakta_cds\n</code></pre>"},{"location":"reference/prokka_gbk_to_json/#src.baktfold.io.prokka_gbk_to_json.convert_misc_rna_feature","title":"<code>convert_misc_rna_feature(feature, rec, id)</code>","text":"<p>Convert a Prokka GenBank misc_RNA (nc_rna) feature to a Bakta-style feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The misc_RNA feature from the Prokka GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Bakta-style misc_RNA feature.</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def convert_misc_rna_feature(feature, rec, id):\n    \"\"\"\n    Convert a Prokka GenBank misc_RNA (nc_rna) feature to a Bakta-style feature.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The misc_RNA feature from the Prokka GBK.\n        rec: Bio.SeqRecord\n            The record containing the sequence.\n\n    Returns:\n        dict: Bakta-style misc_RNA feature.\n    \"\"\"\n\n    seq = str(rec.seq)\n\n    # Coordinates (GBK is 0-based, Bakta is 1-based)\n    start = int(feature.location.start) + 1\n    stop = int(feature.location.end)\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n\n    qualifiers = feature.qualifiers\n\n    # Fields that Prokka may or may not include\n    gene = qualifiers.get(\"gene\", [None])[0]\n    product = qualifiers.get(\"product\", [None])[0]\n    locus_tag = qualifiers.get(\"locus_tag\", [None])[0]\n\n    # If gene missing, use product (Bakta often fills 'gene' for sRNA/tmRNA/etc.)\n    if gene is None:\n        gene = product\n\n    # Extract nucleotide sequence\n    nt_seq = seq[start-1:stop]\n    if strand == \"-\":\n        comp = str.maketrans(\"ACGTacgt\", \"TGCAtgca\")\n        nt_seq = nt_seq.translate(comp)[::-1]\n\n    misc_entry = {\n        \"type\": \"ncRNA\", # bakta uses ncRNA -&gt; will be misc_rna in Prokka\n        \"class\": None,             # bakta's classes are not in Prokka\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"gene\": gene,\n        \"product\": product,\n        \"score\": None,\n        \"evalue\": None,\n        \"db_xrefs\": [so.SO_NCRNA_GENE.id],        \n        \"nt\": nt_seq,\n        \"id\": id,\n        \"locus\": locus_tag\n    }\n\n    return misc_entry\n</code></pre>"},{"location":"reference/prokka_gbk_to_json/#src.baktfold.io.prokka_gbk_to_json.convert_repeat_region_feature","title":"<code>convert_repeat_region_feature(feature, rec, id)</code>","text":"<p>Convert a Prokka GenBank repeat_region (CRISPR) feature to a simplified Bakta-style feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The repeat_region feature (crispr) from the Prokka GBK.</p> required <code>rec</code> <p>Bio.SeqRecord The full GenBank record containing the sequence.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Simplified Bakta-style CRISPR feature.</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def convert_repeat_region_feature(feature, rec, id):\n    \"\"\"\n    Convert a Prokka GenBank repeat_region (CRISPR) feature to a simplified Bakta-style feature.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The repeat_region feature (crispr) from the Prokka GBK.\n        rec: Bio.SeqRecord\n            The full GenBank record containing the sequence.\n\n    Returns:\n        dict: Simplified Bakta-style CRISPR feature.\n    \"\"\"\n\n    # Coordinates (Bakta uses 1-based)\n    start = int(feature.location.start) + 1\n    stop = int(feature.location.end)\n\n    qualifiers = feature.qualifiers\n    note = qualifiers.get(\"note\", [None])[0]\n    rpt_family = qualifiers.get(\"rpt_family\", [None])[0]\n    rpt_type = qualifiers.get(\"rpt_type\", [None])[0]\n    rpt_unit_seq = qualifiers.get(\"rpt_unit_seq\", [None])[0]\n\n    strand = \"?\"\n\n    # always just take the positive strand to get the NT seq (crispr repeat region)\n    seq =  str(rec.seq)\n    nt_seq = seq[start-1:stop]\n\n    # Minimal Bakta-like CRISPR structure\n    crispr_entry = {\n        \"type\": \"crispr\",\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand, # matches Bakta and is required\n        \"family\": rpt_family,       # e.g., \"CRISPR\"\n        \"rpt_type\": rpt_type,       # e.g., \"direct\"\n        \"repeat_unit\": rpt_unit_seq, # the actual consensus repeat\n        \"product\": note, # won't be the same as Bakta as different lookup method used - but needed for the gff writing\n        \"nt\": nt_seq, # needed for batka .ffn writeout\n        \"id\": id, # bakta_id needed \n        # \"locus\": None, # no locus tag like Bakta\n        \"db_xrefs\": [so.SO_CRISPR.id]\n    }\n\n    return crispr_entry\n</code></pre>"},{"location":"reference/prokka_gbk_to_json/#src.baktfold.io.prokka_gbk_to_json.convert_rrna_feature","title":"<code>convert_rrna_feature(feature, rec, id)</code>","text":"<p>Convert a Prokka GenBank rRNA feature to Bakta-style JSON.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The rRNA feature from the Prokka GBK.</p> required <code>rec</code> <p>str The record from the GBK.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Bakta-style rRNA feature</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def convert_rrna_feature(feature, rec, id):\n    \"\"\"\n    Convert a Prokka GenBank rRNA feature to Bakta-style JSON.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The rRNA feature from the Prokka GBK.\n        rec: str\n            The record from the GBK.\n    Returns:\n        dict: Bakta-style rRNA feature\n    \"\"\"\n    start = int(feature.location.start) + 1  # GBK is 0-based\n    stop = int(feature.location.end)\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n\n    qualifiers = feature.qualifiers\n    product = qualifiers.get(\"product\", [None])[0]\n    locus_tag = qualifiers.get(\"locus_tag\", [None])[0]\n\n    # Infer gene type from product if possible\n    gene_map = {\n        \"16S ribosomal RNA\": \"rrs\",\n        \"23S ribosomal RNA\": \"rrl\",\n        \"5S ribosomal RNA\": \"rrf\"\n    }\n    gene = gene_map.get(product, None)\n\n    so_map = {\n        \"16S ribosomal RNA\": so.SO_RRNA_16S.id,\n        \"23S ribosomal RNA\": so.SO_RRNA_23S.id,\n        \"5S ribosomal RNA\": so.SO_RRNA_5S.id\n    }\n\n    specific_so = so_map.get(product, None)\n\n    contig_seq = str(rec.seq)\n\n    nt_seq = contig_seq[start-1:stop]\n    if strand == \"-\":\n        comp = str.maketrans(\"ACGTacgt\", \"TGCAtgca\")\n        nt_seq = nt_seq.translate(comp)[::-1]\n\n    rrna_entry = {\n        \"type\": \"rRNA\",\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"gene\": gene,\n        \"product\": product,\n        \"coverage\": None,  # Prokka does not provide\n        \"score\": None,     # Prokka does not provide\n        \"evalue\": None,    # Prokka does not provide\n        \"db_xrefs\": [so.SO_RRNA.id, specific_so], \n        \"nt\": nt_seq,\n        \"id\": id,\n        \"locus\": locus_tag\n    }\n\n    return rrna_entry\n</code></pre>"},{"location":"reference/prokka_gbk_to_json/#src.baktfold.io.prokka_gbk_to_json.convert_tmrna_feature","title":"<code>convert_tmrna_feature(feature, rec, id)</code>","text":"<p>Convert a Prokka GenBank tmRNA feature to Bakta-style feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>Bio.SeqFeature The tmRNA feature from the Prokka GBK.</p> required <code>rec</code> <p>str The record from the GBK</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Bakta-style tmRNA feature</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def convert_tmrna_feature(feature, rec, id):\n    \"\"\"\n    Convert a Prokka GenBank tmRNA feature to Bakta-style feature.\n\n    Parameters:\n        feature: Bio.SeqFeature\n            The tmRNA feature from the Prokka GBK.\n        rec: str\n            The record from the GBK\n\n    Returns:\n        dict: Bakta-style tmRNA feature\n    \"\"\"\n\n    seq =  str(rec.seq)\n\n    start = int(feature.location.start) + 1  # GBK is 0-based\n    stop = int(feature.location.end)\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n\n    qualifiers = feature.qualifiers\n    gene = qualifiers.get(\"gene\", [None])[0]\n    locus_tag = qualifiers.get(\"locus_tag\", [None])[0]\n    product = qualifiers.get(\"product\", [None])[0]\n\n    # Extract the nucleotide sequence of the tmRNA\n    nt_seq = seq[start-1:stop]\n    if strand == \"-\":\n        comp = str.maketrans(\"ACGTacgt\", \"TGCAtgca\")\n        nt_seq = nt_seq.translate(comp)[::-1]\n\n\n\n    tmrna_entry = {\n        \"type\": \"tmRNA\",\n        \"sequence\": rec.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"gene\": gene,\n        \"product\": product,\n        \"db_xrefs\": [so.SO_TMRNA.id], \n        # \"tag\": tag_info,  no tag in tmrna for prokka - no information on it in the output\n        \"nt\": nt_seq,\n        \"id\": id,\n        \"locus\": locus_tag\n    }\n\n    return tmrna_entry\n</code></pre>"},{"location":"reference/prokka_gbk_to_json/#src.baktfold.io.prokka_gbk_to_json.convert_trna_feature","title":"<code>convert_trna_feature(feature, seq_record, id)</code>","text":"<p>Convert a Prokka tRNA SeqFeature to a Bakta tRNA JSON entry.</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def convert_trna_feature(feature, seq_record, id):\n    \"\"\"\n    Convert a Prokka tRNA SeqFeature to a Bakta tRNA JSON entry.\n    \"\"\"\n\n    # ------------ Location ------------\n    start = int(feature.location.start) + 1\n    stop  = int(feature.location.end)\n    strand = \"+\" if feature.location.strand == 1 else \"-\"\n\n    # ------------ Extract nt sequence ------------\n    nt_seq = feature.extract(seq_record.seq)\n    nt = str(nt_seq)\n\n    # ------------ Basic qualifiers ------------\n    product = feature.qualifiers.get(\"product\", [None])[0]\n    locus = feature.qualifiers.get(\"locus_tag\", [None])[0]\n\n    # ------------ amino acid ------------\n    # Prokka product examples:\n    #   \"tRNA-Trp\"\n    #   \"tRNA-Leu\"\n    amino_acid = None\n    if product and product.startswith(\"tRNA-\"):\n        amino_acid = product.split(\"-\")[1]\n\n\n    # ------------ anticodon ------------\n    anti_codon = None\n\n    # anticodons are in notes\n\n    notes = feature.qualifiers.get(\"note\", [])\n\n    # Expect a note like: \"tRNA-Ser(gga)\"\n    for note in notes:\n        # Remove spaces for safety\n        n = note.replace(\" \", \"\")\n\n        # Extract part inside parentheses (anticodon)\n        if \"(\" in n and \")\" in n:\n            anti_codon = n.split(\"(\")[1].split(\")\")[0].lower()\n\n        # Extract amino acid:\n        # tRNA-Ser(gga) \u2192 \"Ser\"\n        if \"tRNA-\" in n:\n            try:\n                # tRNA-Ser(gga) \u2192 \"Ser(gga)\" \u2192 split('(')[0] \u2192 \"Ser\"\n                aa_section = n.split(\"tRNA-\")[1]\n                aa_clean = aa_section.split(\"(\")[0]\n                amino_acid = aa_clean\n            except Exception:\n                pass\n\n    # ------------ Anti-codon position detection ------------\n    # Prokka doesnt have it - dont include\n    # anti_codon_pos = None\n\n    # ------------ score ------------\n    # nothing in prokka\n    score = None\n\n    # ------------ db_xrefs ------------\n    # doesnt exist for prokka\n    db_xrefs = feature.qualifiers.get(\"db_xref\", [])\n    # add so_term\n    so_term = AMINO_ACID_DICT.get(amino_acid.lower(), ('', None))[1]\n\n    if (so_term):\n        db_xrefs.append(so_term.id)\n\n    # ------------ final Bakta-form dict ------------\n    bakta_trna = {\n        \"type\": \"tRNA\",\n        \"sequence\": seq_record.id,\n        \"start\": start,\n        \"stop\": stop,\n        \"strand\": strand,\n        \"gene\": \"trn\" + (amino_acid[0].lower() if amino_acid else \"?\"),\n        \"product\": product,\n        \"amino_acid\": amino_acid,\n        \"anti_codon\": anti_codon,\n        \"score\": score,\n        \"nt\": nt,\n        \"db_xrefs\": db_xrefs,\n       #  \"anti_codon_pos\": anti_codon_pos,  dont include, not in prokka output\n        \"id\": id,\n        \"locus\": locus\n    }\n\n    return bakta_trna\n</code></pre>"},{"location":"reference/prokka_gbk_to_json/#src.baktfold.io.prokka_gbk_to_json.get_bakta_style_id_from_locus_tag","title":"<code>get_bakta_style_id_from_locus_tag(records)</code>","text":"<p>Gets 10 char bakta-style ID tag based off the 8 char locus tag in first CDS on the first Prokka record + 2 random chars</p> <p>Assumes all records will have the same locus tag prefix</p> <p>Will always add 2 chars to make ID unique vs locus tag</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def get_bakta_style_id_from_locus_tag(records):\n    \"\"\"\n    Gets 10 char bakta-style ID tag based off the 8 char locus tag in first CDS on the first Prokka record + 2 random chars\n\n    Assumes all records will have the same locus tag prefix\n\n    Will always add 2 chars to make ID unique vs locus tag\n    \"\"\"\n\n    if not records:\n        raise ValueError(\"No GenBank records found.\")\n\n    for record in records:\n\n        for feat in record.features:\n            if feat.type == \"CDS\":\n                locus_tag_list = feat.qualifiers.get(\"locus_tag\") # returns None if doesn't exist\n\n                if locus_tag_list:\n                    locus_tag = locus_tag_list[0]\n\n                    if len(locus_tag) &gt; 6:\n\n                        locus_tag_prefix = locus_tag[:-6] # trims off _00001 from CDS\n\n                        rand_two_chars = random_n_letter_id(2)\n\n                        # by default prokka locus tag is 8 chars. So this returns a 10 char string (same as bakta defaults)\n\n                        id_tag = f\"{locus_tag_prefix}{rand_two_chars}\"\n\n                        return id_tag\n\n\n                    else:\n                        return random_n_letter_id(10)\n\n                # fallback if locus_tag missing or too short\n                return random_n_letter_id(10)\n\n    # No CDS feature found at all (shouldn't happen)\n    return random_n_letter_id(10)\n</code></pre>"},{"location":"reference/prokka_gbk_to_json/#src.baktfold.io.prokka_gbk_to_json.get_transl_table","title":"<code>get_transl_table(records)</code>","text":"<p>Gets translation table based off the first CDS on the first record</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def get_transl_table(records):\n    \"\"\"\n    Gets translation table based off the first CDS on the first record\n    \"\"\"\n\n    if not records:\n        raise ValueError(\"No GenBank records found.\")\n\n    record_1 = records[0]\n\n    for feat in record_1.features:\n        if feat.type == \"CDS\":\n            # Translation table may be string \u2192 convert to int\n            transl = feat.qualifiers.get(\"transl_table\", [\"11\"])[0]\n            try:\n                return int(transl)\n            except ValueError:\n                return 11\n\n        # If no CDS found, default to 11\n        return 11\n</code></pre>"},{"location":"reference/prokka_gbk_to_json/#src.baktfold.io.prokka_gbk_to_json.parse_prokka_version","title":"<code>parse_prokka_version(record)</code>","text":"<p>Extract Prokka version from COMMENT field: Example COMMENT: 'Annotated using prokka 1.14.6 ...'</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def parse_prokka_version(record):\n    \"\"\"\n    Extract Prokka version from COMMENT field:\n    Example COMMENT:\n    'Annotated using prokka 1.14.6 ...'\n    \"\"\"\n\n    comments = record.annotations.get(\"comment\", \"\") or record.annotations.get(\"comments\", \"\")\n    if not comments:\n        return \"unknown\"\n\n    m = re.search(r\"[Pp]rokka[\\s_]?v?(\\d+\\.\\d+\\.\\d+)\", comments)\n    if m:\n        return m.group(1)\n\n    # fallback pattern\n    m = re.search(r\"prokka[^0-9]*(\\d+\\.\\d+\\.\\d+)\", comments)\n    if m:\n        return m.group(1)\n\n    return \"unknown\"\n</code></pre>"},{"location":"reference/prokka_gbk_to_json/#src.baktfold.io.prokka_gbk_to_json.prokka_gbk_to_json","title":"<code>prokka_gbk_to_json(records, output_json)</code>","text":"<p>Convert Prokka-generated GenBank SeqRecord objects into a Bakta-style JSON annotation file.</p> <p>This function takes one or more Biopython SeqRecord objects (typically parsed from a Prokka GenBank file) and reconstructs a JSON structure following the Bakta output schema. It extracts genome metadata, statistics, annotated features, nucleotide sequences, and Prokka version information. Features are converted to Bakta-compatible dictionaries and sorted in the same order Bakta expects.</p> The JSON file contains <p>\u2022 <code>genome</code> block \u2013 high-level organism metadata (genus, species, strain, etc.) \u2022 <code>stats</code> block \u2013 genome statistics derived from all records \u2022 <code>features</code> block \u2013 all features converted to Bakta-style objects, sorted   by feature type and genomic position \u2022 <code>sequences</code> block \u2013 contig/sequence entries in Bakta format \u2022 <code>run</code> block \u2013 timestamps and duration placeholder \u2022 <code>version</code> block \u2013 Prokka version and database metadata</p>"},{"location":"reference/prokka_gbk_to_json/#src.baktfold.io.prokka_gbk_to_json.prokka_gbk_to_json--parameters","title":"Parameters","text":"list of SeqRecord <p>A list of Biopython SeqRecord objects already parsed from a Prokka GenBank file. Must contain at least one record. The COMMENT field is expected to contain Prokka metadata.</p> str <p>Path to the output JSON file to be written.</p>"},{"location":"reference/prokka_gbk_to_json/#src.baktfold.io.prokka_gbk_to_json.prokka_gbk_to_json--returns","title":"Returns","text":"<p>bool     True if the JSON was successfully written.</p>"},{"location":"reference/prokka_gbk_to_json/#src.baktfold.io.prokka_gbk_to_json.prokka_gbk_to_json--raises","title":"Raises","text":"<p>ValueError     If <code>records</code> is empty.</p>"},{"location":"reference/prokka_gbk_to_json/#src.baktfold.io.prokka_gbk_to_json.prokka_gbk_to_json--notes","title":"Notes","text":"<p>\u2022 Features are processed in a fixed Bakta-like order:     [\"tRNA\", \"tmRNA\", \"rRNA\", \"misc_RNA\", \"repeat_region\", \"CDS\", \"assembly_gap\"] \u2022 Feature IDs are generated in Bakta-style using the locus tag prefix. \u2022 Per-contig sorting is performed by genomic start coordinate. \u2022 Runtime values in the <code>run</code> block are placeholders (duration = \"0.00 min\"). \u2022 The function does not validate that the GenBank file truly originates from   Prokka; that should be checked beforehand.</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def prokka_gbk_to_json(records, output_json):\n    \"\"\"\n    Convert Prokka-generated GenBank SeqRecord objects into a Bakta-style JSON\n    annotation file.\n\n    This function takes one or more Biopython SeqRecord objects (typically parsed\n    from a Prokka GenBank file) and reconstructs a JSON structure following the\n    Bakta output schema. It extracts genome metadata, statistics, annotated features,\n    nucleotide sequences, and Prokka version information. Features are converted\n    to Bakta-compatible dictionaries and sorted in the same order Bakta expects.\n\n    The JSON file contains:\n      \u2022 `genome` block \u2013 high-level organism metadata (genus, species, strain, etc.)\n      \u2022 `stats` block \u2013 genome statistics derived from all records\n      \u2022 `features` block \u2013 all features converted to Bakta-style objects, sorted\n        by feature type and genomic position\n      \u2022 `sequences` block \u2013 contig/sequence entries in Bakta format\n      \u2022 `run` block \u2013 timestamps and duration placeholder\n      \u2022 `version` block \u2013 Prokka version and database metadata\n\n    Parameters\n    ----------\n    records : list of SeqRecord\n        A list of Biopython SeqRecord objects already parsed from a Prokka GenBank\n        file. Must contain at least one record. The COMMENT field is expected to\n        contain Prokka metadata.\n\n    output_json : str\n        Path to the output JSON file to be written.\n\n    Returns\n    -------\n    bool\n        True if the JSON was successfully written.\n\n    Raises\n    ------\n    ValueError\n        If `records` is empty.\n\n    Notes\n    -----\n    \u2022 Features are processed in a fixed Bakta-like order:\n        [\"tRNA\", \"tmRNA\", \"rRNA\", \"misc_RNA\", \"repeat_region\", \"CDS\", \"assembly_gap\"]\n    \u2022 Feature IDs are generated in Bakta-style using the locus tag prefix.\n    \u2022 Per-contig sorting is performed by genomic start coordinate.\n    \u2022 Runtime values in the `run` block are placeholders (duration = \"0.00 min\").\n    \u2022 The function does not validate that the GenBank file truly originates from\n      Prokka; that should be checked beforehand.\n    \"\"\"\n\n    complete = False\n\n    # records = list(SeqIO.parse(genbank_path, \"genbank\"))\n\n    if len(records) == 0:\n        raise ValueError(\"No GenBank records found.\")\n    elif len(records) &gt;= 1:\n        prokka_version = parse_prokka_version(records[0])\n\n    translation_table = get_transl_table(records)\n\n    bakta_id_prefix = get_bakta_style_id_from_locus_tag(records)\n\n    # ----------------------------\n    # Genome block \n    # ----------------------------\n\n    genome_block = {\n        \"genus\": None,\n        \"species\": None,\n        \"strain\": None,\n        \"taxon\": None,\n        \"complete\": True,\n        \"gram\": \"?\",\n        \"translation_table\": translation_table\n    }\n\n    # ----------------------------\n    # Stats - on whole GBK\n    # ----------------------------\n    stats_block = calc_genome_stats(records)\n\n    # ----------------------------\n    # Features block\n    # ----------------------------\n\n    # order by id creation block to match how ids are generated bakta\n    ORDER = [\"tRNA\", \"tmRNA\", \"rRNA\", \"misc_RNA\", \"repeat_region\", \"CDS\", \"assembly_gap\"]\n    # bakta has oriC detection too - prokka doesn't I think so leaving it out \n\n    features = []\n    i = 1\n    for rec in records:\n        for ftype in ORDER:\n            for feat in rec.features:\n                if feat.type != ftype:\n                    continue\n\n                id = f\"{bakta_id_prefix}_{i}\"\n\n                if ftype == \"CDS\":\n                    features.append(convert_cds_feature(feat, rec, translation_table, id))\n                elif ftype == \"tRNA\":\n                    features.append(convert_trna_feature(feat, rec, id))\n                elif ftype == \"tmRNA\":\n                    features.append(convert_tmrna_feature(feat, rec, id))\n                elif ftype == \"rRNA\":\n                    features.append(convert_rrna_feature(feat, rec, id))\n                elif ftype == \"misc_RNA\":\n                    features.append(convert_misc_rna_feature(feat, rec, id))\n                elif ftype == \"repeat_region\":\n                    features.append(convert_repeat_region_feature(feat, rec, id))\n                elif ftype == \"assembly_gap\":\n                    features.append(convert_assembly_gap_feature(feat, rec, id))\n\n                i +=1\n\n\n    # ----------------------------\n    # Sort features within each contig like Bakta\n    # ----------------------------\n\n    features_by_contig = defaultdict(list)\n    for f in features:\n        features_by_contig[f[\"sequence\"]].append(f)\n\n    # Sort each contig's features by start and flatten back\n    sorted_features = []\n    for contig_id in features_by_contig:\n        sorted_features.extend(sorted(features_by_contig[contig_id], key=lambda x: x[\"start\"]))\n\n    # Replace the original features list\n    features = sorted_features\n\n    # ----------------------------\n    # Sequences block\n    # ----------------------------\n\n    sequences = []\n    for rec in records:\n        sequences.append(build_bakta_sequence_entry(rec))\n\n    # just to put in a time\n    start_time = datetime.now()\n\n    bakta_json = {\n        \"genome\": genome_block,\n        \"stats\": stats_block,\n        \"features\": features,\n        \"sequences\": sequences,\n        \"run\": {\n            \"start\": start_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"end\": start_time.strftime(\"%Y-%m-%d %H:%M:%S\"),       \n            \"duration\": \"0.00 min\"   \n        },\n        \"version\": {\n            \"prokka\": prokka_version,  \n            \"db\": {\n                \"version\": prokka_version,\n                \"type\": \"prokka_dbs\"\n            }\n        }\n    }\n\n\n    Path(output_json).parent.mkdir(parents=True, exist_ok=True)\n    with open(output_json, \"w\") as fh:\n        json.dump(bakta_json, fh, indent=4)\n        complete = True\n\n    return complete\n</code></pre>"},{"location":"reference/prokka_gbk_to_json/#src.baktfold.io.prokka_gbk_to_json.random_n_letter_id","title":"<code>random_n_letter_id(n=4)</code>","text":"<p>generates a n letter id prefix </p> <p>n=2 to append to  Prokka locus tag  for bakta id to make it different n=10 if the locus tag is somehow missing (should never happen)</p> Source code in <code>src/baktfold/io/prokka_gbk_to_json.py</code> <pre><code>def random_n_letter_id(n=4):\n    \"\"\"\n    generates a n letter id prefix \n\n    n=2 to append to  Prokka locus tag  for bakta id to make it different\n    n=10 if the locus tag is somehow missing (should never happen) \n    \"\"\"\n    return ''.join(random.choices(string.ascii_uppercase, k=n))\n</code></pre>"},{"location":"reference/results/","title":"Results","text":""},{"location":"reference/results/#src.baktfold.results.tophit.get_tophit","title":"<code>get_tophit(result_tsv, structures, cath=False)</code>","text":"<p>Process Foldseek output to extract top hit and weighted bitscores.</p> <p>Parameters:</p> Name Type Description Default <code>result_tsv</code> <code>Path</code> <p>Path to the Foldseek result TSV file.</p> required <code>structures</code> <code>bool</code> <p>Flag indicating whether structures have been added.</p> required <code>cath</code> <code>bool</code> <p>Flag indicating whether this is for CATH database (all greedy besthits kept not just top)</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[pd.DataFrame, pd.DataFrame]</code> <p>Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing two DataFrames: 1. DataFrame containing the top functions extracted from the Foldseek output. 2. DataFrame containing weighted bitscores for different functions.</p> Source code in <code>src/baktfold/results/tophit.py</code> <pre><code>def get_tophit(\n    result_tsv: Path,\n    structures: bool,\n    cath: bool = False\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Process Foldseek output to extract top hit and weighted bitscores.\n\n    Args:\n        result_tsv (Path): Path to the Foldseek result TSV file.\n        structures (bool): Flag indicating whether structures have been added.\n        cath (bool): Flag indicating whether this is for CATH database (all greedy besthits kept not just top)\n\n    Returns:\n        Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing two DataFrames:\n            1. DataFrame containing the top functions extracted from the Foldseek output.\n            2. DataFrame containing weighted bitscores for different functions.\n    \"\"\"\n\n    logger.info(\"Processing Foldseek output\")\n\n    if structures:\n\n        col_list = [\n            \"query\",\n            \"target\",\n            \"bitscore\",\n            \"fident\",\n            \"evalue\",\n            \"qStart\",\n            \"qEnd\",\n            \"qLen\",\n            \"tStart\",\n            \"tEnd\",\n            \"tLen\",\n            \"alntmscore\",\n            \"lddt\"\n        ]\n    else:\n\n        col_list = [\n            \"query\",\n            \"target\",\n            \"bitscore\",\n            \"fident\",\n            \"evalue\",\n            \"qStart\",\n            \"qEnd\",\n            \"qLen\",\n            \"tStart\",\n            \"tEnd\",\n            \"tLen\",\n        ]\n\n    foldseek_df = pd.read_csv(\n        result_tsv, delimiter=\"\\t\", index_col=False, names=col_list\n    )\n\n\n    # in case the foldseek output is empty\n    if foldseek_df.empty:\n        logger.error(\n            \"Foldseek found no hits whatsoever - please check whether your input\"\n        )\n\n\n    # add qcov and tcov \n    foldseek_df[\"qCov\"] = ((foldseek_df[\"qEnd\"] - foldseek_df[\"qStart\"] ) / foldseek_df[\"qLen\"]).round(2)\n    foldseek_df[\"tCov\"] = ((foldseek_df[\"tEnd\"] - foldseek_df[\"tStart\"] ) / foldseek_df[\"tLen\"]).round(2)\n\n    # reorder\n    qLen_index = foldseek_df.columns.get_loc(\"qLen\")\n    tLen_index = foldseek_df.columns.get_loc(\"tLen\")\n\n    new_column_order = (\n        list(\n            [\n                col\n                for col in foldseek_df.columns[: qLen_index + 1]\n                if col not in [\"qCov\", \"tStart\",\"tEnd\",\t\"tLen\", \"tCov\"]\n            ]\n        )\n        + [\"qCov\", \"tStart\",\"tEnd\",\t\"tLen\", \"tCov\"]\n        + list(\n            [\n                col\n                for col in foldseek_df.columns[tLen_index + 1 :]\n                if col not in [\"qCov\", \"tStart\",\"tEnd\",\t\"tLen\", \"tCov\"]\n            ]\n        )\n    )\n    foldseek_df = foldseek_df.reindex(columns=new_column_order)\n\n\n    if not cath:\n        # get only the tophit - will always be the first hit for each query (top bitscore)\n        foldseek_df = foldseek_df.drop_duplicates(subset=\"query\", keep=\"first\")\n    # otherwise, the df will contain all greedy tophits from CATH\n\n\n    return foldseek_df\n</code></pre>"},{"location":"reference/subcommands/","title":"Subcommands","text":""},{"location":"reference/subcommands/#src.baktfold.subcommands.predict.mask_low_confidence_aa","title":"<code>mask_low_confidence_aa(sequence, scores, threshold=0.5)</code>","text":"<p>Masks all low confidence AA to X if their corresponding ProstT5 confidence score is below the given threshold.</p> <p>sequence (str): The amino acid sequence. scores (List[float]): A list of confidence scores for each amino acid. threshold (float, optional): The confidence threshold below which amino acids are converted to lowercase. Default is 0.5.</p> <p>str: The modified amino acid sequence with low-confidence residues in lowercase.</p> Source code in <code>src/baktfold/subcommands/predict.py</code> <pre><code>def mask_low_confidence_aa(sequence, scores, threshold=0.5):\n    \"\"\"\n    Masks all low confidence AA to X if their corresponding ProstT5 confidence score is below the given threshold.\n\n    Parameters:\n    sequence (str): The amino acid sequence.\n    scores (List[float]): A list of confidence scores for each amino acid.\n    threshold (float, optional): The confidence threshold below which amino acids are converted to lowercase. Default is 0.5.\n\n    Returns:\n    str: The modified amino acid sequence with low-confidence residues in lowercase.\n    \"\"\"\n    return \"\".join('X' if float(score) &lt; threshold else aa \n                   for aa, score in zip(sequence, *scores))\n</code></pre>"},{"location":"reference/subcommands/#src.baktfold.subcommands.predict.subcommand_predict","title":"<code>subcommand_predict(hypotheticals, cds_dict, output, prefix, cpu, omit_probs, model_dir, model_name, checkpoint_path, batch_size, save_per_residue_embeddings, save_per_protein_embeddings, threads, mask_threshold, has_duplicate_locus)</code>","text":"<p>Wrapper command for baktfold predict. Predicts embeddings using ProstT5 encoder + CNN prediction head.</p> <p>Parameters:</p> Name Type Description Default <code>hypotheticals</code> <code>Dict[str, any]</code> <p>feature dict for all Bakta hypothetical proteins</p> required <code>cds_dict</code> <code>Dict[str, any]</code> <p>id:aa dictionary</p> required <code>output</code> <code>str</code> <p>Output directory path.</p> required <code>prefix</code> <code>str</code> <p>Prefix for output file names.</p> required <code>cpu</code> <code>bool</code> <p>Flag indicating whether to use CPU for prediction.</p> required <code>omit_probs</code> <code>bool</code> <p>Flag indicating whether to omit prediction probabilities from ProstT5.</p> required <code>model_dir</code> <code>str</code> <p>Directory containing the ProstT5 model.</p> required <code>model_name</code> <code>str</code> <p>Name of the ProstT5 model.</p> required <code>checkpoint_path</code> <code>Path</code> <p>Path to ProstT5 CNN checkpoint.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for prediction.</p> required <code>proteins_flag</code> <code>bool</code> <p>True if baktfold proteins-predict, false otherwise</p> required <code>save_per_residue_embeddings</code> <code>bool</code> <p>Whether to save per residue embeddings to h5 file. Defaults to False.</p> required <code>save_per_protein_embeddings</code> <code>bool</code> <p>Whether to save mean per protein embeddings to h5 file. Defaults to False.</p> required <p>Returns:</p> Name Type Description <code>hypotheticals</code> <code>Dict[str, any]</code> <p>feature dict for all Bakta hypothetical proteins. Updated with ProstT5 3Di strings (unmasked)</p> Source code in <code>src/baktfold/subcommands/predict.py</code> <pre><code>def subcommand_predict(\n    hypotheticals: dict,\n    cds_dict: dict,\n    output: Path,\n    prefix: str,\n    cpu: bool,\n    omit_probs: bool,\n    model_dir: Path,\n    model_name: str,\n    checkpoint_path: Path,\n    batch_size: int,\n    save_per_residue_embeddings: bool,\n    save_per_protein_embeddings: bool,\n    threads: int,\n    mask_threshold: float,\n    has_duplicate_locus: bool\n) -&gt; bool:\n    \"\"\"\n    Wrapper command for baktfold predict. Predicts embeddings using ProstT5 encoder + CNN prediction head.\n\n    Args:\n        hypotheticals (Dict[str, any]): feature dict for all Bakta hypothetical proteins\n        cds_dict (Dict[str, any]): id:aa dictionary\n        output (str): Output directory path.\n        prefix (str): Prefix for output file names.\n        cpu (bool): Flag indicating whether to use CPU for prediction.\n        omit_probs (bool): Flag indicating whether to omit prediction probabilities from ProstT5.\n        model_dir (str): Directory containing the ProstT5 model.\n        model_name (str): Name of the ProstT5 model.\n        checkpoint_path (Path): Path to ProstT5 CNN checkpoint.\n        batch_size (int): Batch size for prediction.\n        proteins_flag (bool): True if baktfold proteins-predict, false otherwise\n        save_per_residue_embeddings (bool, optional): Whether to save per residue embeddings to h5 file. Defaults to False.\n        save_per_protein_embeddings (bool, optional): Whether to save mean per protein embeddings to h5 file. Defaults to False.\n\n    Returns:\n        hypotheticals (Dict[str, any]): feature dict for all Bakta hypothetical proteins. Updated with ProstT5 3Di strings (unmasked)\n    \"\"\"\n\n    logger.info('Predicting 3Di sequences using ProstT5')\n\n    fasta_aa: Path = Path(output) / f\"{prefix}_aa.fasta\"\n\n    ############\n    # prostt5\n    ############\n\n    fasta_3di: Path = Path(output) / f\"{prefix}_3di.fasta\"\n    # embeddings h5 - will only be generated if flag is true\n    output_h5_per_residue: Path = Path(output) / f\"{prefix}_embeddings_per_residue.h5\"\n    output_h5_per_protein: Path = Path(output) / f\"{prefix}_embeddings_per_protein.h5\"\n\n    if cpu is True:\n        half_precision = False\n    else:\n        half_precision = True\n\n    if omit_probs:\n        output_probs = False\n    else:\n        output_probs = True\n\n    prediction_dict = get_embeddings(\n        hypotheticals,\n        cds_dict,\n        output,\n        prefix,\n        model_dir,\n        model_name,\n        checkpoint_path,\n        fasta_3di,\n        output_h5_per_residue,\n        output_h5_per_protein,\n        half_precision=half_precision,\n        max_residues=5000,\n        max_seq_len=1000,\n        max_batch=batch_size,\n        cpu=cpu,\n        output_probs=output_probs,\n        save_per_residue_embeddings=save_per_residue_embeddings,\n        save_per_protein_embeddings=save_per_protein_embeddings,\n        threads=threads,\n        mask_threshold=mask_threshold,\n        has_duplicate_locus=has_duplicate_locus\n    )\n\n    mask_prop_threshold = mask_threshold/100\n\n    #######\n    # update the feature dict with 3Di \n    # easiest just \n    #######\n\n\n\n    # for feat in hypotheticals:\n    #     pred = prediction_dict.get(feat['locus']) # None if it doesn't exist\n    #     feat['3di'] = pred[2].tolist() if pred is not None else None\n\n    ########\n    ## write the AA CDS to file\n    ######\n\n\n    # check all the lengths of the predictions are &gt;0 in case of OOMs and filter out those that arent\n    prediction_dict = {\n                k: v for k, v in prediction_dict.items() if len(v[0]) &gt; 0\n            }\n\n\n    with open(fasta_aa, \"w+\") as out_f:\n        for cds_id, prot_seq in cds_dict.items():\n\n            out_f.write(f\"&gt;{cds_id}\\n\")\n\n                # prediction_contig_dict[seq_id][2] these are teh ProstT5 confidence scores from 0-1 - need to convert to list\n\n            try:\n                # this will fail if ProstT5 OOM fails (or fails for some other reason)\n                prot_seq = mask_low_confidence_aa(prot_seq, prediction_dict[cds_id][2].tolist(), threshold=mask_prop_threshold)\n            except (KeyError, IndexError):\n                # in that case, just return 'X' aka masked proteins\n                prot_seq = \"X\" * len(prot_seq)\n\n            out_f.write(f\"{prot_seq}\\n\")\n\n\n    return hypotheticals\n</code></pre>"},{"location":"reference/subcommands/#src.baktfold.subcommands.compare.subcommand_compare","title":"<code>subcommand_compare(hypotheticals, output, threads, evalue, sensitivity, database, prefix, predictions_dir, structures, structure_dir, logdir, proteins_flag, max_seqs, ultra_sensitive, extra_foldseek_params, custom_db, foldseek_gpu, custom_annotations, has_duplicate_locus, fast)</code>","text":"<p>Compare 3Di or PDB structures to the baktfold DB</p> <p>Parameters:</p> Name Type Description Default <code>hypotheticals</code> <code>Dict</code> <p>hypothetical features dictionary</p> required <code>output</code> <code>Path</code> <p>Path to the output directory.</p> required <code>threads</code> <code>int</code> <p>Number of threads to use.</p> required <code>evalue</code> <code>float</code> <p>E-value threshold.</p> required <code>card_vfdb_evalue</code> <code>float</code> <p>E-value threshold for CARD and VFDB databases.</p> required <code>sensitivity</code> <code>float</code> <p>Sensitivity threshold.</p> required <code>database</code> <code>Path</code> <p>Path to the reference database.</p> required <code>prefix</code> <code>str</code> <p>Prefix for output files.</p> required <code>predictions_dir</code> <code>Optional[Path]</code> <p>Path to the directory containing predictions.</p> required <code>structures</code> <code>bool</code> <p>Flag indicating whether structures files are used.</p> required <code>structure_dir</code> <code>Optional[Path]</code> <p>Path to the directory containing structures (.pdb or .cif) files.</p> required <code>logdir</code> <code>Path</code> <p>Path to the directory for log files.</p> required <code>proteins_flag</code> <code>bool</code> <p>Flag indicating whether proteins are used.</p> required <code>max_seqs</code> <code>int</code> <p>Maximum results per query sequence allowed to pass the prefilter for foldseek.</p> required <code>ultra_sensitive</code> <code>bool</code> <p>Whether to skip foldseek prefilter for maximum sensitivity</p> required <code>extra_foldseek_params</code> <code>str</code> <p>Extra foldseek search parameters</p> required <code>custom_db</code> <code>str</code> <p>Custom foldseek database</p> required <code>foldseek_gpu</code> <code>bool</code> <p>Use Foldseek-GPU acceleration and ungappedprefilter</p> required <code>custom_annotations</code> <code>Optional[Path]</code> <p>Path to the tsv containing the custom_db annotations, 2 columns </p> required <code>has_duplicate_locus</code> <code>bool</code> <p>If same locus tag has multiple annots (can happen in some euks)</p> required <code>fast</code> <code>bool</code> <p>If true, skips AFDB search</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if sub-databases are created successfully, False otherwise.</p> Source code in <code>src/baktfold/subcommands/compare.py</code> <pre><code>def subcommand_compare(\n    hypotheticals: Dict,\n    output: Path,\n    threads: int,\n    evalue: float,\n    sensitivity: float,\n    database: Path,\n    prefix: str,\n    predictions_dir: Optional[Path],\n    structures: bool,\n    structure_dir: Optional[Path],\n    logdir: Path,\n    proteins_flag: bool,\n    max_seqs: int,\n    ultra_sensitive: bool,\n    extra_foldseek_params: str,\n    custom_db: str,\n    foldseek_gpu: bool,\n    custom_annotations: Optional[Path],\n    has_duplicate_locus: bool, \n    fast: bool\n) -&gt; bool:\n    \"\"\"\n    Compare 3Di or PDB structures to the baktfold DB\n\n    Parameters:\n        hypotheticals (Dict):  hypothetical features dictionary\n        output (Path): Path to the output directory.\n        threads (int): Number of threads to use.\n        evalue (float): E-value threshold.\n        card_vfdb_evalue (float): E-value threshold for CARD and VFDB databases.\n        sensitivity (float): Sensitivity threshold.\n        database (Path): Path to the reference database.\n        prefix (str): Prefix for output files.\n        predictions_dir (Optional[Path]): Path to the directory containing predictions.\n        structures (bool): Flag indicating whether structures files are used.\n        structure_dir (Optional[Path]): Path to the directory containing structures (.pdb or .cif) files.\n        logdir (Path): Path to the directory for log files.\n        proteins_flag (bool): Flag indicating whether proteins are used.\n        max_seqs (int): Maximum results per query sequence allowed to pass the prefilter for foldseek.\n        ultra_sensitive (bool): Whether to skip foldseek prefilter for maximum sensitivity\n        extra_foldseek_params (str): Extra foldseek search parameters\n        custom_db (str): Custom foldseek database\n        foldseek_gpu (bool): Use Foldseek-GPU acceleration and ungappedprefilter\n        custom_annotations (Optional[Path]): Path to the tsv containing the custom_db annotations, 2 columns \n        has_duplicate_locus (bool): If same locus tag has multiple annots (can happen in some euks)\n        fast (bool): If true, skips AFDB search\n    Returns:\n        bool: True if sub-databases are created successfully, False otherwise.\n    \"\"\"\n\n\n    # input predictions or structures\n    if structures is False:\n        # prostT5\n        fasta_aa_input: Path = Path(predictions_dir) / f\"{prefix}_aa.fasta\"\n        fasta_3di_input: Path = Path(predictions_dir) / f\"{prefix}_3di.fasta\"\n\n    fasta_aa: Path = Path(output) / f\"{prefix}_aa.fasta\"\n    fasta_3di: Path = Path(output) / f\"{prefix}_3di.fasta\"\n\n    ## copy the AA and 3Di from predictions directory \n    # if structures is false and baktfold compare is the command\n    # Otherwise it will just copy itself\n\n    if structures is False:\n        if fasta_3di_input.exists():\n            logger.info(\n                f\"Checked that the 3Di CDS file {fasta_3di_input} exists from baktfold predict\"\n            )\n            if fasta_3di.exists() is False:\n                shutil.copyfile(fasta_3di_input, fasta_3di)\n        else:\n            logger.error(\n                f\"The 3Di CDS file {fasta_3di_input} does not exist. Please run baktfold predict and/or check the prediction directory {predictions_dir}\"\n            )\n        # copy the aa to file\n        if fasta_aa_input.exists():\n            logger.info(\n                f\"Checked that the AA CDS file {fasta_aa_input} exists from baktfold predict.\"\n            )\n            if fasta_aa.exists() is False:\n                shutil.copyfile(fasta_aa_input, fasta_aa)\n        else:\n            logger.error(\n                f\"The AA CDS file {fasta_aa_input} does not exist. Please run baktfold predict and/or check the prediction directory {predictions_dir}\"\n                )\n\n    ## write the AAs to file if structures is true because can't just copy from prediction_dir\n    else:\n        ## write the CDS to file\n        logger.info(f\"Writing the AAs to file {fasta_aa}.\")\n\n        with open(fasta_aa, \"w+\") as out_f:\n            for entry in hypotheticals:\n                if has_duplicate_locus:\n                    header = f\"&gt;{entry['id']}\\n\"\n                else:\n                    header = f\"&gt;{entry['locus']}\\n\"\n                seq = f\"{entry['aa']}\\n\"\n                out_f.write(header)\n                out_f.write(seq)\n\n\n    ############\n    # create foldseek db\n    ############\n\n    foldseek_query_db_path: Path = Path(output) / \"foldseek_db\"\n    foldseek_query_db_path.mkdir(parents=True, exist_ok=True)\n\n    if structures is True:\n        logger.info(\"Creating a foldseek query database from structures.\")\n\n        generate_foldseek_db_from_structures(\n            fasta_aa,\n            foldseek_query_db_path,\n            structure_dir,\n            logdir,\n            prefix,\n            proteins_flag,\n        )\n    else:\n        generate_foldseek_db_from_aa_3di(\n            fasta_aa, fasta_3di, foldseek_query_db_path, logdir, prefix\n        )\n\n    short_db_name = prefix\n\n    # db search \n\n    database_name = \"swissprot\"\n\n    if short_db_name == database_name:\n        logger.error(\n            f\"Please choose a different -p {prefix} as this conflicts with the {database_name}\"\n        )\n\n    #####\n    # foldseek search\n    #####\n\n    query_db: Path = Path(foldseek_query_db_path) / short_db_name\n    target_db: Path = Path(database) / database_name\n\n    # make result and temp dirs\n    result_db_base: Path = Path(output) / \"result_db\"\n    result_db_base.mkdir(parents=True, exist_ok=True)\n    result_db: Path = Path(result_db_base) / \"result_db\"\n\n    temp_db: Path = Path(output) / \"temp_db\"\n    temp_db.mkdir(parents=True, exist_ok=True)\n\n    # make result tsv\n    result_tsv: Path = Path(output) / \"foldseek_results_swissprot.tsv\"\n\n    # run foldseek search\n    run_foldseek_search(\n        query_db,\n        target_db,\n        result_db,\n        temp_db,\n        threads,\n        logdir,\n        evalue,\n        sensitivity,\n        max_seqs,\n        ultra_sensitive,\n        extra_foldseek_params,\n        foldseek_gpu,\n        structures\n    )\n\n\n    create_result_tsv(query_db, target_db, result_db, result_tsv, logdir, foldseek_gpu, structures, threads)\n\n    swissprot_df = get_tophit(result_tsv, structures, cath=False)\n\n\n\n\n    #####\n    # foldseek search AFDB Clusters\n    # by default yes, but not if no fast\n    #####\n\n    if not fast:\n\n        database_name = \"AFDBClusters\"\n\n        if short_db_name == database_name:\n            logger.error(\n                f\"Please choose a different -p {prefix} as this conflicts with the {database_name}\"\n            )\n\n        query_db: Path = Path(foldseek_query_db_path) / short_db_name\n        target_db: Path = Path(database) / database_name\n\n        # make result and temp dirs\n        result_db_base: Path = Path(output) / \"result_db\"\n        result_db_base.mkdir(parents=True, exist_ok=True)\n        result_db: Path = Path(result_db_base) / \"result_afdb_db\"\n\n        temp_db: Path = Path(output) / \"temp_db\"\n        temp_db.mkdir(parents=True, exist_ok=True)\n\n        # make result tsv\n        result_tsv: Path = Path(output) / \"foldseek_results_afdb_clusters.tsv\"\n\n        # run foldseek search\n        run_foldseek_search(\n            query_db,\n            target_db,\n            result_db,\n            temp_db,\n            threads,\n            logdir,\n            evalue,\n            sensitivity,\n            max_seqs,\n            ultra_sensitive,\n            extra_foldseek_params,\n            foldseek_gpu,\n            structures\n        )\n\n\n        create_result_tsv(query_db, target_db, result_db, result_tsv, logdir, foldseek_gpu, structures, threads)\n\n        afdbclusters_df = get_tophit(result_tsv,structures, cath=False)\n\n    else:\n        logger.info(\"Skipping AFDB Clusters search as --fast specified.\")\n\n    #####\n    # foldseek search pdb\n    #####\n\n\n    database_name = \"pdb\"\n\n    if short_db_name == database_name:\n        logger.error(\n            f\"Please choose a different -p {prefix} as this conflicts with the {database_name}\"\n        )\n\n    query_db: Path = Path(foldseek_query_db_path) / short_db_name\n    target_db: Path = Path(database) / database_name\n\n    # make result and temp dirs\n    result_db_base: Path = Path(output) / \"result_db\"\n    result_db_base.mkdir(parents=True, exist_ok=True)\n    result_db: Path = Path(result_db_base) / \"result_pdb_db\"\n\n    temp_db: Path = Path(output) / \"temp_db\"\n    temp_db.mkdir(parents=True, exist_ok=True)\n\n    # make result tsv\n    result_tsv: Path = Path(output) / \"foldseek_results_pdb.tsv\"\n\n    # run foldseek search\n    run_foldseek_search(\n        query_db,\n        target_db,\n        result_db,\n        temp_db,\n        threads,\n        logdir,\n        evalue,\n        sensitivity,\n        max_seqs,\n        ultra_sensitive,\n        extra_foldseek_params,\n        foldseek_gpu,\n        structures\n    )\n\n\n    create_result_tsv(query_db, target_db, result_db, result_tsv, logdir, foldseek_gpu, structures, threads)\n\n    pdb_df = get_tophit(result_tsv,structures, cath=False)\n\n\n    #####\n    # foldseek search cath\n    #####\n\n\n    database_name = \"cath\"\n\n    if short_db_name == database_name:\n        logger.error(\n            f\"Please choose a different -p {prefix} as this conflicts with the {database_name}\"\n        )\n\n    query_db: Path = Path(foldseek_query_db_path) / short_db_name\n    target_db: Path = Path(database) / database_name\n\n    # make result and temp dirs\n    result_db_base: Path = Path(output) / \"result_db\"\n    result_db_base.mkdir(parents=True, exist_ok=True)\n    result_db: Path = Path(result_db_base) / \"result_cath_db\"\n    result_db_greedy_best_hits: Path = Path(result_db_base) / \"result_cath_db_greedy_best_hits\"\n\n    temp_db: Path = Path(output) / \"temp_db\"\n    temp_db.mkdir(parents=True, exist_ok=True)\n\n    # make result tsv\n    result_tsv: Path = Path(output) / \"foldseek_results_cath.tsv\"\n    result_greedy_tsv: Path = Path(output) /  \"foldseek_results_cath_greedy_tophit\"\n\n    # run foldseek search\n    run_foldseek_search(\n        query_db,\n        target_db,\n        result_db,\n        temp_db,\n        threads,\n        logdir,\n        evalue,\n        sensitivity,\n        max_seqs,\n        ultra_sensitive,\n        extra_foldseek_params,\n        foldseek_gpu,\n        structures\n    )\n\n    # this keeps the greedy best hits for cath\n    # we actually don't keep the single tophit - multidomain/fold proteins should have multiple non-overlapping CATH hits\n    # this is equivalent to using --greedy-best-hits with foldseek easy-search\n    summarise_hits(result_db, result_db_greedy_best_hits, logdir, threads)\n\n    # saves all CATH hits first\n    create_result_tsv(query_db, target_db, result_db, result_tsv, logdir, foldseek_gpu, structures, threads)\n    # save greedy CATH tophits\n    create_result_tsv(query_db, target_db, result_db_greedy_best_hits, result_greedy_tsv, logdir, foldseek_gpu, structures, threads)\n\n    # this just reads it in with appropriate headers\n    cath_df = get_tophit(result_greedy_tsv, structures, cath=True)\n\n    # write tophits\n    swissprot_tophit_path: Path = Path(output) / \"baktfold_swissprot_tophit.tsv\"\n    io.write_foldseek_tophit(swissprot_df, swissprot_tophit_path)\n\n    if not fast:\n        afdb_tophit_path: Path = Path(output) / \"baktfold_afdbclusters_tophit.tsv\"\n        io.write_foldseek_tophit(afdbclusters_df, afdb_tophit_path)\n\n    pdb_tophit_path: Path = Path(output) / \"baktfold_pdb_tophit.tsv\"\n    io.write_foldseek_tophit(pdb_df, pdb_tophit_path)\n\n    cath_tophit_path: Path = Path(output) / \"baktfold_cath_tophit.tsv\"\n    io.write_foldseek_tophit(cath_df, cath_tophit_path)\n    # remove result_greedy_tsv (identical to tophit, will make it confusing)\n    remove_file(result_greedy_tsv) \n\n    # custom db output \n\n    #####\n    # custom db\n    #####\n\n\n    if custom_db:\n\n        try:\n\n            logger.info(f\"Foldseek will also be run against your custom database {custom_db}\")\n            # make result and temp dirs\n            result_db_custom: Path = Path(result_db_base) / \"result_db_custom\"\n            result_tsv_custom: Path = Path(output) / \"foldseek_results_custom.tsv\"\n\n            run_foldseek_search(\n            query_db,\n            Path(custom_db),\n            result_db_custom,\n            temp_db,\n            threads,\n            logdir,\n            evalue,\n            sensitivity,\n            max_seqs,\n            ultra_sensitive,\n            extra_foldseek_params,\n            foldseek_gpu,\n            structures\n        )\n\n            create_result_tsv(query_db, Path(custom_db),\n                result_db_custom,\n                result_tsv_custom, logdir, foldseek_gpu, structures, threads)\n\n            custom_df = get_tophit(result_tsv_custom,structures, cath=False)\n\n            custom_db_tophit_path: Path = Path(output) / \"baktfold_custom_db_tophit.tsv\"\n            io.write_foldseek_tophit(custom_df, custom_db_tophit_path)\n\n        except:\n            logger.error(f\"Foldseek failed to run against your custom database {custom_db}. Please check that it is formatted correctly as a Foldseek database\")\n\n\n    ####\n    # lookup\n    ####\n\n    if proteins_flag: # baktfold proteins \n\n        # note aas passed as hypotheticals to the overall function - so in and out as aas\n\n        aas = pstc.parse(hypotheticals, swissprot_df, 'swissprot', has_duplicate_locus=False)\n        if not fast:\n            aas = pstc.parse(aas, afdbclusters_df, 'afdb', has_duplicate_locus=False)\n        aas = pstc.parse(aas, pdb_df, 'pdb', has_duplicate_locus=False)\n        aas = pstc.parse(aas, cath_df, 'cath', has_duplicate_locus=False)\n        if custom_db:\n            aas = pstc.parse(aas, custom_df, 'custom_db', has_duplicate_locus=False)\n\n        # get the lookup descriptions for each of them\n        # this requires the DB\n\n        #aas = pstc.lookup(aas, Path(database), custom_annotations)\n        aas = pstc.lookup_sql(aas, Path(database), threads)\n        # add the custom annotations if it is provided\n        if custom_annotations:\n            aas = pstc.lookup_custom(aas, Path(database), custom_annotations)\n\n        return aas\n\n    else: # baktfold run\n\n        # add the Swissprot and AFDB and PDB tophits to the json\n        hypotheticals = pstc.parse(hypotheticals, swissprot_df, 'swissprot', has_duplicate_locus)\n        if not fast:\n            hypotheticals = pstc.parse(hypotheticals, afdbclusters_df, 'afdb', has_duplicate_locus)\n        hypotheticals = pstc.parse(hypotheticals, pdb_df, 'pdb', has_duplicate_locus)\n        hypotheticals = pstc.parse(hypotheticals, cath_df, 'cath', has_duplicate_locus)\n        if custom_db:\n            hypotheticals = pstc.parse(hypotheticals, custom_df, 'custom_db', has_duplicate_locus)\n\n        # get the lookup descriptions for each of them\n        # hypotheticals = pstc.lookup(hypotheticals, Path(database), custom_annotations)\n        hypotheticals = pstc.lookup_sql(hypotheticals, Path(database), threads)\n        if custom_annotations:\n            hypotheticals = pstc.lookup_custom(hypotheticals, Path(database), custom_annotations)\n\n        return hypotheticals\n</code></pre>"},{"location":"reference/utils/","title":"Utils","text":"<p>Originally taken from Michael Hall's tbpore https://github.com/mbhall88/tbpore/blob/main/tbpore/external_tools.py</p> <p>Also used by a variety of other tools (Dnaapler, Plassembler, Pharokka)</p>"},{"location":"reference/utils/#src.baktfold.utils.external_tools.ExternalTool","title":"<code>ExternalTool</code>","text":"<p>Class for running external tools.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>str</code> <p>The path to the tool to run.</p> required <code>input</code> <code>str</code> <p>The input file.</p> required <code>output</code> <code>str</code> <p>The output file.</p> required <code>params</code> <code>str</code> <p>The parameters to pass to the tool.</p> required <code>logdir</code> <code>Path</code> <p>The directory to store log files.</p> required <p>Attributes:</p> Name Type Description <code>command</code> <code>List[str]</code> <p>The command to run.</p> <code>out_log</code> <code>str</code> <p>The path to the stdout log file.</p> <code>err_log</code> <code>str</code> <p>The path to the stderr log file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n&gt;&gt;&gt; tool.command\n[\"tool\", \"params\", \"output\", \"input\"]\n&gt;&gt;&gt; tool.out_log\n\"logdir/tool_1234567890abcdef1234567890abcdef.out\"\n&gt;&gt;&gt; tool.err_log\n\"logdir/tool_1234567890abcdef1234567890abcdef.err\"\n</code></pre> Source code in <code>src/baktfold/utils/external_tools.py</code> <pre><code>class ExternalTool:\n    \"\"\"\n    Class for running external tools.\n\n    Args:\n      tool (str): The path to the tool to run.\n      input (str): The input file.\n      output (str): The output file.\n      params (str): The parameters to pass to the tool.\n      logdir (Path): The directory to store log files.\n\n    Attributes:\n      command (List[str]): The command to run.\n      out_log (str): The path to the stdout log file.\n      err_log (str): The path to the stderr log file.\n\n    Examples:\n      &gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n      &gt;&gt;&gt; tool.command\n      [\"tool\", \"params\", \"output\", \"input\"]\n      &gt;&gt;&gt; tool.out_log\n      \"logdir/tool_1234567890abcdef1234567890abcdef.out\"\n      &gt;&gt;&gt; tool.err_log\n      \"logdir/tool_1234567890abcdef1234567890abcdef.err\"\n    \"\"\"\n    def __init__(self, tool: str, input: str, output: str, params: str, logdir: Path):\n        \"\"\"\n        Initializes an ExternalTool object.\n\n        Args:\n          tool (str): The path to the tool to run.\n          input (str): The input file.\n          output (str): The output file.\n          params (str): The parameters to pass to the tool.\n          logdir (Path): The directory to store log files.\n\n        Attributes:\n          command (List[str]): The command to run.\n          out_log (str): The path to the stdout log file.\n          err_log (str): The path to the stderr log file.\n\n        Examples:\n          &gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n          &gt;&gt;&gt; tool.command\n          [\"tool\", \"params\", \"output\", \"input\"]\n          &gt;&gt;&gt; tool.out_log\n          \"logdir/tool_1234567890abcdef1234567890abcdef.out\"\n          &gt;&gt;&gt; tool.err_log\n          \"logdir/tool_1234567890abcdef1234567890abcdef.err\"\n        \"\"\"\n        logdir = Path(logdir)   \n        self.command: List[str] = self._build_command(tool, input, output, params)\n        Path(logdir).mkdir(parents=True, exist_ok=True)\n        command_hash = hashlib.sha256(self.command_as_str.encode(\"utf-8\")).hexdigest()\n        tool_name = Path(tool).name\n        logfile_prefix: Path = logdir / f\"{tool_name}_{command_hash}\"\n        self.out_log = f\"{logfile_prefix}.out\"\n        self.err_log = f\"{logfile_prefix}.err\"\n\n    @property\n    def command_as_str(self) -&gt; str:\n        \"\"\"\n        Returns the command as a string.\n\n        Returns:\n          str: The command as a string.\n\n        Examples:\n          &gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n          &gt;&gt;&gt; tool.command_as_str\n          \"tool params output input\"\n        \"\"\"\n        return shlex.join(self.command)\n\n    @staticmethod\n    def _build_command(tool: str, input: str, output: str, params: str) -&gt; List[str]:\n        \"\"\"\n        Builds the command to run.\n\n        Args:\n          tool (str): The path to the tool to run.\n          input (str): The input file.\n          output (str): The output file.\n          params (str): The parameters to pass to the tool.\n\n        Returns:\n          List[str]: The command to run.\n\n        Examples:\n          &gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n          &gt;&gt;&gt; tool._build_command(\"tool\", \"input\", \"output\", \"params\")\n          [\"tool\", \"params\", \"output\", \"input\"]\n        \"\"\"\n        # note: shlex.join does not allow us to shlex.split() later\n        # this is explicitly a \" \".join()\n        command = \" \".join([tool, params, output, input])\n        escaped_command = shlex.split(command)\n        return escaped_command\n\n    def run(self) -&gt; None:\n        \"\"\"\n        Runs the tool.\n\n        Examples:\n          &gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n          &gt;&gt;&gt; tool.run()\n        \"\"\"\n        with open(self.out_log, \"w\") as stdout_fh, open(self.err_log, \"w\") as stderr_fh:\n            print(f\"Command line: {self.command_as_str}\", file=stderr_fh)\n            logger.info(f\"Started running {self.command_as_str} ...\")\n            self._run_core(self.command, stdout_fh=stdout_fh, stderr_fh=stderr_fh)\n            logger.info(f\"Done running {self.command_as_str}\")\n\n    \"\"\"\n    stream to terminal (aria2c) so the user knows how long it is taking\n    \"\"\"\n\n    def run_stream(self) -&gt; None:\n        \"\"\"\n        Runs the tool and streams the output to the terminal.\n\n        Examples:\n          &gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n          &gt;&gt;&gt; tool.run_stream()\n        \"\"\"\n        with open(self.out_log, \"w\") as stdout_fh, open(self.err_log, \"w\") as stderr_fh:\n            print(f\"Command line: {self.command_as_str}\", file=stderr_fh)\n            logger.info(f\"Started running {self.command_as_str} ...\")\n\n            process = subprocess.Popen(\n                self.command,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT,\n                bufsize=1,\n                universal_newlines=True,\n            )\n\n            for line in process.stdout:\n                print(line, end=\"\")         # Live output to terminal\n                stdout_fh.write(line)       # Also write to stdout log\n\n            process.stdout.close()\n            return_code = process.wait()\n\n            logger.info(f\"Done running {self.command_as_str}\")\n\n            if return_code != 0:\n                raise subprocess.CalledProcessError(return_code, self.command)\n\n\n    @staticmethod\n    def _run_core(command: List[str], stdout_fh, stderr_fh) -&gt; None:\n        \"\"\"\n        Runs the tool.\n\n        Args:\n          command (List[str]): The command to run.\n          stdout_fh: The file handle to write stdout to.\n          stderr_fh: The file handle to write stderr to.\n\n        Examples:\n          &gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n          &gt;&gt;&gt; tool._run_core([\"tool\", \"params\", \"output\", \"input\"], stdout_fh, stderr_fh)\n        \"\"\"\n        subprocess.check_call(command, stdout=stdout_fh, stderr=stderr_fh)\n\n    @staticmethod\n    def run_tools(\n        tools_to_run: Tuple[\"ExternalTool\", ...], ctx: Optional[click.Context] = None\n    ) -&gt; None:\n        \"\"\"\n        Runs a list of tools.\n\n        Args:\n          tools_to_run (Tuple[ExternalTool]): The list of tools to run.\n          ctx (Optional[click.Context]): The click context.\n\n        Examples:\n          &gt;&gt;&gt; tool1 = ExternalTool(\"tool1\", \"input1\", \"output1\", \"params1\", \"logdir\")\n          &gt;&gt;&gt; tool2 = ExternalTool(\"tool2\", \"input2\", \"output2\", \"params2\", \"logdir\")\n          &gt;&gt;&gt; ExternalTool.run_tools((tool1, tool2))\n          &gt;&gt;&gt; ExternalTool.run_tools((tool1, tool2), ctx)\n        \"\"\"\n        for tool in tools_to_run:\n            try:\n                tool.run()\n            except subprocess.CalledProcessError as error:\n                logger.error(\n                    f\"Error calling {tool.command_as_str} (return code {error.returncode})\"\n                )\n                logger.error(f\"Please check stdout log file: {tool.out_log}\")\n                logger.error(f\"Please check stderr log file: {tool.err_log}\")\n                logger.error(\"Temporary files are preserved for debugging\")\n                logger.error(\"Exiting...\")\n\n                if ctx:\n                    ctx.exit(1)\n                else:\n                    sys.exit(1)\n\n    \"\"\"\n    Only one toolf\n    \"\"\"\n\n    @staticmethod\n    def run_tool(tool: \"ExternalTool\", ctx: Optional[click.Context] = None) -&gt; None:\n        \"\"\"\n        Runs the given external tool.\n\n        Args:\n          tool (ExternalTool): The external tool to run.\n          ctx (Optional[click.Context]): The click context to use. Defaults to None.\n\n        Returns:\n          None.\n\n        Raises:\n          subprocess.CalledProcessError: If there is an error calling the external tool.\n\n        Examples:\n          &gt;&gt;&gt; tool = ExternalTool()\n          &gt;&gt;&gt; ExternalTool.run_tool(tool)\n          None\n        \"\"\"\n        try:\n            tool.run()\n        except subprocess.CalledProcessError as error:\n            logger.error(\n                f\"Error calling {tool.command_as_str} (return code {error.returncode})\"\n            )\n            logger.error(f\"Please check stdout log file: {tool.out_log}\")\n            logger.error(f\"Please check stderr log file: {tool.err_log}\")\n            logger.error(\"Temporary files are preserved for debugging\")\n            logger.error(\"Exiting...\")\n\n            if ctx:\n                ctx.exit(1)\n            else:\n                sys.exit(1)\n\n\n    \"\"\"\n    Only download - so can print the aria2c output to screen\n    \"\"\"\n\n    @staticmethod\n    def run_download(tool: \"ExternalTool\", ctx: Optional[click.Context] = None) -&gt; None:\n        \"\"\"\n        Runs the given external tool and prints the aria2c output to the screen.\n\n        Args:\n          tool (ExternalTool): The external tool to run.\n          ctx (Optional[click.Context]): The click context to use. Defaults to None.\n\n        Returns:\n          None.\n\n        Raises:\n          subprocess.CalledProcessError: If there is an error calling the external tool.\n\n        Examples:\n          &gt;&gt;&gt; tool = ExternalTool()\n          &gt;&gt;&gt; ExternalTool.run_download(tool)\n          None\n        \"\"\"\n        try:\n            tool.run_stream()\n        except subprocess.CalledProcessError as error:\n            logger.error(\n                f\"Error calling {tool.command_as_str} (return code {error.returncode})\"\n            )\n            logger.error(f\"Please check stdout log file: {tool.out_log}\")\n            logger.error(f\"Please check stderr log file: {tool.err_log}\")\n            logger.error(\"Temporary files are preserved for debugging\")\n            logger.error(\"Exiting...\")\n\n            if ctx:\n                ctx.exit(1)\n            else:\n                sys.exit(1)\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.external_tools.ExternalTool.command_as_str","title":"<code>command_as_str: str</code>  <code>property</code>","text":"<p>Returns the command as a string.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The command as a string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n&gt;&gt;&gt; tool.command_as_str\n\"tool params output input\"\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.external_tools.ExternalTool.__init__","title":"<code>__init__(tool, input, output, params, logdir)</code>","text":"<p>Initializes an ExternalTool object.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>str</code> <p>The path to the tool to run.</p> required <code>input</code> <code>str</code> <p>The input file.</p> required <code>output</code> <code>str</code> <p>The output file.</p> required <code>params</code> <code>str</code> <p>The parameters to pass to the tool.</p> required <code>logdir</code> <code>Path</code> <p>The directory to store log files.</p> required <p>Attributes:</p> Name Type Description <code>command</code> <code>List[str]</code> <p>The command to run.</p> <code>out_log</code> <code>str</code> <p>The path to the stdout log file.</p> <code>err_log</code> <code>str</code> <p>The path to the stderr log file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n&gt;&gt;&gt; tool.command\n[\"tool\", \"params\", \"output\", \"input\"]\n&gt;&gt;&gt; tool.out_log\n\"logdir/tool_1234567890abcdef1234567890abcdef.out\"\n&gt;&gt;&gt; tool.err_log\n\"logdir/tool_1234567890abcdef1234567890abcdef.err\"\n</code></pre> Source code in <code>src/baktfold/utils/external_tools.py</code> <pre><code>def __init__(self, tool: str, input: str, output: str, params: str, logdir: Path):\n    \"\"\"\n    Initializes an ExternalTool object.\n\n    Args:\n      tool (str): The path to the tool to run.\n      input (str): The input file.\n      output (str): The output file.\n      params (str): The parameters to pass to the tool.\n      logdir (Path): The directory to store log files.\n\n    Attributes:\n      command (List[str]): The command to run.\n      out_log (str): The path to the stdout log file.\n      err_log (str): The path to the stderr log file.\n\n    Examples:\n      &gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n      &gt;&gt;&gt; tool.command\n      [\"tool\", \"params\", \"output\", \"input\"]\n      &gt;&gt;&gt; tool.out_log\n      \"logdir/tool_1234567890abcdef1234567890abcdef.out\"\n      &gt;&gt;&gt; tool.err_log\n      \"logdir/tool_1234567890abcdef1234567890abcdef.err\"\n    \"\"\"\n    logdir = Path(logdir)   \n    self.command: List[str] = self._build_command(tool, input, output, params)\n    Path(logdir).mkdir(parents=True, exist_ok=True)\n    command_hash = hashlib.sha256(self.command_as_str.encode(\"utf-8\")).hexdigest()\n    tool_name = Path(tool).name\n    logfile_prefix: Path = logdir / f\"{tool_name}_{command_hash}\"\n    self.out_log = f\"{logfile_prefix}.out\"\n    self.err_log = f\"{logfile_prefix}.err\"\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.external_tools.ExternalTool.run","title":"<code>run()</code>","text":"<p>Runs the tool.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n&gt;&gt;&gt; tool.run()\n</code></pre> Source code in <code>src/baktfold/utils/external_tools.py</code> <pre><code>def run(self) -&gt; None:\n    \"\"\"\n    Runs the tool.\n\n    Examples:\n      &gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n      &gt;&gt;&gt; tool.run()\n    \"\"\"\n    with open(self.out_log, \"w\") as stdout_fh, open(self.err_log, \"w\") as stderr_fh:\n        print(f\"Command line: {self.command_as_str}\", file=stderr_fh)\n        logger.info(f\"Started running {self.command_as_str} ...\")\n        self._run_core(self.command, stdout_fh=stdout_fh, stderr_fh=stderr_fh)\n        logger.info(f\"Done running {self.command_as_str}\")\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.external_tools.ExternalTool.run_download","title":"<code>run_download(tool, ctx=None)</code>  <code>staticmethod</code>","text":"<p>Runs the given external tool and prints the aria2c output to the screen.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>ExternalTool</code> <p>The external tool to run.</p> required <code>ctx</code> <code>Optional[click.Context]</code> <p>The click context to use. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> <p>Raises:</p> Type Description <code>subprocess.CalledProcessError</code> <p>If there is an error calling the external tool.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool = ExternalTool()\n&gt;&gt;&gt; ExternalTool.run_download(tool)\nNone\n</code></pre> Source code in <code>src/baktfold/utils/external_tools.py</code> <pre><code>@staticmethod\ndef run_download(tool: \"ExternalTool\", ctx: Optional[click.Context] = None) -&gt; None:\n    \"\"\"\n    Runs the given external tool and prints the aria2c output to the screen.\n\n    Args:\n      tool (ExternalTool): The external tool to run.\n      ctx (Optional[click.Context]): The click context to use. Defaults to None.\n\n    Returns:\n      None.\n\n    Raises:\n      subprocess.CalledProcessError: If there is an error calling the external tool.\n\n    Examples:\n      &gt;&gt;&gt; tool = ExternalTool()\n      &gt;&gt;&gt; ExternalTool.run_download(tool)\n      None\n    \"\"\"\n    try:\n        tool.run_stream()\n    except subprocess.CalledProcessError as error:\n        logger.error(\n            f\"Error calling {tool.command_as_str} (return code {error.returncode})\"\n        )\n        logger.error(f\"Please check stdout log file: {tool.out_log}\")\n        logger.error(f\"Please check stderr log file: {tool.err_log}\")\n        logger.error(\"Temporary files are preserved for debugging\")\n        logger.error(\"Exiting...\")\n\n        if ctx:\n            ctx.exit(1)\n        else:\n            sys.exit(1)\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.external_tools.ExternalTool.run_stream","title":"<code>run_stream()</code>","text":"<p>Runs the tool and streams the output to the terminal.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n&gt;&gt;&gt; tool.run_stream()\n</code></pre> Source code in <code>src/baktfold/utils/external_tools.py</code> <pre><code>def run_stream(self) -&gt; None:\n    \"\"\"\n    Runs the tool and streams the output to the terminal.\n\n    Examples:\n      &gt;&gt;&gt; tool = ExternalTool(\"tool\", \"input\", \"output\", \"params\", \"logdir\")\n      &gt;&gt;&gt; tool.run_stream()\n    \"\"\"\n    with open(self.out_log, \"w\") as stdout_fh, open(self.err_log, \"w\") as stderr_fh:\n        print(f\"Command line: {self.command_as_str}\", file=stderr_fh)\n        logger.info(f\"Started running {self.command_as_str} ...\")\n\n        process = subprocess.Popen(\n            self.command,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            bufsize=1,\n            universal_newlines=True,\n        )\n\n        for line in process.stdout:\n            print(line, end=\"\")         # Live output to terminal\n            stdout_fh.write(line)       # Also write to stdout log\n\n        process.stdout.close()\n        return_code = process.wait()\n\n        logger.info(f\"Done running {self.command_as_str}\")\n\n        if return_code != 0:\n            raise subprocess.CalledProcessError(return_code, self.command)\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.external_tools.ExternalTool.run_tool","title":"<code>run_tool(tool, ctx=None)</code>  <code>staticmethod</code>","text":"<p>Runs the given external tool.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>ExternalTool</code> <p>The external tool to run.</p> required <code>ctx</code> <code>Optional[click.Context]</code> <p>The click context to use. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> <p>Raises:</p> Type Description <code>subprocess.CalledProcessError</code> <p>If there is an error calling the external tool.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool = ExternalTool()\n&gt;&gt;&gt; ExternalTool.run_tool(tool)\nNone\n</code></pre> Source code in <code>src/baktfold/utils/external_tools.py</code> <pre><code>@staticmethod\ndef run_tool(tool: \"ExternalTool\", ctx: Optional[click.Context] = None) -&gt; None:\n    \"\"\"\n    Runs the given external tool.\n\n    Args:\n      tool (ExternalTool): The external tool to run.\n      ctx (Optional[click.Context]): The click context to use. Defaults to None.\n\n    Returns:\n      None.\n\n    Raises:\n      subprocess.CalledProcessError: If there is an error calling the external tool.\n\n    Examples:\n      &gt;&gt;&gt; tool = ExternalTool()\n      &gt;&gt;&gt; ExternalTool.run_tool(tool)\n      None\n    \"\"\"\n    try:\n        tool.run()\n    except subprocess.CalledProcessError as error:\n        logger.error(\n            f\"Error calling {tool.command_as_str} (return code {error.returncode})\"\n        )\n        logger.error(f\"Please check stdout log file: {tool.out_log}\")\n        logger.error(f\"Please check stderr log file: {tool.err_log}\")\n        logger.error(\"Temporary files are preserved for debugging\")\n        logger.error(\"Exiting...\")\n\n        if ctx:\n            ctx.exit(1)\n        else:\n            sys.exit(1)\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.external_tools.ExternalTool.run_tools","title":"<code>run_tools(tools_to_run, ctx=None)</code>  <code>staticmethod</code>","text":"<p>Runs a list of tools.</p> <p>Parameters:</p> Name Type Description Default <code>tools_to_run</code> <code>Tuple[ExternalTool]</code> <p>The list of tools to run.</p> required <code>ctx</code> <code>Optional[click.Context]</code> <p>The click context.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool1 = ExternalTool(\"tool1\", \"input1\", \"output1\", \"params1\", \"logdir\")\n&gt;&gt;&gt; tool2 = ExternalTool(\"tool2\", \"input2\", \"output2\", \"params2\", \"logdir\")\n&gt;&gt;&gt; ExternalTool.run_tools((tool1, tool2))\n&gt;&gt;&gt; ExternalTool.run_tools((tool1, tool2), ctx)\n</code></pre> Source code in <code>src/baktfold/utils/external_tools.py</code> <pre><code>@staticmethod\ndef run_tools(\n    tools_to_run: Tuple[\"ExternalTool\", ...], ctx: Optional[click.Context] = None\n) -&gt; None:\n    \"\"\"\n    Runs a list of tools.\n\n    Args:\n      tools_to_run (Tuple[ExternalTool]): The list of tools to run.\n      ctx (Optional[click.Context]): The click context.\n\n    Examples:\n      &gt;&gt;&gt; tool1 = ExternalTool(\"tool1\", \"input1\", \"output1\", \"params1\", \"logdir\")\n      &gt;&gt;&gt; tool2 = ExternalTool(\"tool2\", \"input2\", \"output2\", \"params2\", \"logdir\")\n      &gt;&gt;&gt; ExternalTool.run_tools((tool1, tool2))\n      &gt;&gt;&gt; ExternalTool.run_tools((tool1, tool2), ctx)\n    \"\"\"\n    for tool in tools_to_run:\n        try:\n            tool.run()\n        except subprocess.CalledProcessError as error:\n            logger.error(\n                f\"Error calling {tool.command_as_str} (return code {error.returncode})\"\n            )\n            logger.error(f\"Please check stdout log file: {tool.out_log}\")\n            logger.error(f\"Please check stderr log file: {tool.err_log}\")\n            logger.error(\"Temporary files are preserved for debugging\")\n            logger.error(\"Exiting...\")\n\n            if ctx:\n                ctx.exit(1)\n            else:\n                sys.exit(1)\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.util.log_fmt","title":"<code>log_fmt = '[&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss}&lt;/green&gt;] &lt;level&gt;{level: &lt;8}&lt;/level&gt; | &lt;level&gt;{message}&lt;/level&gt;'</code>  <code>module-attribute</code>","text":"<p>begin and end functions</p>"},{"location":"reference/utils/#src.baktfold.utils.util.OrderedCommands","title":"<code>OrderedCommands</code>","text":"<p>             Bases: <code>click.Group</code></p> <p>This class will preserve the order of subcommands, which is useful when printing --help</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>class OrderedCommands(click.Group):\n    \"\"\"This class will preserve the order of subcommands, which is useful when printing --help\"\"\"\n\n    def list_commands(self, ctx: click.Context):\n        \"\"\"\n        Returns a list of subcommands in the order they were added.\n\n        Args:\n          ctx (click.Context): The click context.\n\n        Returns:\n          list: A list of subcommands in the order they were added.\n        \"\"\"\n        return list(self.commands)\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.util.OrderedCommands.list_commands","title":"<code>list_commands(ctx)</code>","text":"<p>Returns a list of subcommands in the order they were added.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>click.Context</code> <p>The click context.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list of subcommands in the order they were added.</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def list_commands(self, ctx: click.Context):\n    \"\"\"\n    Returns a list of subcommands in the order they were added.\n\n    Args:\n      ctx (click.Context): The click context.\n\n    Returns:\n      list: A list of subcommands in the order they were added.\n    \"\"\"\n    return list(self.commands)\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.util.baktfold_base","title":"<code>baktfold_base(rel_path)</code>","text":"<p>Returns the absolute path to the given relative path.</p> <p>Parameters:</p> Name Type Description Default <code>rel_path</code> <code>str</code> <p>The relative path to the file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The absolute path to the file.</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def baktfold_base(rel_path):\n    \"\"\"\n    Returns the absolute path to the given relative path.\n\n    Args:\n      rel_path (str): The relative path to the file.\n\n    Returns:\n      str: The absolute path to the file.\n    \"\"\"\n    return os.path.join(os.path.dirname(os.path.realpath(__file__)), rel_path)\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.util.begin_baktfold","title":"<code>begin_baktfold(params, subcommand, no_log=False)</code>","text":"<p>Begin baktfold process.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Dict[str, Any]</code> <p>A dictionary of parameters for baktfold.</p> required <code>subcommand</code> <code>str</code> <p>Subcommand indicating the baktfold operation.</p> required <code>no_log</code> <code>bool</code> <p>No log file</p> <code>False</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Start time of the baktfold process.</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def begin_baktfold(params: Dict[str, Any], subcommand: str, no_log: bool = False) -&gt; int:\n    \"\"\"\n    Begin baktfold process.\n\n    Parameters:\n        params (Dict[str, Any]): A dictionary of parameters for baktfold.\n        subcommand (str): Subcommand indicating the baktfold operation.\n        no_log (bool): No log file\n\n    Returns:\n        int: Start time of the baktfold process.\n    \"\"\"\n    # get start time\n    start_time = time.time()\n\n    cfg.run_start = datetime.now()\n\n    # initial logging stuff\n    if not no_log:\n        log_file = os.path.join(params[\"--output\"], f\"baktfold_{subcommand}_{start_time}.log\")\n        # adds log file\n        logger.add(log_file)\n    logger.add(lambda _: sys.exit(1), level=\"ERROR\")\n\n    print_splash()\n    logger.info(\"baktfold: rapid &amp; standardized annotation of bacterial genomes, MAGs &amp; plasmids using protein structural information\")\n\n    logger.info(f\"You are using baktfold version {get_version()}\")\n    logger.info(\"Repository homepage is https://github.com/gbouras13/baktfold\")\n    logger.info(f\"You are running baktfold {subcommand}\")\n    logger.info(f\"Listing parameters\")\n    for key, value in params.items():\n        logger.info(f\"Parameter: {key} {value}\")\n\n    return start_time\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.util.clean_up_temporary_files","title":"<code>clean_up_temporary_files(output, prefix)</code>","text":"<p>Clean up temporary files generated during the baktfold process.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Path</code> <p>Path to the output directory.</p> required <code>prefix</code> <code>str</code> <p>prefix str</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def clean_up_temporary_files(output: Path, prefix: str) -&gt; None:\n    \"\"\"\n    Clean up temporary files generated during the baktfold process.\n\n    Parameters:\n        output (Path): Path to the output directory.\n        prefix (str): prefix str\n\n\n    Returns:\n        None\n    \"\"\"\n\n    baktfold_aa: Path = Path(output) / f\"{prefix}_aa.fasta\"\n    result_tsv_swissprot: Path = Path(output) / \"foldseek_results_swissprot.tsv\"\n    result_tsv_afdb: Path = Path(output) / \"foldseek_results_afdb_clusters.tsv\"\n    result_tsv_pdb: Path = Path(output) / \"foldseek_results_pdb.tsv\"\n    result_tsv_cath: Path = Path(output) / \"foldseek_results_cath.tsv\"\n    result_tsv_custom: Path = Path(output) / \"foldseek_results_custom.tsv\"\n    foldseek_db: Path = Path(output) / \"foldseek_db\"\n    result_db_base: Path = Path(output) / \"result_db\"\n    temp_db: Path = Path(output) / \"temp_db\"\n\n    remove_directory(result_db_base)\n    remove_directory(temp_db)\n    remove_directory(foldseek_db)\n\n    remove_file(baktfold_aa)\n    remove_file(result_tsv_swissprot)\n    remove_file(result_tsv_afdb)\n    remove_file(result_tsv_pdb)\n    remove_file(result_tsv_custom)\n    remove_file(result_tsv_cath)\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.util.echo_click","title":"<code>echo_click(msg, log=None)</code>","text":"<p>Prints a message to stdout and optionally to a log file.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>The message to print.</p> required <code>log</code> <code>str</code> <p>The path to the log file.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def echo_click(msg, log=None):\n    \"\"\"\n    Prints a message to stdout and optionally to a log file.\n\n    Args:\n      msg (str): The message to print.\n      log (str): The path to the log file.\n\n    Returns:\n      None\n    \"\"\"\n    click.echo(msg, nl=False, err=True)\n    if log:\n        with open(log, \"a\") as lo:\n            lo.write(msg)\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.util.end_baktfold","title":"<code>end_baktfold(start_time, subcommand)</code>","text":"<p>Finish baktfold process and log elapsed time.</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>float</code> <p>Start time of the process.</p> required <code>subcommand</code> <code>str</code> <p>Subcommand name indicating the baktfold operation.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def end_baktfold(start_time: float, subcommand: str) -&gt; None:\n    \"\"\"\n    Finish baktfold process and log elapsed time.\n\n    Parameters:\n        start_time (float): Start time of the process.\n        subcommand (str): Subcommand name indicating the baktfold operation.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Determine elapsed time\n    elapsed_time = time.time() - start_time\n    elapsed_time = round(elapsed_time, 2)\n\n    cfg.run_end = datetime.now()\n    run_duration = (cfg.run_end - cfg.run_start).total_seconds()\n    # logger.info(f'If you use these results please cite Baktfold: https://doi.org/{bc.BAKTA_DOI}')\n    logger.info(f'If you use these results please cite Baktfold: https://github.com/gbouras13/baktfold')\n    logger.info(f'baktfold {subcommand} successfully finished in {int(run_duration / 60):02}:{int(run_duration % 60):02} [mm:ss].')\n\n\n    # Show elapsed time for the process\n    logger.info(f\"baktfold {subcommand} has finished\")\n    logger.info(\"Elapsed time: \" + str(elapsed_time) + \" seconds\")\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.util.get_type_rank","title":"<code>get_type_rank(f)</code>","text":"<p>ranks eukaryotic features 1) in order of gene -&gt; mRNA -&gt; CDS and gene -&gt; tRNA dynamically adjusts if 5'UTR and 3'UTR is present</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def get_type_rank(f):\n    \"\"\"\n    ranks eukaryotic features 1) in order of gene -&gt; mRNA -&gt; CDS and gene -&gt; tRNA\n    dynamically adjusts if 5'UTR and 3'UTR is present\n    \"\"\"\n    t = f['type']\n    strand = f.get('strand', '+')  # default to + if missing\n\n    # fixed ranks\n    base_order = {\n        'gene': 0,\n        'mRNA': 1,\n        'cds': 3,\n        'tRNA': 6\n    }\n\n    # dynamic UTR ordering\n    if t == bc.FEATURE_5UTR:\n        return 2 if strand == '+' else 4\n    if t == bc.FEATURE_3UTR:\n        return 4 if strand == '+' else 2\n\n    return base_order.get(t, 99)   # non-protein features become 99\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.util.get_version","title":"<code>get_version()</code>","text":"<p>Returns the version number from the VERSION file.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>The version number.</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def get_version():\n    \"\"\"\n    Returns the version number from the VERSION file.\n\n    Returns:\n      str: The version number.\n    \"\"\"\n    with open(baktfold_base(\"VERSION\"), \"r\") as f:\n        version = f.readline()\n    return version\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.util.print_citation","title":"<code>print_citation()</code>","text":"<p>Prints the contents of the CITATION file to stdout.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def print_citation():\n    \"\"\"\n    Prints the contents of the CITATION file to stdout.\n\n    Returns:\n      None\n    \"\"\"\n    with open(baktfold_base(\"CITATION\"), \"r\") as f:\n        for line in f:\n            echo_click(line)\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.util.print_splash","title":"<code>print_splash()</code>","text":"<p>Prints the splash screen to stdout.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def print_splash():\n    \"\"\"\n    Prints the splash screen to stdout.\n\n    Returns:\n      None\n    \"\"\"\n    click.echo(\n        \"\"\"\\b\n\n  _           _    _    __      _     _ \n | |         | |  | |  / _|    | |   | |\n | |__   __ _| | _| |_| |_ ___ | | __| |\n | '_ \\ / _` | |/ / __|  _/ _ \\| |/ _` |\n | |_) | (_| |   &lt;| |_| || (_) | | (_| |\n |_.__/ \\__,_|_|\\_\\\\__|_| \\___/|_|\\__,_|\n\n\n\"\"\"\n    )\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.util.remove_directory","title":"<code>remove_directory(dir_path)</code>","text":"<p>Remove a directory and all its contents if it exists.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Path</code> <p>Path to the directory to remove.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def remove_directory(dir_path: Path) -&gt; None:\n    \"\"\"\n    Remove a directory and all its contents if it exists.\n\n    Parameters:\n        dir_path (Path): Path to the directory to remove.\n\n    Returns:\n        None\n    \"\"\"\n    if dir_path.exists():\n        shutil.rmtree(dir_path)\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.util.remove_file","title":"<code>remove_file(file_path)</code>","text":"<p>Remove a file if it exists.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the file to remove.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def remove_file(file_path: Path) -&gt; None:\n    \"\"\"\n    Remove a file if it exists.\n\n    Parameters:\n        file_path (Path): Path to the file to remove.\n\n    Returns:\n        None\n    \"\"\"\n    if file_path.exists():\n        file_path.unlink()  # Use unlink to remove the file\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.util.sort_euk_feature_key","title":"<code>sort_euk_feature_key(f)</code>","text":"<p>Sorts a feature dictionary by start, locus, type rank, and stop.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>dict</code> <p>The feature dictionary.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple of the sorted values.</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def sort_euk_feature_key(f):\n    \"\"\"\n    Sorts a feature dictionary by start, locus, type rank, and stop.\n\n    Args:\n      f (dict): The feature dictionary.\n\n    Returns:\n      tuple: A tuple of the sorted values.\n    \"\"\"\n    start = f.get('start', float('inf'))\n    stop = f.get('stop', float('inf'))\n    locus = f.get('locus')\n    type_rank = get_type_rank(f)\n\n    if locus and type_rank != 99:\n        # Within a locus \u2192 sort by type rank second and stop last (if multiple CDS e.g.)\n        return (start, 0, locus, type_rank, stop)\n    else:\n        # Non-locus or non-gene features \u2192 sort only by start\n        return (start, 1, '', 99, stop)\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.util.touch_file","title":"<code>touch_file(path)</code>","text":"<p>Update the access and modification times of a file to the current time, creating the file if it does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/utils/util.py</code> <pre><code>def touch_file(path: Path) -&gt; None:\n    \"\"\"\n    Update the access and modification times of a file to the current time, creating the file if it does not exist.\n\n    Parameters:\n        path (Path): Path to the file.\n\n    Returns:\n        None\n    \"\"\"\n    with open(path, \"a\"):\n        os.utime(path, None)\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.validation.check_dependencies","title":"<code>check_dependencies()</code>","text":"<p>Checks the dependencies and versions of non Python programs (i.e. Foldseek)</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/baktfold/utils/validation.py</code> <pre><code>def check_dependencies() -&gt; None:\n    \"\"\"\n    Checks the dependencies and versions of non Python programs (i.e. Foldseek)\n\n    Parameters:\n        None\n\n    Returns:\n        None\n\n    \"\"\"\n\n    #############\n    # foldseek\n    #############\n    try:\n        process = sp.Popen([\"foldseek\", \"version\"], stdout=sp.PIPE, stderr=sp.STDOUT)\n    except:\n        logger.error(\"Foldseek not found. Please reinstall baktfold.\")\n\n    foldseek_out, _ = process.communicate()\n    foldseek_out = foldseek_out.decode()\n\n    foldseek_version = foldseek_out.strip()\n\n    if \"941cd33\" in foldseek_version:\n        foldseek_major_version=10\n        foldseek_minor_version=\"941cd33\"\n        logger.info(\n        f\"Foldseek version found is v{foldseek_major_version}.{foldseek_minor_version}\"\n    )\n    else:\n        logger.warning(f\"Foldseek version found is v{foldseek_version}\")\n        logger.warning(f\"baktfold is recommended to be run with Foldseek v10.941cd33\")\n        logger.warning(f\"Using a different Foldseek version is likely to work without issue, but this cannot be guaranteed.\")\n\n\n    logger.info(\"Foldseek version is ok\")\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.validation.check_genbank_and_prokka","title":"<code>check_genbank_and_prokka(filepath, euk)</code>","text":"<p>Validate that an input file is a readable GenBank file and check whether it was annotated using Prokka. The function transparently supports compressed files (e.g., .gz, .bz2, .xz, .zst) via <code>xopen</code>.</p> Validation steps <p>\u2022 Attempts to parse the file as GenBank using Biopython. \u2022 Logs an error and returns None if no GenBank records are found. \u2022 Checks the COMMENT field of each record for a Prokka signature   (\"Annotated using prokka\", case-insensitive). \u2022 If no Prokka annotation is detected, a warning is logged but parsing continues as it is a valid genbank.</p>"},{"location":"reference/utils/#src.baktfold.utils.validation.check_genbank_and_prokka--parameters","title":"Parameters","text":"str <p>Path to the GenBank or compressed GenBank file.</p> flag <p>whether or not the input is eukaryotic (skips prokka)</p>"},{"location":"reference/utils/#src.baktfold.utils.validation.check_genbank_and_prokka--returns","title":"Returns","text":"<p>list[SeqRecord] or None     A list of Biopython SeqRecord objects if parsing succeeds.     Returns None if the file is not valid GenBank or cannot be parsed.</p> Source code in <code>src/baktfold/utils/validation.py</code> <pre><code>def check_genbank_and_prokka(filepath, euk):\n    \"\"\"\n    Validate that an input file is a readable GenBank file and check whether it was\n    annotated using Prokka. The function transparently supports compressed files\n    (e.g., .gz, .bz2, .xz, .zst) via `xopen`.\n\n    Validation steps:\n      \u2022 Attempts to parse the file as GenBank using Biopython.\n      \u2022 Logs an error and returns None if no GenBank records are found.\n      \u2022 Checks the COMMENT field of each record for a Prokka signature\n        (\"Annotated using prokka\", case-insensitive).\n      \u2022 If no Prokka annotation is detected, a warning is logged but parsing continues as it is a valid genbank.\n\n    Parameters\n    ----------\n    filepath : str\n        Path to the GenBank or compressed GenBank file.\n    euk: flag\n        whether or not the input is eukaryotic (skips prokka)\n\n    Returns\n    -------\n    list[SeqRecord] or None\n        A list of Biopython SeqRecord objects if parsing succeeds.\n        Returns None if the file is not valid GenBank or cannot be parsed.\n    \"\"\"\n\n    logger.add(lambda _: sys.exit(1), level=\"ERROR\")\n\n    is_valid_genbank = False\n    is_prokka = False\n\n    try:\n        # Use xopen so gzip/bz2/xz/zst work automatically\n        with xopen(filepath, \"rb\") as handle:\n            # SeqIO.parse expects text handle -&gt; decode\n            # Use .read() is too big; instead wrap in TextIOWrapper\n            import io\n            text_handle = io.TextIOWrapper(handle, encoding=\"utf-8\", errors=\"replace\")\n\n            records = list(SeqIO.parse(text_handle, \"genbank\"))\n\n        if not records:\n            logger.error(f\"Input file {filepath} is not GenBank format. Please check your input\")\n            return None\n        else:\n            is_valid_genbank = True\n\n\n        # Scan comments for Prokka signature\n        if not euk:\n            for rec in records:\n                comment = rec.annotations.get(\"comment\", \"\") or \"\"\n                if \"annotated using prokka\" in comment.lower():\n                    is_prokka = True\n                    break\n\n\n            if is_prokka is False:\n                logger.warning(f\"Input file {filepath} does not appear to come from Prokka.\")\n                logger.warning(f\"Conversion will proceed but no guarantee of success.\")\n\n    except Exception:\n        logger.error(f\"There was an error parsing {filepath}. Please check your input\")\n        return None\n\n    return records\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.validation.instantiate_dirs","title":"<code>instantiate_dirs(output_dir, force)</code>","text":"<p>Checks and instantiates the output directory.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>Union[str, Path]</code> <p>Path to the output directory.</p> required <code>force</code> <code>bool</code> <p>Force flag indicating whether to overwrite existing directory.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Final output directory path.</p> Source code in <code>src/baktfold/utils/validation.py</code> <pre><code>def instantiate_dirs(output_dir: Union[str, Path], force: bool) -&gt; Path:\n    \"\"\"\n    Checks and instantiates the output directory.\n\n    Parameters:\n        output_dir (Union[str, Path]): Path to the output directory.\n        force (bool): Force flag indicating whether to overwrite existing directory.\n\n    Returns:\n        Path: Final output directory path.\n    \"\"\"\n\n    # Checks the output directory\n    # remove outdir on force\n    logger.add(lambda _: sys.exit(1), level=\"ERROR\")\n    logger.info(f\"Checking the output directory {output_dir}\")\n    if force is True:\n        if Path(output_dir).exists():\n            logger.info(f\"Removing {output_dir} because --force was specified\")\n            shutil.rmtree(output_dir)\n        else:\n            logger.info(\n                \"--force was specified even though the output directory does not already exist. Continuing\"\n            )\n    else:\n        if Path(output_dir).exists():\n            logger.error(\n                \"Output directory already exists and force was not specified. Please specify -f or --force to overwrite the output directory\"\n            )\n\n    # instantiate outdir\n    if Path(output_dir).exists() is False:\n        Path(output_dir).mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.validation.validate_input","title":"<code>validate_input(input, threads)</code>","text":"<p>Validate the input file format and retrieve genomic data.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Path</code> <p>Path to the input file.</p> required <code>threads</code> <code>int</code> <p>Number of threads to use for prediction.</p> required <p>Returns:</p> Type Description <code>Dict[str, Union[bool, Dict]]</code> <p>Dict[str, Union[bool, Dict]]: A dictionary containing validation flags and genomic data.</p> Source code in <code>src/baktfold/utils/validation.py</code> <pre><code>def validate_input(input: Path, threads: int) -&gt; Dict[str, Union[bool, Dict]]:\n    \"\"\"\n    Validate the input file format and retrieve genomic data.\n\n    Parameters:\n        input (Path): Path to the input file.\n        threads (int): Number of threads to use for prediction.\n\n    Returns:\n        Dict[str, Union[bool, Dict]]: A dictionary containing validation flags and genomic data.\n    \"\"\"\n\n    # validates input\n    fasta_flag = False\n    gb_dict, method = get_genbank(input)\n\n    if not gb_dict:\n        logger.warning(f\"{input} was not a Genbank format file\")\n        logger.warning(\n            f\"Now checking if the input {input} is a genome in nucleotide FASTA format\"\n        )\n        logger.warning(f\"pyrodigal-gv will be used to predict CDS\")\n        logger.warning(f\"baktfold will not predict tRNAs, tmRNAs or CRISPR repeats\")\n        logger.warning(\n            f\"Please use pharokka https://github.com/gbouras13/pharokka if you would like to predict these\"\n        )\n        logger.warning(\n            f\"And then use the genbank output pharokka.gbk as --input for baktfold\"\n        )\n\n        # check the contig ids are &lt; 54 chars\n        for record in SeqIO.parse(input, \"fasta\"):\n            # Check if the length of the record ID is 54 characters or more\n            if len(record.id) &gt;= 54:\n                logger.warning(\n                    f\"The contig header {record.id} is longer than 54 characters. It is recommended that you use shorter contig headers as this can create issues downstream.\"\n                )\n\n        gb_dict = get_fasta_run_pyrodigal_gv(input, threads)\n        if not gb_dict:\n            logger.warning(\"Error: no records found in FASTA file\")\n            logger.error(\"Please check your input\")\n        else:\n            logger.info(\n                f\"Successfully parsed input {input} as a FASTA and predicted CDS\"\n            )\n            fasta_flag = True\n    else:\n        logger.info(f\"Successfully parsed input {input} as a {method} style Genbank file.\")\n\n    return fasta_flag, gb_dict, method\n</code></pre>"},{"location":"reference/utils/#src.baktfold.utils.validation.validate_outfile","title":"<code>validate_outfile(outfile, force)</code>","text":"<p>Checks and instantiates the output file for baktfold convert-prokka</p> <p>Parameters:</p> Name Type Description Default <code>outfile</code> <code>Union[str, Path]</code> <p>Path to the output file.</p> required <code>force</code> <code>bool</code> <p>Force flag indicating whether to overwrite existing outfile.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Final output file path.</p> Source code in <code>src/baktfold/utils/validation.py</code> <pre><code>def validate_outfile(outfile: Union[str, Path], force: bool) -&gt; Path:\n    \"\"\"\n    Checks and instantiates the output file for baktfold convert-prokka\n\n    Parameters:\n        outfile (Union[str, Path]): Path to the output file.\n        force (bool): Force flag indicating whether to overwrite existing outfile.\n\n    Returns:\n        Path: Final output file path.\n    \"\"\"\n\n    # Checks the output directory\n    # remove outdir on force\n    logger.add(lambda _: sys.exit(1), level=\"ERROR\")\n    logger.info(f\"Checking the output file {outfile}\")\n    if force is True:\n        if Path(outfile).exists():\n            logger.info(f\"Removing {outfile} because --force was specified\")\n            Path(outfile).unlink()\n        else:\n            logger.info(\n                f\"--force was specified even though the output file {outfile} does not already exist. Continuing\"\n            )\n    else:\n        if Path(outfile).exists():\n            logger.error(\n                f\"Output file {outfile} already exists and force was not specified. Please specify -f or --force to overwrite the output file\"\n            )\n</code></pre>"}]}